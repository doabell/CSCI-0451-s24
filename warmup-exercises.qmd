---
title: Warmup Exercises
number-sections: true
author: Phil
---


::: {.hidden}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\paren}[1]{\left( #1 \right)}
$$

:::

### Classification Rates {#sec-classification-rates}

#### Part 1

COVID-19 rapid tests have approximately an 80% sensitivity rate, which means that, in an individual who truly has COVID-19, the probability of a rapid test giving a positive result is roughly 80%. [These numbers are mostly made-up.]{.aside} On the other hand, the probability of a rapid test giving a positive result for an individual who truly does **not** have COVID-19 is 5%. Suppose that approximately 4% of the population are currently infected with COVID-19. [Example 2.3.1 of [Murphy](https://github.com/probml/pml-book/releases/latest/download/book1.pdf), page 46, has a good review of the relevant probability and the definition of each of the rates below.]{.aside}

Write a Python function called `rate_summary` that prints the following output, filling in the correct values for each of the specified rates:  

```python
s = 0.8           # test sensitivity
f = 0.02          # probability of positive test if no COVID
prevalence = 0.05 # fraction of population infected

rate_summary(s, f, current_infection)
```

```
The true positive rate is ___.
The false positive rate is ___.
The true negative rate is ___. 
The false positive rate is ___. 
```

#### Part 2

1. Suppose that scientists found an alternative rapid test which had a 75% sensitivity rate with a 0% chance of a positive test on someone who is truly not infected. Would you suggest replacing the old rapid tests with these alternative tests? Why? [You don't necessarily need to use your function from the previous part in this part.]{.aside}
2. What if the alternative test had an 85% sensitivity rate and a 10% chance of a positive test on someone who is truly not infected?  

#### Part 3

It's all well and good to do the math, but what about when we actually have data? Write a function called `rate_summary_2` that accepts two columns of a `pandas.DataFrame` (or equivalently two one-dimensional `numpy.arrays` of equal length). Call these `y` and `y_pred`. Assume that both `y` and `y_pred` are binary arrays (i.e. arrays of 0s and 1s). `y` represents the true outcome, whereas `y_pred` represents the prediction from an algorithm or test. Here's an example of the kind of data we are thinking about: 

```{python}
import pandas as pd

url = "https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/toy-classification-data.csv"
df = pd.read_csv(url)

df.head() # just for visualizing the first few rows
```

You should be able to use your function like this: 

```python
# y is the true label, y_pred is the prediction
rate_summary_2(df["y"], df["y_pred"]) 
```

```
The true positive rate is ___.
The false positive rate is ___.
The true negative rate is ___. 
The false positive rate is ___. 
```

##### Hints

An excellent solution for this part will not use any for-loops. Computing each of the four rates can be performed in a single compact line of code. To begin thinking of how you might do this, you may want to experiment with code like the following: 

```python
df[["y"]] == df[["y_pred"]]
df[["y"]].sum(), df[["y"]].sum()
```

### Perceptron {#sec-perceptron}

#### Part 1

Sketch the line in $\R^2$ described by the equation 
$$ 
\bracket{\vw, \vx}  =  b\;, 
$$ {#eq-perceptron-boundary}

where $\vw = \paren{1, -\frac{1}{2}}^T \in \R^2$ and $b = \frac{1}{2}$. Here, $\bracket{\vw, \vx} = \sum_{i = 1}^n w_i x_i$ is the inner product (or dot product) between the vectors $\vw$ and $\vw$. 

#### Part 2

Write a quick Python function called `perceptron_classify(w, b, x)`. `w` and `x` should both be 1d numpy arrays of the same length, and `b` should be a scalar. Your function should return `0` if $\bracket{\vw, \vx}  <  b$ and `1` if $\bracket{\vw, \vx}  \geq  b$. [An excellent solution will use neither a `for`-loop nor an `if`-statement.]{.aside}

Verify that your function works on a few simple examples. 

#### Part 3

Consider a line of the general form of @eq-perceptron-boundary. Let's allow $\vx$ and $\vw$ to both be $n$-dimensional, so that this equation defines a hyperplane in $\R^n$. Suppose that we wanted to represent *the same hyperplane* in $\R^n$ using an equation of the form 

$$
\bracket{\tilde{\vw}, \tilde{\vx}} = 0\;.
$$ {#eq-perceptron-boundary-2}

for some $\tilde{\vw} \in \R^{n+1}$. Define $\tilde{\vx} = (\vx, 1)$. How could you define $\tilde{\vw}$ to make @eq-perceptron-boundary-2 equivalent to @eq-perceptron-boundary?



### Empirical Risk Minimization {#sec-erm}

Let's play a game! Here is the setup: 

I have a coin with probability of heads equal to $p \in [0,1]$. I am going to ask you to pick a number $\hat{p} \in [0,1]$. Then, I flip my coin. 

[This game is more fun for me than it is for you.]{.aside}

- If my coin comes up heads, you give me $-\log \hat{p}$ dollars. 
- If my coin comes up tails, you give me $\log (1-\hat{p})$ dollars. 

#### Part 1

Compute the *expected* amount of money you will give me when we play this game in terms of $p$ and $\hat{p}$. Call this quantity $R(\hat{p}, p)$. This is the *risk* of the guess $\hat{p}$. 

#### Part 2

[Take the derivative and set it equal to 0! Don't forget to check that you've found a minimum of $R(\hat{p}, p)$ rather than a maximum or an inflection point.]{.aside}

Suppose I tell you the value of $p$. Write a mathematical proof to show that your best choice of $\hat{p}$ (the one that loses you the least money) is $\hat{p} = p$. 

#### Part 3

Now suppose that I *don't* tell you the true value of $p$. Instead, I let you observe $n$ coin flips before asking you to make your guess. Describe: 

- A suggestion for choosing $\hat{p}$ based only on the results of the previous flips. 
- A way to estimate the risk (expected amount of money lost) based only on the results of the previous flips. 

Your answer should depend on $\hat{p}$ but not on $p$! 

