---
title: Warmup Exercises
number-sections: true
author: Phil
bibliography: refs.bib
---


::: {.hidden}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\paren}[1]{\left( #1 \right)}
$$

:::


## Perceptron {#sec-perceptron}

#### Part 1

Sketch the line in $\R^2$ described by the equation 
$$ 
\bracket{\vw, \vx}  =  b\;, 
$$ {#eq-perceptron-boundary}

where $\vw = \paren{1, -\frac{1}{2}}^T \in \R^2$ and $b = \frac{1}{2}$. Here, $\bracket{\vw, \vx} = \sum_{i = 1}^n w_i x_i$ is the inner product (or dot product) between the vectors $\vw$ and $\vw$. 

#### Part 2

Write a quick Python function called `perceptron_classify(w, b, x)`. `w` and `x` should both be 1d numpy arrays of the same length, and `b` should be a scalar. Your function should return `0` if $\bracket{\vw, \vx}  <  b$ and `1` if $\bracket{\vw, \vx}  \geq  b$. [An excellent solution will use neither a `for`-loop nor an `if`-statement.]{.aside}

Verify that your function works on a few simple examples. 

#### Part 3

Consider a line of the general form of @eq-perceptron-boundary. Let's allow $\vx$ and $\vw$ to both be $n$-dimensional, so that this equation defines a hyperplane in $\R^n$. Suppose that we wanted to represent *the same hyperplane* in $\R^n$ using an equation of the form 

$$
\bracket{\tilde{\vw}, \tilde{\vx}} = 0\;.
$$ {#eq-perceptron-boundary-2}

for some $\tilde{\vw} \in \R^{n+1}$. Define $\tilde{\vx} = (\vx, 1)$. How could you define $\tilde{\vw}$ to make @eq-perceptron-boundary-2 equivalent to @eq-perceptron-boundary?

## Convexity {#sec-convexity}

As you learned in DaumÃ©, informally, a convex function is a function that is "bowl-shaped." Hardt and Recht give the formal definition, which has the benefit of applying to functions of many variables. 

#### Part 1

Consider the 0-1 step function that I've plotted below: 

```{python}
#| echo: false
from matplotlib import pyplot as plt
plt.rcParams["figure.figsize"] = (5, 3)
```

```{python}
from matplotlib import pyplot as plt 
import numpy as np

fig, ax = plt.subplots(1, 1) 
y_hat = np.linspace(-1, 1, 101)

loss = lambda y_hat, y: 1 - 1*(y_hat*y > 0)

ax.set(xlabel = r"$\hat{y}$", 
       ylabel = r"$\ell(\hat{y}, y)$")

ax.plot(y_hat, loss(y_hat, 1))
```

Show pictorially that this function is not convex. No proof needed -- just the right drawing. 

### Part 2: Second Derivative Test

Another way to tell whether a function is convex is to check its second derivative. If a function $f:S\rightarrow \R$ has a convex domain $S\subseteq \R$, if $f$ is everywhere twice-differentiable, and if $\frac{d^2f(z_0)}{dz^2} > 0$ for all $z_0 \in S$, then $f$ is convex. 

Use the second derivative test to check that the following two functions are convex: [The base of the logarithm doesn't really matter, but for this course it is always most convenient to assume logs base $e$, which you might also have seen written $\ln$.]{.aside}

$$
\begin{aligned}
f(z) &= - \log z \\ 
g(z) &= - \log(1-z)\;.
\end{aligned}
$$ 



### Part 3: Plotting Practice

In a Jupyter notebook, write a simple program to plot each of the functions $f$ and $g$ from Part 2. Some of the Part 1 code is likely to help you. 

### Part 4: Convexity in Many Variables

Recall the Hardt and Recht definition of convexity: a function $f:\R^p \rightarrow \R$ is convex if, for any $\lambda \in [0,1]$ and any points $\vz_1, \vz_2 \in \R^p$, 

$$
f(\lambda \vz_1 + (1-\lambda)\vz_2) \leq \lambda f(\vz_1) + (1-\lambda)f(\vz_2)\;. 
$$

Using this definition, write a short mathematical proof that the function $f(\vz) = \norm{\vz} = \sqrt{\bracket{\vz, \vz}}$ is convex. You will want to use the *triangle inequality*, which says that $\norm{\vz_1 + \vz_2} \leq \norm{\vz_1} + \norm{\vz_2}$. [This proof requires just a few lines if you carefully use your definitions!]{.aside}

## Gradient Descent {#sec-gradient-descent} 

Consider the quadratic function $g(z) = \frac{1}{2}az^2 + bz + c$. 

1. Prove that $g$ has a critical point at the point $z^* = -\frac{b}{a}$ (*hint: solve $g'(z^*) = 0$*).
2. What must be true about the constants $a$, $b$, and $c$ to ensure that this point is a *local minimum* of $g$? (*Hint: second derivative test*). 
3. Suppose now that we are able to evaluate the function $g$, as well as its derivative $g'$, but not able to use algebra to find $z^*$ (this mirrors our situation in most practical problems). Instead, we are going to use the following algorithm to attempt to approximate $z^*$: 
    - Begin with some initial guess $z^{(0)}$. 
    - In each time-step $t$, compute $z^{(t+1)} \gets z^{(t)} - \alpha g'(z^{(t)})$, where $\alpha > 0$ is the *learning rate*. 
    - In practice we would need to specify a stopping criterion, but for this theoretical problem we don't need to worry about it. 
4. Using algebra, prove that for any timestep $t$, 
$$
(z^* - z^{(t+1)})^2 = (a\alpha - 1)^2(z^* - z^{(t)})^2\;. 
$$
5. Let's think of $\abs{z^* - z^{(t)}}$ as the *error* in our current estimate $z^{(t)}$. Using the recurrence above, conclude that, for any $t$, the error $\abs{z^* - z^{(t)}}$ satisfies 
$$
\abs{z^* - z^{(t)}} = \abs{a\alpha - 1}^{t}\abs{z^* - z^{(0)}}\;.
$$
6. For $\alpha \in (\alpha_*, \alpha^*)$, we are guaranteed that the error $\abs{z^* - z^{(t)}}\rightarrow 0$ as $t\rightarrow \infty$. What are $\alpha_*$ and $\alpha^*$?  
7. Suppose that $\alpha$ is within the necessary range. I want to guarantee that $\abs{z^* - z^{(t)}} < \epsilon$ for some small $\epsilon > 0$ (in practice we often call this the *tolerance*). Conclude that the number of steps necessary to reach this tolerance is no greater than 
$$
\bar{t} = \frac{ \log \epsilon - \log \abs{z^* - z^{(0)}}}{\log \abs{a\alpha - 1}}\;.
$$

Ignoring constants with respect to $\epsilon$, we say that this algorithm for finding the minimum of $g$ with tolerance $\epsilon$ has a $\log \epsilon$ a convergence rate. 

## Gradient Descent (Again) {#sec-gradient-descent-2}

Consider the function $f(w_0, w_1) = \sin(w_0w_1)$. You can define this function like this: 

```{python}
#| code-fold: false

import numpy as np
def f(w):
    return np.sin(w[0]*w[1])
```

Mathematically, the gradient of this function is 

$$\nabla f(w_0, w_1) = (w_1\cos w_0w_1, w_0 \cos w_0w_1)^T.$$ 

1. Implement a simple loop that uses gradient descent to find a minimum of this function. 
    - You'll have to choose the learning rate $\alpha$. 
    - The `np.cos()` function will be useful for programming the gradient. 
    - It's not the fastest approach, but if you're not show how to program the gradient you can always first implement it as a list of two floats, and then use `np.array(my_list)` to convert it into a numpy array. 
    - You'll also need to pick a random starting guess. 
2. Find two initial guesses for the parameter vector $\vw$ such that you get two *different* final minimizers (this is possible because $f$ is not convex).  


## Overfitting and the Scientific Method {#sec-overfitting}


[![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/The_Scientific_Method.svg/520px-The_Scientific_Method.svg.png) Image from [Wikipedia](https://en.wikipedia.org/wiki/Scientific_method).]{.aside}

In [the scientific method](https://en.wikipedia.org/wiki/Scientific_method), it is often emphasized that we need to formulate a hypothesis *before* performing an experiment. It's fine for the hypothesis to be based on *previous* experiments. However, the scientific method never allows us to perform an experiment, formulate a hypothesis, and then say that the experiment supported the (new) hypothesis. 

We can think of scientific theories as systems of thought that help us make predictions about new phenomena. With this in mind, ***please write a short paragraph explaining the importance of hypothesis-first science using the language of machine learning.*** In your explanation, please use the following vocabulary:  

- Training data. 
- Training accuracy. 
- Validation/testing data. 
- Validation/testing accuracy. 
- Overfitting. 

## The Coin-Flipping Game {#sec-erm}

Let's play a game! Here is the setup: 

I have a coin with probability of heads equal to $p \in [0,1]$. I am going to ask you to pick a number $\hat{p} \in [0,1]$. Then, I flip my coin. 

[This game is more fun for me than it is for you.]{.aside}

- If my coin comes up heads, you give me $-\log \hat{p}$ dollars. 
- If my coin comes up tails, you give me $-\log (1-\hat{p})$ dollars. 

#### Part 1

Compute the *expected* amount of money you will give me when we play this game in terms of $p$ and $\hat{p}$. Call this quantity $R(\hat{p}, p)$. This is the *risk* of the guess $\hat{p}$. 

#### Part 2

[Take the derivative and set it equal to 0! Don't forget to check that you've found a minimum of $R(\hat{p}, p)$ rather than a maximum or an inflection point.]{.aside}

Suppose I tell you the value of $p$. Write a mathematical proof to show that your best choice of $\hat{p}$ (the one that loses you the least money) is $\hat{p} = p$. 

#### Part 3

Now suppose that I *don't* tell you the true value of $p$. Instead, I let you observe $n$ coin flips before asking you to make your guess. Describe: 

- A suggestion for choosing $\hat{p}$ based only on the results of the previous flips. 
- A way to estimate the risk (expected amount of money lost) based only on the results of the previous flips. 

Your answer should depend on $\hat{p}$ but not on $p$! 


## Balancing Classification Rates {#sec-classification-rates-2}

[You can do this first part just by copying and pasting lecture code. It doesn't matter much how good your model is -- just make sure you're able to get predictions.]{.aside}

Use the code from [our recent lecture](/lecture-notes/classification-in-practice.qmd) to download the Titanic data set as a Pandas data frame and train a model on the training data. Then download the test data. Compute `y_pred`, the vector of predictions of your model on the test data. 

Then, write a function that verifies eq. (2.6) in Alexandra Chouldechova's paper "[Fair Prediction with disparate impact](https://via.hypothes.is/https://arxiv.org/pdf/1703.00056.pdf)." Here's what your function should do: 

[The positive predictive value is $\mathrm{PPV} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}}$.]{.aside}

1. Given vectors `y_pred` of predictions and `y_test` of actual labels, compute the False Negative Rate (FNR), False Positive Rate (FPR), prevalence $p$, and positive predictive value (PPV). 
2. Return as a tuple the lefthand side and righthand side of eq. (2.6) in Chouldechova. 
3. Verify that the two numbers are equal! 

## Limits of The Quantitative Approach to Discrimination {#sec-limits-quantitative}

I'll give you each a number in Slack. The numbers correspond to the following sections of @narayanan2022limits. These are: 

1. The null hypothesis allocates the burden of proof (p. 7-8)
2. Compounding inequality is far below the radar of quantitative methods (p. 9-10)
3. Snapshot datasets hide discrimination (p. 10-11)
4. Explaining away discrimination (p. 12-13)
5. What counts as evidence is a subjective choice (p. 5-7)

For your assigned section, please write a short paragraph (4-5 simple sentences is fine). You should: 

- Summarize Narayanan's key points in that section. 
- In one of the sentences, describe which aspects of the Uber case study (p. 13-16) reflect the ideas of the section you described. 

Bring your paragraph in class and be ready to read it to your group. 

## Classification Rates {#sec-classification-rates}

#### Part 1

COVID-19 rapid tests have approximately an 80% sensitivity rate, which means that, in an individual who truly has COVID-19, the probability of a rapid test giving a positive result is roughly 80%. [These numbers are mostly made-up.]{.aside} On the other hand, the probability of a rapid test giving a positive result for an individual who truly does **not** have COVID-19 is 5%. Suppose that approximately 4% of the population are currently infected with COVID-19. [Example 2.3.1 of [Murphy](https://github.com/probml/pml-book/releases/latest/download/book1.pdf), page 46, has a good review of the relevant probability and the definition of each of the rates below.]{.aside}

Write a Python function called `rate_summary` that prints the following output, filling in the correct values for each of the specified rates:  

```python
s = 0.8           # test sensitivity
f = 0.02          # probability of positive test if no COVID
prevalence = 0.05 # fraction of population infected

rate_summary(s, f, current_infection)
```

```
The true positive rate is ___.
The false positive rate is ___.
The true negative rate is ___. 
The false positive rate is ___. 
```

#### Part 2

1. Suppose that scientists found an alternative rapid test which had a 75% sensitivity rate with a 0% chance of a positive test on someone who is truly not infected. Would you suggest replacing the old rapid tests with these alternative tests? Why? [You don't necessarily need to use your function from the previous part in this part.]{.aside}
2. What if the alternative test had an 85% sensitivity rate and a 10% chance of a positive test on someone who is truly not infected?  

#### Part 3

It's all well and good to do the math, but what about when we actually have data? Write a function called `rate_summary_2` that accepts two columns of a `pandas.DataFrame` (or equivalently two one-dimensional `numpy.arrays` of equal length). Call these `y` and `y_pred`. Assume that both `y` and `y_pred` are binary arrays (i.e. arrays of 0s and 1s). `y` represents the true outcome, whereas `y_pred` represents the prediction from an algorithm or test. Here's an example of the kind of data we are thinking about: 

```{python}
import pandas as pd

url = "https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/toy-classification-data.csv"
df = pd.read_csv(url)

df.head() # just for visualizing the first few rows
```

You should be able to use your function like this: 

```python
# y is the true label, y_pred is the prediction
rate_summary_2(df["y"], df["y_pred"]) 
```

```
The true positive rate is ___.
The false positive rate is ___.
The true negative rate is ___. 
The false positive rate is ___. 
```

##### Hints

An excellent solution for this part will not use any for-loops. Computing each of the four rates can be performed in a single compact line of code. To begin thinking of how you might do this, you may want to experiment with code like the following: 

```python
df[["y"]] == df[["y_pred"]]
df[["y"]].sum(), df[["y"]].sum()
```
