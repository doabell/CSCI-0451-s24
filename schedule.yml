- date: 2023-02-13
  header: | 
    Welcome!
  summary: |
    We discuss how the course works and begin our discussion of classification and auditing. 
  objectives: 
    - Getting Oriented
  reading:
    - <a href="syllabus.qmd">Course syllabus</a>
    - <a href="collaboration.qmd">Collaboration</a>
    - <a href="https://via.hypothes.is/https://www.jessestommel.com/why-i-dont-grade/">Why I Don't Grade</a> by Jesse Stommel
    - <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf">Daumé 1.1-1.5</a>
  notes: 
    - |
      <a href="slides/welcome.qmd">Welcome slides</a>
    - |
      <a href="lecture-notes/intro-classification.ipynb">Lecture notes</a>
  module: Classification
  publish: true
  warmup: <a href="software.qmd">Set up your software</a>.
  assignments: No really, <br><a href="software.qmd">set up your software</a>.
- date: 2023-02-15
  header: | 
    More On Classification and Auditing
  summary: |
    We continue our discussion of classification and introduce algorithmic bias. 
  objectives: 
    - Social Responsibility
    - Navigation
    - Experimentation
  reading:
    - <a href="https://via.hypothes.is/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias</a> from <i>ProPublica</i>
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/#3.-Data-Manipulation-with-Pandas">Data Manipulation with Pandas</a> from <i>The Python Data Science Handbook</i> by Jake VanderPlas
    - Example 2.3.1 from <a href="https://github.com/probml/pml-book/releases/latest/download/book1.pdf">Murphy</a> 
  notes: 
    - |
      <a href="lecture-notes/intro-classification.ipynb">Lecture notes</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#classification-rates"> Classification rates </a>
- date: 2023-02-20
  header: | 
    Classification: The Perceptron
  summary: |
    We study the perceptron algorithm, a historical method that serves as the foundation for many modern classifiers. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch04.pdf">Daumé 4.1-4.5, 4.7</a>
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/#2.-Introduction-to-NumPy">Introduction to Numpy</a> from <i>The Python Data Science Handbook</i> by Jake VanderPlas
    - <a href="https://nbviewer.org/github/PhilChodrow/PIC16B/blob/master/lectures/math/linear-algebra-I.ipynb">Linear algebra with Numpy</a>
  optional: 
    - <a href="https://via.hypothes.is/https://arxiv.org/pdf/2102.05242.pdf">Hardt and Recht</a>, p. 37-41
  notes: 
    - TBD
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#perceptron"> Perceptron </a>
  assignments: 
    - |
      <a href="assignments/blog-posts/blog-post-perceptron.qmd">Blog post: perceptron</a>
- date: 2023-02-22
  header: | 
    Models, Loss Functions, and Algorithms
  summary: |
    We generalize our study of the perceptron to a broader framework for supervised machine learning called <i>empirical risk minimization</i>. Within this framework, we begin to study logistic regression and the primal support vector machine for binary classification.
  objectives: 
    - Theory
    - Implementation
  reading:
    - TBD
  notes: 
    - TBD
  module: Classification
  publish: true
  warmup: TBD
  assignments:
    - |
      <a href="assignments/process/goal-setting.ipynb">Reflective goal-setting</a> <br><b>ACTUAL DUE DATE: 2/24</b>
- date: 2023-02-27
  header: | 
    Optimization
  summary: |
    We discuss standard mathematical methods for empirical risk minimization, including gradient descent and stochastic gradient descent. We also recontextualize the perceptron algorithm as stochastic subgradient descent for the primal support vector machine. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - TBD
  notes: 
    - TBD
  module: Classification
  publish: true
  warmup: TBD
  assignments:
    - <a href="assignments/process/goal-setting.ipynb">Reflective goal-setting</a>