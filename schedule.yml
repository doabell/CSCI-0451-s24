- date: 2023-02-13
  header: | 
    Welcome!
  summary: |
    We discuss how the course works and begin our discussion of classification and auditing. 
  objectives: 
    - Getting Oriented
  reading:
    - <a href="syllabus.qmd">Course syllabus</a>
    - <a href="collaboration.qmd">Collaboration</a>
    - <a href="https://via.hypothes.is/https://www.jessestommel.com/why-i-dont-grade/">Why I Don't Grade</a> by Jesse Stommel
    - <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf">Daumé 1.1-1.5</a>
  notes: 
    - |
      <a href="slides/welcome.qmd">Welcome slides</a>
  module: Classification
  publish: true
  warmup: <a href="software.qmd">Set up your software</a>.
  assignments: No really, <br><a href="software.qmd">set up your software</a>.
- date: 2023-03-01
  header: | 
    More On Classification and Auditing
  summary: |
    We continue our discussion of classification and introduce algorithmic bias. 
  objectives: 
    - Social Responsibility
    - Navigation
    - Experimentation
  reading:
    - <a href="https://via.hypothes.is/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias</a> from <i>ProPublica</i>
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/#3.-Data-Manipulation-with-Pandas">Data Manipulation with Pandas</a> from <i>The Python Data Science Handbook</i> by Jake VanderPlas
    - Example 2.3.1 from <a href="https://github.com/probml/pml-book/releases/latest/download/book1.pdf">Murphy</a> 
  notes: 
    - |
      <a href="lecture-notes/intro-classification.ipynb">Lecture notes</a>
  module: Classification
  publish: false
  warmup: <a href="warmup-exercises.qmd#classification-rates"> Classification rates </a>
  assignments:
    - |
      <a href="assignments/process/goal-setting.ipynb">Reflective goal-setting</a> <br><b>ACTUAL REQUIRED DUE DATE: 2/24</b>
- date: 2023-02-15
  header: | 
    Classification: The Perceptron
  summary: |
    We study the perceptron algorithm, a historical method that serves as the foundation for many modern classifiers. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch04.pdf">Daumé 4.1-4.5, 4.7</a>
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/#2.-Introduction-to-NumPy">Introduction to Numpy</a> from The Python Data Science Handbook by Jake VanderPlas
    - <a href="https://nbviewer.org/github/PhilChodrow/PIC16B/blob/master/lectures/math/linear-algebra-I.ipynb">Linear algebra with Numpy</a>
  optional: 
    - <a href="https://via.hypothes.is/https://arxiv.org/pdf/2102.05242.pdf">Hardt and Recht</a>, p. 33-41 (if you need to see a definition of a function gradient, see <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch07.pdf">Daumé p. 93</a>)
  notes: 
    - |
      <a href="lecture-notes/perceptron.ipynb">Lecture notes</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-perceptron">Perceptron</a>
  assignments: 
    - |
      <a href="assignments/blog-posts/blog-post-perceptron.qmd">Blog post: perceptron</a>
- date: 2023-02-20
  header: | 
    Convex Linear Models and Logistic Regression
  summary: |
    We discuss the modeling choices necessary to make the empirical risk minimization problem for linear classifiers tractable. In doing so we discuss convex functions and some of their properties that are relevant for optimization. Finally, we introduce logistic regression as an example of a convex linear classifier. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - |
      <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch02.pdf">Daumé 2.1-2.7</a>
    - |
      <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch07.pdf">Daumé 7.1-7.3</a>
    - <a href="https://via.hypothes.is/https://arxiv.org/pdf/2102.05242.pdf">Hardt and Recht, p. 70-77</a>
  notes: 
    - <a href="lecture-notes/convex-linear-models.ipynb">Lecture notes</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-convexity">Convexity</a>
- date: 2023-02-22
  header: | 
    Optimization via Gradient Descent 
  summary: |
    We discuss standard mathematical methods for empirical risk minimization, including gradient descent and stochastic gradient descent. We also recontextualize the perceptron algorithm as stochastic subgradient descent for a linear classifier with a specific loss function. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch07.pdf">Daumé 7.4-7.6</a>
    - <a href="https://via.hypothes.is/https://mml-book.github.io/book/mml-book.pdf">Diesenroth, Faisal, and Soon, p. 225-233</a>
  notes: 
    - <a href="lecture-notes/gradient-descent.qmd">Lecture notes</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-gradient-descent">Gradient Descent</a>
  assignments:
    - |
      <a href="assignments/blog-posts/blog-post-optimization.qmd">Blog post: gradient descent</a>
- date: 2023-02-27
  header: | 
    Features, Regularization, and Nonlinear Decision Boundaries
  summary: |
    We learn how to use feature maps to help our convex linear classifiers learn nonlinear patterns. We also introduce the problem of overfitting and introduce feature selection and regularization as methods for addressing this problem. 
  objectives: 
    - Theory
    - Implementation
    - Navigation
    - Experimentation
  reading:
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html">Introducing Scikit-Learn</a>
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html">Hyperparameters and Model Validation</a>
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html">Feature Engineering</a>
  notes: 
    - <a href="lecture-notes/features-regularization.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/features-regularization-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-gradient-descent-2">Gradient Descent Again</a>
  assignments: |
    <b>ACTUAL REAL DUE DATE</b>: <a href="assignments/process/goal-setting.ipynb">Reflective Goal-Setting</a> due 2/27 </b>
- date: 2023-03-01
  header: | 
    Classification in Practice
  summary: |
    We work through a complete modeling workflow for the Titanic survival data set. Along the way, we work with data frames and discuss cross-validation. 
  objectives: 
    - Navigation
    - Experimentation
  reading:
    - <a href="https://via.hypothes.is/http://ciml.info/dl/v0_99/ciml-v0_99-ch02.pdf">Daumé Chapter 2</a> <i>You may find it useful to review Chapter 1 as well.</i>
    - <a href="https://via.hypothes.is/https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html">Data Manipulation with Pandas</a> (Focus on the sections up to and including "Aggregation and Grouping")
  notes: 
    - <a href="lecture-notes/classification-in-practice.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/classification-in-practice-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-overfitting">Overfitting and the Scientific Method</a>
  assignments: |
    <a href="assignments/blog-posts/blog-post-kernel-logistic.qmd">Blog post: kernel logistic regression</a> <br> <b>OR</b> <br> <a href="assignments/blog-posts/blog-post-penguins.qmd">Blog post: penguins</a>
- date: 2023-03-06
  header: | 
    Beyond Convex Linear Classifiers
  summary: |
    We discuss several examples of other classifiers at a high level, including some that are nonlinear or nonconvex. 
  objectives: 
    - Navigation
  reading:
    - NA
  notes: 
    - <a href="lecture-notes/more-on-classification.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/more-on-classification-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: 
  assignments: 
- date: 2023-03-08
  header: | 
    Linear Regression
  summary: |
    We introduce linear regression, another convex linear model suitable for predicting real numbers instead of class labels. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - NA
  notes: 
    - <a href="lecture-notes/regression.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/regression-live.ipynb">Live version</a>
  module: Regression
  publish: true
  warmup: 
  assignments: |
    <a href="assignments/blog-posts/blog-post-linear-regression.qmd">Blog post: Linear regression</a> 
- date: 2023-03-13
  header: | 
    Introduction to Bias and Fairness
  summary: |
    TBD
  objectives: 
    - Social Responsibility
    - Experimentation
  reading:
    - <a href="https://via.hypothes.is/https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias</a> by Julia Angwin et al. for ProPublica. 
    - <a href="https://via.hypothes.is/https://arxiv.org/pdf/1703.00056.pdf">Fair prediction with disparate impact</a> by Alexandra Chouldechova, Sections 1 and 2. 
    - <a href="https://arxiv.org/pdf/1609.05807,">Inherent trade-offs in the fair determination of risk scores</a> by Jon Kleinberg et al, pages 1-5. 
  notes: 
    - <a href="lecture-notes/intro-allocative-bias.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/intro-allocative-bias-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-classification-rates-2">Balancing Classification Rates</a>
  assignments: 
- date: 2023-03-15
  header: | 
    Critical Perspectives 
  summary: |
    We discuss limitations of the quantitative approach to studying discrimination, as well as critical perspectives on the role that automated decision systems play in surveilling and controlling marginalized individuals. 
  objectives: 
    - Social Responsibility
    - Experimentation
  reading:
    - <a href="https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf">The Limits of the Quantitative Approach to Discrimination</a>, speech by Arvind Narayanan
    - <a href="https://harpers-org.ezproxy.middlebury.edu/archive/2018/01/the-digital-poorhouse/">"The Digital Poorhouse"</a> by Virginia Eubanks for <i>Harper's Magazine</i>
  notes: 
    - TBD
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-limits-quantitative">Limits of the Quantitative Approach</a>
  assignments: |
    <a href="assignments/blog-posts/blog-post-limits-of-quantitative.qmd">Blog post: Limits of quantitative methods</a> <br> <b> OR </b><br> 
    <a href="assignments/blog-posts/blog-post-bias-allocative.qmd">Blog post: Auditing allocative bias</a>
- date: 2023-03-27
  header: | 
    Vectorization
  summary: |
    We discuss some ways by which complex objects like images and especially text can be represented as numerical vectors for machine learning algorithms. 
  objectives: 
    - Navigation
    - Experimentation
  reading:
    - <a href="https://probml.github.io/pml-book/book1.html">Murphy, Chapter 1</a>. This is not related to vectorization; it's for you to get oriented on some possible project ideas. Don't worry about any math you don't understand. 
    - <a href="project.qmd">Course project description</a> 
  notes: 
    - <a href="lecture-notes/vectorization.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/vectorization-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-project-idea">Pitch a Project Idea</a>
  assignments: |
    <b>ACTUAL REAL DUE DATE</b>: <a href="assignments/process/mid-course.ipynb">Mid-semester reflection</a> due 4/05</b>
- date: 2023-03-29
  header: | 
    Introducing Unsupervised Learning: Topic Modeling
  summary: |
    We begin to discuss <i>unsupervised</i> learning, with topic modeling as our initial example. 
  objectives: 
    - Theory
    - Navigation
    - Experimentation
  reading:
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html">Principal Component Analysis</a> from the Python Data Science Handbook
  notes: 
    - <a href="lecture-notes/introducing-dimensionality-reduction.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/introducing-dimensionality-reduction-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-vectorization">Vectorization Brainstorm</a>
  assignments: |
    <b>ACTUAL REAL DUE DATE</b>: <a href="assignments/project/proposal.qmd">Project Proposal</a> due 4/07</b>
- date: 2023-04-03
  header: | 
    Clustering Data
  summary: |
    We continue our discussion of <i>unsupervised</i> learning with two methods for clustering sets of data. 
  objectives: 
    - Theory
    - Navigation
    - Experimentation
  reading:
    - <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html">K-Means Clustering</a> from the Python Data Science Handbook
  notes: 
    - <a href="lecture-notes/clustering.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/clustering-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-compression">K-Means Compression</a>
  assignments: |
    <a href="assignments/blog-posts/blog-post-image-processing.qmd">Blog post: Unsupervised learning with linear algebra</a> (however, using this time to complete a previous blog post is also highly recommended)
- date: 2023-04-05 
  header: | 
    Introducing Deep Learning
  summary: |
    We begin our discussion of deep learning with a quick theoretical motivation and a first glance at the PyTorch package. 
  objectives: 
    - Theory
    - Navigation
  reading:
    - <a href="https://chinmayhegde.github.io/dl-notes/notes/lecture01/">Lecture 1, Introduction</a> from Chinmay Hegde's course on deep learning at NYU
  notes: 
    - <a href="lecture-notes/intro-deep.qmd">Lecture notes</a>
    - <a href="https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/intro-deep-live.ipynb">Live version</a>
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-intro-tensors">Introducing Tensors</a>
  assignments: 
- date: 2023-04-10 
  header: | 
    Optimization For Deep Learning
  summary: |
    We begin a discussion of the training process for neural networks, which requires efficient computation of gradients via backpropagation and efficient variations of gradient descent. 
  objectives: 
    - Theory
    - Implementation
  reading:
    - <a href="https://chinmayhegde.github.io/dl-notes/notes/lecture02/">Lecture 2, Neural Nets</a> from Chinmay Hegde's course on deep learning at NYU
  notes: 
    - TBD
  module: Classification
  publish: true
  warmup: <a href="warmup-exercises.qmd#sec-backprop">Efficient Differentiation</a>
  assignments: 
