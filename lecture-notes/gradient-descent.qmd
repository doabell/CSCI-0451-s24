---
title: |
  Optimization with Gradient Descent
author: Phil Chodrow
bibliography: ../refs.bib
format: 
  html: 
    code-fold: true
    cache: true
    callout-appearance: minimal
    cap-location: margin
---

::: {.hidden}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\one}[1]{\mathbb{1}\left[ #1 \right]}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\mA}{\mathbf{A}}
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\prob}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\E}{\mathbb{E}}
\newcommand{\dd}[2]{\frac{\partial #1}{\partial #2}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

:::

# Quick Recap

Last time, we considered the problem of *empirical risk minimization* with a *convex* loss function. We assumed that we had data, a pair $(\mX, \vy)$ where 

- $\mX \in \R^{n\times p}$ is the *feature matrix*. There are $n$ distinct observations, encoded as rows. Each of the $p$ columns corresponds to a *feature*: something about each observation that we can measure or infer. Each observation is written $\vx_1, \vx_2,\ldots$. 
$$
\mX = \left[\begin{matrix} & - & \vx_1 & - \\ 
& - & \vx_2 & - \\ 
& \vdots & \vdots & \vdots \\ 
& - & \vx_{n} & - \end{matrix}\right]
$$
- $\vy \in \R^{n}$ is the *target vector*. The target vector gives a label, value, or outcome for each observation. 

Using this data, we defined the empirical risk minimization problem, which had the general form 
$$
\hat{\vw} = \argmin_{\vw} \; L(\vw)\;, 
$${#eq-empirical-risk-minimization}
where 
$$
L(\vw) = \frac{1}{n} \sum_{i = 1}^n \ell(f_{\vw}(\vx_i), y_i)\;.
$$


Here, $f_{\vw}:\R^p \rightarrow \R$ is our predictor function, which takes in a feature vector $\vx_i$ and spits out a prediction $\hat{y}_i$. We are still assuming that $f_{\vw}$ is linear and therefore has the form 

[Originally we considered classifiers of the form $f_{\vw, b}(\vx) = \bracket{\vw, \vx} - b$, but we can ignore $b$ for today by using the assumption that the final column of $\vx$ is a column of $1$s, just like we did for the perceptron.]{.aside}
$$
f_{\vw}(\vx) = \bracket{\vw, \vx}
$${#eq-linear-predictor}



The function $\ell:\R^2 \rightarrow \R$ was our focus last time: this function takes a prediction $\hat{y}_i$ and a true label $y_i$ and returns a number that tells you *how bad* that prediction was. When studying the perceptron, we considered the 0-1 loss: 

[Recall that for this version of the 0-1 loss to make sense we needed to assume that $y\in \{-1, 1\}$, rather than the more standard assumption $y \in \{0, 1\}$ that we'll use more frequently in this course.]{.aside}
$$\ell(\hat{y}, y) = 1 - \mathbb{1}[\hat{y}y > 0]\;,$$ 

which is $0$ if $\hat{y}$ has the same sign as $y$ and 1 if $\hat{y}$ has a different sign from $y$. At the end of the lecture, we introduced the alternative *logistic loss*
$$
\ell(\hat{y}, y) = -y \log \sigma(\hat{y}) - (1-y)\log (1-\sigma(\hat{y}))\;,
$$

where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the *logistic sigmoid* function. Unlike the 0-1 loss, the logistic loss is *strictly convex* in its first argument. Recall that a strictly convex function $g:\R^n\rightarrow \R$  is a function that satisfies the strict inequality 
$$
g(\lambda \vz_1 + (1-\lambda)\vz_2) < \lambda g( \vz_1 ) + (1-\lambda)g(\vz_2)\;.
$$

for any $\vz_1, \vz_2 \in \R^n$ and $\lambda \in [0,1]$. So, when we say that the logistic loss $\ell$ is strictly convex in its first argument, what we mean is that 

$$
\ell(\lambda \hat{y}_1 + (1-\lambda)\hat{y}_2, y) < \lambda \ell( \hat{y}_1, y) + (1-\lambda)\ell(\hat{y}_2, y)
$$
for any true label $y$, possible predictions $\hat{y}_1, \hat{y}_2$, and number $\lambda \in [0,1]$. 

As we discussed, the important thing about empirical risk minimization with linear predictors and convex loss functions is that, if there is an optimal solution, that solution is unique. Furthermore, any local minimizer of the loss is in fact the (unique) global minimizer. So, we can design algorithms that search for *local* minima. In this lecture we are going to discuss *gradient methods*, which are the most common class of optimization algorithms in use today. 

## Gradients

[We're not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.]{.aside}

::: {.callout-note}
::: {#def-gradient}

## Gradient of a Multivariate Function

Let $f:\R^p \rightarrow \R$ be a *multivariate differentiable* function. The *gradient* of $f$ evaluated at point $\vz\in \R^p$ is written $\nabla f(\vz)$, and has value 

$$
\nabla f(\vz) \triangleq 
\left(\begin{matrix}
    \dd{f(\vz)}{z_1} \\ 
    \dd{f(\vz)}{z_2} \\ 
    \cdots \\ 
    \dd{f(\vz)}{z_p} \\ 
\end{matrix}\right) \in \R^p\;.
$$

Here, $\dd{f(\vz)}{z_1}$ is the *partial derivative of $f$ with respect to $z_1$, evaluated at $\vz$*. To compute it: 

> Take the derivative of $f$ *with respect to variable $z_1$, holding all other variables constant, and then evaluate the result at $\vz$. 

:::
:::



::: {.callout-tip}

## Example

Let $p = 3$. Let $f(\vz) = z_2\sin z_1  + z_1e^{2z_3}$. The partial derivatives we need are 

$$
\begin{align}
\dd{f(\vz)}{z_1} &= z_2 \cos z_1 + e^{2z_3}\\ 
\dd{f(\vz)}{z_2} &= \sin z_1\\ 
\dd{f(\vz)}{z_3} &= 2z_1 e^{2z_3}\;. 
\end{align}
$$

So, the gradient of $f$ evaluated at a point $\vz$ is 

$$
\nabla f(\vz) =
\left(\begin{matrix}
    \dd{f(\vz)}{z_1} \\ 
    \dd{f(\vz)}{z_2} \\ 
    \dd{f(\vz)}{z_3} \\ 
\end{matrix}\right) = 
\left(\begin{matrix}
    z_2 \cos z_1 + e^{2z_3}\\
    \sin z_1\\ 
    2z_1 e^{2z_3}
\end{matrix}\right) 
$$

:::

So, a gradient $\nabla f(\vz)$ is a vector of the same dimension as $\vz$. What happens if we combine them? This is where we get to the really important aspects of gradients for practical purposes: 

::: {.callout-note}
::: {#thm-local-minima}

## Local Minima Have $\nabla f(\vz_0) = \vzero$ 

Let $f:\R^p \rightarrow \R$ be differentiable. If $\vz_0$ is a local minimum of $f$, then $\nabla f(\vz_0) = \vzero$. 

:::

::: {#thm-descent-directions}

## $-\nabla f$ Is A Descent Direction

Let $f:\R^p \rightarrow \R$ be differentiable. Then, for any point $\vz \in \R^p$, if $\nabla f(\vz) \neq \vzero$, then there exists a scalar $\alpha$ such that, if $\vz' = \vz - \alpha \nabla f(\vz)$, then $f(\vz') \leq f(\vz)$. 

:::
:::

@thm-local-minima and @thm-descent-directions are important because they give us the mechanics of how to solve the empirical risk minimization problem (@eq-empirical-risk-minimization). Here's our first version: 

::: {.callout-note}

## Algorithm: (Batch) Gradient Descent

**Inputs**: Function $f$, initial starting point $\vz^{(0)}$, *learning rate* $\alpha$. 

Until convergence, in each iteration $t$, 

- Compute $\vz^{(t+1)} \gets \vz^{(t)} - \alpha \nabla f(\vz^{(t)})$.

Return the final value of $\vz^{(t)}$. 

:::



Convergence for gradient descent can be decided in a few different ways. One approach is to declare convergence when $\nabla f(\vz^{(t)})$ is close to 0. Another way is to declare convergence when the improvement in the function $f$ is small enough in magnitude. 

::: {.aside}
Sample code: 

```python
if np.allclose(grad, np.zeros(len(grad))):
    print("converged")

# or

if f(w_new) - f(w_prev) < 1e-6:
    print("converged")

```

:::

The following theorem says that *gradient descent works* if the learning rate is small enough:

::: {.callout-tip}
::: {#thm-gradient-descent-convergence}

Suppose that $f$ is strictly convex, is differentiable, and has a global minimizer $\vz^*$. Then, there exists some $\alpha > 0$ such that gradient descent applied to $f$ produces a sequence of points $\vz^{(0)}, \vz^{(1)},\ldots, \vz^{(t)}$ that converges to $\vz^*$. 

:::
:::


## Gradient Descent For Empirical Risk Minimization

Suppose that we have a per-observation loss function $\ell$ that is strictly convex and differentiable. Suppose that we are still dealing with a linear predictor of the form in @eq-linear-predictor. Then, we know that the empirical risk objective function $L$ is also strictly convex and differentiable. It follows from @thm-gradient-descent-convergence that, if there is a minimizer $\vw^*$ for the empirical risk, then we can find it using gradient descent. To do this, we need to be able to calculate the gradient of the loss function $L$. Here's how this looks. Keep in mind that we are differentiating with respect to $\vw$. 



$$
\begin{align}
\nabla L(\vw) &= \nabla \left(\frac{1}{n} \sum_{i = 1}^n \ell(f_{\vw}(\vx_i), y_i)\right) \\ 
              &= \frac{1}{n} \sum_{i = 1}^n \nabla \ell(f_{\vw}(\vx_i), y_i) \\ 
              &= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(\hat{y}_i, y_i)}{d\hat{y}} \nabla f_{\vw}(\vx_i) \tag{multivariate chain rule} \\ 
              &= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(\hat{y}_i, y_i)}{d\hat{y}} \nabla \vx_i \tag{gradient of a linear function} \\ 
              &= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(\bracket{\vw, \vx_i}, y_i)}{d\hat{y}} \vx_i \tag{$\hat{y}_i = \bracket{\vw, \vx_i}$} \\ 
\end{align}
$$

The good news here is that for linear models, we don't actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form $\frac{d\ell(\hat{y}_i, y_i)}{d\hat{y}}$ and then plug in $\hat{y}_i = \bracket{\vw, \vx_i}$. 

Let's do an example with the logistic loss: 

$$\ell(\hat{y}, y) = -y \log \sigma(\hat{y}) - (1-y)\log (1-\sigma(\hat{y}))\;.$$

A useful fact to know about the logistic sigmoid function $\sigma$ is that $\frac{d\sigma(\hat{y}) }{d\hat{y}} = \sigma(\hat{y}) (1 - \sigma(\hat{y}))$. So, using that and the chain rule, the derivative we need is 

$$
\begin{align}
\frac{d\ell(\hat{y}, y)}{d\hat{y}} &= -y \frac{1}{\sigma(\hat{y})}\frac{d\sigma(\hat{y}) }{d\hat{y}} - (1-y)\frac{1}{1-\sigma(\hat{y})}\left(- \frac{d\sigma(\hat{y}) }{d\hat{y}}\right) \\ 
&= -y \frac{1}{\sigma(\hat{y})}\sigma(\hat{y}) (1 - \sigma(\hat{y})) - (1-y)\frac{1}{1-\sigma(\hat{y})}\left(- \sigma(\hat{y}) (1 - \sigma(\hat{y}))\right) \\ 
&= -y (1 - \sigma(\hat{y})) + (1-y)\sigma(\hat{y}) \\ 
&= \sigma(\hat{y}) - y\;.
\end{align}
$$
Finally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression: 

$$
\begin{align}
\nabla L(\vw) &= \frac{1}{n} \sum_{i = 1}^n (\sigma(\hat{y}_i) - y_i)\vx_i \\ 
              &=\frac{1}{n} \sum_{i = 1}^n (\sigma(\bracket{\vw, \vx_i}) - y_i)\vx_i\;.
\end{align}
$$

So, we can do logistic regression by choosing a learning rate and iterating the update $\vw^{(t+1)} \gets \vw^{(t)} - \alpha \nabla L(\vw^{(t)})$ until convergence. 

Let's see this in action. Here's a data set: 

```{python}
#| fig-cap: Note that this data is **not** linearly separable. The perceptron algorithm wouldn't even have converged for this data set, but logistic regression will do great. 
from matplotlib import pyplot as plt
from sklearn.datasets import make_blobs

p_features = 3

X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])

fig = plt.scatter(X[:,0], X[:,1], c = y)
xlab = plt.xlabel("Feature 1")
ylab = plt.ylabel("Feature 2")
```

The code below is very similar to the code from last time, but I'm going to first transform the feature matrix $\mX$ so that it contains a column of constant features. This is going to make our mathematical life a lot easier. 


```{python}
#| code-fold: false
import numpy as np 
from scipy.optimize import minimize
np.seterr(all='ignore') 

# add a constant feature to the feature matrix
X_ = np.append(X, np.ones((X.shape[0], 1)), 1)

def predict(X, w):
    return X@w

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_loss(y_hat, y): 
    return -y*np.log(sigmoid(y_hat)) - (1-y)*np.log(1-sigmoid(y_hat))

def empirical_risk(X, y, loss, w):
    y_hat = predict(X, w)
    return loss(y_hat, y).mean()
```

I'll start by picking a random parameter vector, visualizing the corresponding line, and computing the loss:  

```{python}
#| code-fold: false

np.random.seed(123)

# pick a random weight vector and calculate the loss
w = .5 - np.random.rand(p_features)
loss = empirical_risk(X_, y, logistic_loss, w)

fig = plt.scatter(X[:,0], X[:,1], c = y)
xlab = plt.xlabel("Feature 1")
ylab = plt.ylabel("Feature 2")

f1 = np.linspace(-3, 3, 101)

p = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = "black")
title = plt.gca().set_title(f"Loss = {loss}")
```

It can be hard to put these kinds of numbers in context, but this is a pretty poor value of the loss. You could probably guess that considering how bad the classifier line looks. 

Now let's go ahead and use gradient descent to compute a better value of the parameter vector $\tilde{\vw}$. I've shown you the main loop of the algorithm, but not the calculation of the gradient. It'll be up to you to implement the gradient (as well as variations of gradient descent) in an upcoming blog post. 

```{python}

#| code-fold: false

from hidden.logistic import gradient

alpha = .001 # learning rate

done = False       # initialize for while loop
prev_loss = np.inf # handy way to start off the loss

# main loop
while not done: 
    w -= alpha*gradient(w, X_, y)                      # gradient step
    new_loss = empirical_risk(X_, y, logistic_loss, w) # compute loss
    
    # check if loss hasn't changed and terminate if so
    if np.isclose(new_loss, prev_loss):          
        done = True
    else:
        prev_loss = new_loss
```

Now we can visualize the resulting classifier and check the value of the loss that it achieves. 

```{python}

loss = empirical_risk(X_, y, logistic_loss, w)

fig = plt.scatter(X[:,0], X[:,1], c = y)
xlab = plt.xlabel("Feature 1")
ylab = plt.ylabel("Feature 2")

f1 = np.linspace(-3, 3, 101)

p = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = "black")
title = plt.gca().set_title(f"Loss = {loss}")
```

That looks better! Note that the data is not linearly separable, but our algorithm still converged to a reasonable solution. 


