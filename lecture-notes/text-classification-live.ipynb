{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  Text Classification and Word Embedding\n",
        "author: Phil Chodrow\n",
        "bibliography: ../refs.bib\n",
        "format: \n",
        "  html: \n",
        "    code-fold: false\n",
        "    cache: true\n",
        "    callout-appearance: minimal\n",
        "    cap-location: margin\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/text-classification.ipynb\" target=\"_parent\">Open these notes in Google Colab</a>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/text-classification-live.ipynb\" target=\"_parent\">Open the live version in Google Colab</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYn6e_yQkcs"
      },
      "source": [
        "\n",
        "*Major components of this set of lecture notes are based on the [Text Classification](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) tutorial from the PyTorch documentation*. \n",
        "\n",
        "## Deep Text Classification and Word Embedding\n",
        "\n",
        "In this set of notes, we'll discuss the problem of *text classification*. Text classification is a common problem in which we aim to classify pieces of text into different categories. These categories might be about:\n",
        "\n",
        "- **Subject matter**: is this news article about news, fashion, finance?\n",
        "- **Emotional valence**: is this tweet happy or sad? Excited or calm? This particular class of questions is so important that it has its own name: sentiment analysis.\n",
        "- **Automated content moderation**: is this Facebook comment a possible instance of abuse or harassment? Is this Reddit thread promoting violence? Is this email spam?\n",
        "\n",
        "We saw text classification previously when we first considered the problem of vectorizing pieces of text. We are now going to look at a somewhat more contemporary approach to text using *word embeddings*. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ehlyVuF5k9Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# for embedding visualization later\n",
        "import plotly.express as px \n",
        "import plotly.io as pio\n",
        "\n",
        "# for VSCode plotly rendering\n",
        "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
        "\n",
        "# for appearance\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4-XX3OeRqDy"
      },
      "source": [
        "For this example, we are going to use a data set containing headlines from a large number of different news articles on the website [HuffPost](https://www.huffpost.com/). I retrieved this data [from Kaggle](https://www.kaggle.com/rmisra/news-category-dataset). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBL5qxsfRjNU"
      },
      "outputs": [],
      "source": [
        "# access the data\n",
        "url = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/news/News_Category_Dataset_v2.json\"\n",
        "df  = pd.read_json(url, lines=True)\n",
        "df  = df[[\"category\", \"headline\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9HZozyjRtJS"
      },
      "source": [
        "There are over 200,000 headlines listed here, along with the category in which they appeared on the website.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oPXnkwyg6Isx",
        "outputId": "c8fa6b07-8747-4891-b540-2fc6b05282ab"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2ra_NceRvjX"
      },
      "source": [
        "Our task will be to teach an algorithm to classify headlines by predicting the category based on the text of the headline. \n",
        "\n",
        "Training a model on this much text data can require a lot of time, so we are going to simplify the problem a little bit, by reducing the number of categories. Let's take a look at which categories we have: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNYPjmAt6URy",
        "outputId": "d7ec91ee-32e5-4e2c-f792-89c29f9a1689"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"category\").size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Q7aVwoR0Mj"
      },
      "source": [
        "Some of these categories are a little odd:\n",
        "\n",
        "- \"Women\"? \n",
        "- \"Weird News\"? \n",
        "- What's the difference between \"Style,\" \"Style & Beauty,\" and \"Taste\"? ). \n",
        "- \"Parenting\" vs. \"Parents\"? \n",
        "- Etc?...\n",
        "\n",
        "Well, there are definitely some questions here! Let's just choose a few categories, and discard the rest. We're going to give each of the categories an integer that we'll use to encode the category in the target variable. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fDALcby56ZTm",
        "outputId": "e8ce4efc-1c46-4c9f-a282-1293f662bc99"
      },
      "outputs": [],
      "source": [
        "categories = {\n",
        "    \"STYLE\"   : 0,\n",
        "    \"SCIENCE\" : 1, \n",
        "    \"TECH\" : 2\n",
        "}\n",
        "\n",
        "df = df[df[\"category\"].apply(lambda x: x in categories.keys())]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "DPzCb-2F6cgo",
        "outputId": "cfd748f8-aeb6-4cfc-ec8f-864ee5c0643d"
      },
      "outputs": [],
      "source": [
        "df[\"category\"] = df[\"category\"].apply(categories.get)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw7SoCFfSPFS"
      },
      "source": [
        "Next we need to wrap this Pandas dataframe as a Torch data set. While we've been using pre-implemented Torch classes for things like directories of images, in this case it's not so hard to just implement our own Dataset. We just need to implement `__getitem__()` to return the appropriate row of the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVBnABQj-JK8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB1dLvKTSt5_"
      },
      "source": [
        "Now let's perform a train-validation split and make Datasets from each one. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n2sL27cTGmH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLvLzWFG_gCK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrvMizMKT7xB"
      },
      "source": [
        "Each element of our data sets is a tuple of text and label: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6PuAee5UMDh",
        "outputId": "3a6af575-8fa6-4fa5-f63f-0ed3b1755559"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EPyRjXrURyi"
      },
      "source": [
        "## Text Vectorization (Again)\n",
        "\n",
        "Now we need to vectorize our text. This time, we're not going to use one-hot encodings. Instead, we are going to treat each sentence as a sequence of words, and identify each word via an integer index. First we'll use a *tokenizer* to split each sentence into individual words: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzscJseA7EoZ",
        "outputId": "f6e3274d-7d15-45f9-f9e4-728aa9c5327d"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "tokenized = tokenizer(train_data[194][0])\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScciv9fVPCh"
      },
      "source": [
        "You might reasonably disagree about whether this is a good tokenization: should punctuation marks be included? Should \"you're\" really have become \"you\", \"'\", and \"re\"? These are excellent questions that we won't discuss too much further right now. \n",
        "\n",
        "We're now ready to build a *vocabulary*. A vocabulary is a mapping from words to integers. The code below loops through the training data and uses it to build such a mapping. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7NlpLEYTYvE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "586ITAVzV8AT"
      },
      "source": [
        "Here are the first couple elements of the vocabulary: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0EG4V74V-Hg",
        "outputId": "447390e1-83f2-4f13-96b7-bc99b3a6b610"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKD0tFt8V4Mg"
      },
      "source": [
        "This vocabulary can be applied on a list of tokens like this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GyZGy0DTtCR",
        "outputId": "7b2ace4d-34a3-457c-a3d5-d0ff4a6aa75c"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "naVpI_owWY1O"
      },
      "source": [
        "# Batch Collation\n",
        "\n",
        "Now we're ready to construct the function that is going to actually pass a batch of data to our training loop. Here are the main steps: \n",
        "\n",
        "1. We pull some feature data (i.e. a batch of headlines). \n",
        "2. We represent each headline as a sequence of integers using the `vocab`. \n",
        "3. We pad the headlines with an unused integer index if necessary so that all headlines have the same length. This index corresponds to \"blank\" or \"no words in this slot.\" \n",
        "4. We return the batch of headlines as a consolidated tensor. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7stGe3YaUVw0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3g5vnO7UZ0p"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_data, batch_size=8, shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3KCgLc6YU3f"
      },
      "source": [
        "Let's take a look at a batch of data now: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1-w7y8oFOgc",
        "outputId": "98e6a698-98d2-45f9-aeba-7b91c12f9619"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LjJtEx-yYdAz"
      },
      "source": [
        "The first element is the list of labels. The second is the concatenated sequence of integers representing 8 headlines worth of text. The final one is the list of offsets that tells us where each of the 8 headlines begins. \n",
        "\n",
        "## Modeling\n",
        "\n",
        "### Word Embedding\n",
        "\n",
        "A *word embedding* refers to a representation of a word in a vector space. Each word is assigned an individual vector. The general aim of a word embedding is to create a representation such that words with related meanings are close to each other in a vector space, while words with different meanings are farther apart. One usually hopes for the *directions* connecting words to be meaningful as well. Here's a nice diagram illustrating some of the general concepts: \n",
        "\n",
        "![](https://miro.medium.com/max/1838/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
        "\n",
        "*Image credit: [Towards Data Science](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)*\n",
        "\n",
        "Word embeddings are often produced as intermediate stages in many machine learning algorithms. In our case, we're going to add an embedding layer at the very base of our model. We'll allow the user to flexibly specify the number of dimensions. \n",
        "\n",
        "We'll typically expect pretty low-dimensional embeddings for this lecture, but state-of-the-art embeddings will typically have a much higher number of dimensions. For example, the [Embedding Projector demo](http://projector.tensorflow.org/) supplied by TensorFlow uses a default dimension of 200. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzqYh85bbcV6"
      },
      "source": [
        "Let's learn and train a model! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC4S2B1lVCJE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIyrmwb-VJvb"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(dataloader):\n",
        "    epoch_start_time = time.time()\n",
        "    # keep track of some counts for measuring accuracy\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 300\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # form prediction on batch\n",
        "        predicted_label = model(text)\n",
        "        # evaluate loss on prediction\n",
        "        loss = loss_fn(predicted_label, label)\n",
        "        # compute gradient\n",
        "        loss.backward()\n",
        "        # take an optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        # for printing accuracy\n",
        "        total_acc   += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        \n",
        "    print(f'| epoch {epoch:3d} | train accuracy {total_acc/total_count:8.3f} | time: {time.time() - epoch_start_time:5.2f}s')\n",
        "    # print('| end of epoch {:3d} | time: {:5.2f}s | '.format(epoch,\n",
        "    #                                        time.time() - epoch_start_time))\n",
        "    \n",
        "def evaluate(dataloader):\n",
        "\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predicted_label = model(text)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ9VNwzFVKmv",
        "outputId": "c7efbe25-d0a2-4225-d814-e1af7bd4ffaf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt44xzYvbwmG",
        "outputId": "9be492a0-41ab-4d49-ceff-34ebb268b3ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRPsk1JJeskR"
      },
      "source": [
        "Our accuracy on validation data is much lower than what we achieved on the training data. This is a possible sign of overfitting. Regardless, this predictive performance is much better than what we would have achieved by guesswork: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXHyxIMtevX1",
        "outputId": "932e7818-6d7e-4c20-c02c-75edeeb5e340"
      },
      "outputs": [],
      "source": [
        "df_train.groupby(\"category\").size() / len(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRmd06Ffe-4J"
      },
      "source": [
        "## Inspecting Word Embeddings\n",
        "\n",
        "Recall from our discussion of image classification that the intermediate layers learned by the model can help us understand the representations that the model uses to construct its final outputs. In the case of word embeddings, we can simply extract this matrix from the corresponding layer of the model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW32MQ1PFtk3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBLDZVqyhpei"
      },
      "source": [
        "Let's also extract the words from our vocabular: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4xqf15nGlSW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtGHSZoBhtkX"
      },
      "source": [
        "The weight matrix itself has 16 columns, which is too many for us to conveniently visualize. So, instead we are going to use our friend PCA to extract a 2-dimensional representation that we can plot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdG3cJF1HNIl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj_EJzYZh2eo"
      },
      "source": [
        "We'll use the Plotly package to do the plotting. Plotly works best with dataframes: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "-LAMah-AHcbt",
        "outputId": "eaed0eb6-cbaa-4877-ff06-fc1b60721359"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la_zN3B0h7Ry"
      },
      "source": [
        "And, let's plot! We've used Plotly for the interactivity: hover over a dot to see the word it corresponds to. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "b0yeW65qHkFY",
        "outputId": "4ea01cb9-bd75-4065-f79f-89d095fa4537"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(embedding_df, \n",
        "                 x = \"x0\", \n",
        "                 y = \"x1\", \n",
        "                 size = list(np.ones(len(embedding_df))),\n",
        "                 size_max = 10,\n",
        "                 hover_name = \"word\")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guz5RCNEiOsO"
      },
      "source": [
        "We've made an embedding! We might notice that this embedding appears to be a little bit \"stretched out\" in three main directions. Each one corresponds to one of the three classes in our training data. \n",
        "\n",
        "## Bias in Text Embeddings\n",
        "\n",
        "Whenever we create a machine learning model that might conceivably have impact on the thoughts or actions of human beings, we have a responsibility to understand the limitations and biases of that model. Biases can enter into machine learning models through several routes, including the data used as well as choices made by the modeler along the way. For example, in our case: \n",
        "\n",
        "1. **Data**: we used data from a popular news source. \n",
        "2. **Modeler choice**: we only used data corresponding to a certain subset of labels. \n",
        "\n",
        "With these considerations in mind, let's see what kinds of words our model associates with female and male genders. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4iRk0XcI3aZ"
      },
      "outputs": [],
      "source": [
        "feminine = [\"she\", \"her\", \"woman\"]\n",
        "masculine = [\"he\", \"him\", \"man\"]\n",
        "\n",
        "highlight_1 = [\"strong\", \"powerful\", \"smart\",     \"thinking\", \"brave\", \"muscle\"]\n",
        "highlight_2 = [\"hot\",    \"sexy\",     \"beautiful\", \"shopping\", \"children\", \"thin\"]\n",
        "\n",
        "def gender_mapper(x):\n",
        "    if x in feminine:\n",
        "        return 1\n",
        "    elif x in masculine:\n",
        "        return 4\n",
        "    elif x in highlight_1:\n",
        "        return 3\n",
        "    elif x in highlight_2:\n",
        "        return 2\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "embedding_df[\"highlight\"] = embedding_df[\"word\"].apply(gender_mapper)\n",
        "embedding_df[\"size\"]      = np.array(1.0 + 50*(embedding_df[\"highlight\"] > 0))\n",
        "\n",
        "# \n",
        "sub_df = embedding_df[embedding_df[\"highlight\"] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "nCn5ohur83eY",
        "outputId": "f92609a5-b848-4067-bc4b-2d815b5d037e"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px \n",
        "\n",
        "fig = px.scatter(sub_df, \n",
        "                 x = \"x0\", \n",
        "                 y = \"x1\", \n",
        "                 color = \"highlight\",\n",
        "                 size = list(sub_df[\"size\"]),\n",
        "                 size_max = 10,\n",
        "                 hover_name = \"word\", \n",
        "                 text = \"word\")\n",
        "\n",
        "fig.update_traces(textposition='top center')\n",
        "\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ--iXLFiwaJ"
      },
      "source": [
        "Our text classification model's word embedding is unambiguously sexist. \n",
        "\n",
        "- Words like \"hot\", \"sexy\", and \"shopping\" are more closely located to feminine words like \"she\", \"her\", and \"woman\".\n",
        "- Words like \"strong\", \"smart\", and \"thinking\" are more closely located to masculine words like \"he\", \"him\", and \"man\". \n",
        "\n",
        "Where did these biases come from? \n",
        "\n",
        "- The primary source is the data itself: HuffPost headlines in certain categories can be highly gendered, and the \"Style\" category is an example of this. \n",
        "- A secondary source is the choices that I made as a modeler. In particular, I intentionally chose categories that would emphasize biases in the data and make them easy to visualize. \n",
        "\n",
        "While I could have made different choices and obtained different results, this episode highlights a fundamental set of questions usually underexamined in contemporary machine learning: \n",
        "\n",
        "- What biases are built into my data source? \n",
        "- How do my choices about which data to use influence the biases present in my model? \n",
        "\n",
        "For more on the topic of bias in language models, you may wish to read the now-infamous paper by Emily Bender, Angelina McMillan-Major, Timnt Gebru, and \"Shmargaret Shmitchell\" (Margaret Mitchell), \"[On the Dangers of Stochastic Parrots](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf).\" This is the paper that ultimately led to the firing of the final two authors by Google in late 2020 and early 2021. \n",
        "\n",
        "Here's a very recent example (from Margaret Mitchell) illustrating gender bias in ChatGPT: \n",
        "\n",
        "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I replicated this (my screenshot below).<br>Really great example of gender bias, for those of you who need a canonical example to make the point. <a href=\"https://t.co/O1A8Tk7oI1\">https://t.co/O1A8Tk7oI1</a> <a href=\"https://t.co/hKt4HSBzh3\">pic.twitter.com/hKt4HSBzh3</a></p>&mdash; MMitchell (@mmitchell_ai) <a href=\"https://twitter.com/mmitchell_ai/status/1650110045781393410?ref_src=twsrc%5Etfw\">April 23, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNqg/CAncHESRiKP7ztpmQi",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
