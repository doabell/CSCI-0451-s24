{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Penguin Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "target_ix_dict = {\n",
        "  \"Adelie Penguin (Pygoscelis adeliae)\" : 0,\n",
        "  \"Chinstrap penguin (Pygoscelis antarctica)\" : 1,\n",
        "  \"Gentoo penguin (Pygoscelis papua)\" : 2\n",
        "}\n",
        "\n",
        "class PenguinsDataset(Dataset):\n",
        "  def __init__(self, train = True):\n",
        "    if train: \n",
        "      url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\n",
        "    else:\n",
        "      url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\n",
        "\n",
        "    df = pd.read_csv(url)\n",
        "    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n",
        "    df = df[df[\"Sex\"] != \".\"]\n",
        "    df = pd.get_dummies(df, columns = [\n",
        "      \"Island\", \"Stage\", \"Clutch Completion\", \"Sex\"\n",
        "    ])\n",
        "    df = df.dropna()\n",
        "\n",
        "    self.df = df\n",
        "    self.transform = lambda x: torch.tensor(x, dtype = torch.float32)\n",
        "    self.target_ix = lambda x: target_ix_dict[x]\n",
        "    self.target_transform = lambda x: torch.zeros(\n",
        "    3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(x), value=1)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    features = self.df.drop([\"Species\"], axis = 1).iloc[idx,:]\n",
        "    label    = self.df.iloc[idx,:][\"Species\"]\n",
        "    features = self.transform(features)\n",
        "    \n",
        "    label    = self.target_ix(label)\n",
        "    label    = self.target_transform(label)\n",
        "\n",
        "    features = features.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    return features, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "  device = \"cpu\"\n",
        "elif torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are our data sets. `PenguinsDataset` is a custom class that I implemented above, while `DataLoader` is a utility from `torch` that automatically handles things like batching and randomization for gradient descent. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "train = PenguinsDataset()\n",
        "train_dataloader = DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "val = PenguinsDataset(False)\n",
        "val_dataloader = DataLoader(val, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we're ready to define our model. To see how we do things in torch, let's start by implementing logistic regression. To start, we need to create a predictor model that does the prediction step of logistic regression. If you'll recall, this is nothing more than matrix multiplication. The `nn.Linear` \"layer\" supplied by `torch` implements this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class penguinLogistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear = nn.Sequential(\n",
        "      nn.Linear(14, 3) # (number of features, number of class labels)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can optimize this model using the cross-entropy loss and the Adam optimizer. As you'll recall, logistic regression is nothing more than matrix multiplication plus the cross-entropy loss, so this is a logistic regression model! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = penguinLogistic()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to actually perform the optimization. The `torch` package gives us a lot of control over exactly how this happens, and we'll go over the details in a future lecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#| code-fold: true\n",
        "\n",
        "def training_loop(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "\n",
        "  train_size = len(train_dataloader.dataset)\n",
        "  val_size = len(val_dataloader.dataset)\n",
        "\n",
        "  train_loss_history = []\n",
        "  train_acc_history  = []\n",
        "  val_loss_history   = []\n",
        "  val_acc_history    = []\n",
        "\n",
        "  for t in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc  = 0.0\n",
        "    val_loss   = 0.0\n",
        "    val_acc    = 0.0\n",
        "\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      pred = model(X)\n",
        "      fit = loss_fn(pred, y)\n",
        "      train_loss += fit.item() / train_size\n",
        "      \n",
        "      train_acc += (pred.argmax(dim = 1) == y.argmax(dim = 1)).sum() / train_size\n",
        "      \n",
        "      # Backpropagation\n",
        "      \n",
        "      fit.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_loss_history += [train_loss]\n",
        "    train_acc_history  += [train_acc]\n",
        "\n",
        "    for batch, (X, y) in enumerate(val_dataloader):\n",
        "      with torch.no_grad():\n",
        "        pred = model(X)\n",
        "        fit = loss_fn(pred, y)\n",
        "        val_loss += fit.item() / val_size\n",
        "        val_acc += (pred.argmax(dim = 1) == y.argmax(dim = 1)).sum() / val_size\n",
        "\n",
        "    val_loss_history += [val_loss]\n",
        "    val_acc_history  += [val_acc]\n",
        "\n",
        "    if t % 50 == 0:\n",
        "      print(f\"epoch {t}: val_loss = {val_loss}, val_accuracy = {val_acc}\")\n",
        "    \n",
        "  return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_histories(tlh, tah, vlh, vah):\n",
        "\n",
        "  fig, axarr = plt.subplots(1, 2, figsize = (6, 3))\n",
        "\n",
        "  axarr[0].plot(tlh, label = \"train\")\n",
        "  axarr[0].plot(vlh, label = \"validation\")\n",
        "  axarr[0].legend()\n",
        "  axarr[0].set(xlabel = \"epoch\", title = \"loss\")\n",
        "  axarr[0].semilogy()\n",
        "\n",
        "  axarr[1].plot(tah, label = \"train\")\n",
        "  axarr[1].plot(vah, label = \"validation\")\n",
        "  axarr[1].legend()\n",
        "  axarr[1].set(xlabel = \"epoch\", title = \"accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go ahead and train! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tlh, tah, vlh, vah = training_loop(model, train_dataloader, val_dataloader, 1e-4, 500)\n",
        "plot_histories(tlh, tah, vlh, vah)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our model is doing much better than we would expect from random guessing on this problem, although it may not be competitive with many of the models that you implemented in your analysis of the penguins data set. Further training or tweaks to parameters like batch sizes and learning rates could potentially help improve performance here. \n",
        "\n",
        "Let's try adding a hidden layer, as in @eq-single-layer. To do this, we need to add a nonlinearity $\\alpha$ and more `Linear` layers. The important points are: \n",
        "\n",
        "1. The first dimension of the first linear layer needs to match the number of features of the input. \n",
        "2. The final dimension of the last linear layer needs to match the number of possible labels. \n",
        "3. The final dimension of each linear layer needs to match the first dimension of the next layer. \n",
        "\n",
        "These rules follow directly from the need to make all the matrix multiplications turn out right. \n",
        "\n",
        "So, let's try a model with a single layer of 100 hidden units: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class penguinNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(14, 100), # U_1\n",
        "          nn.ReLU(),          # common choice of alpha these days\n",
        "          nn.Linear(100, 3)   # W\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_relu_stack(x)\n",
        "\n",
        "model = penguinNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can optimize it using the same approach as before: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "tlh, tah, vlh, vah = training_loop(model, train_dataloader, val_dataloader, 1e-4, 500)\n",
        "plot_histories(tlh, tah, vlh, vah)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model has also done well on this task, although still not closer to perfect accuracy. It's also interesting to note that the model's loss and accuracy both vary quite wildly during the training process. This is not uncommon, but may also point to a possible benefit of using a smaller step-size. \n",
        "\n",
        "It might appear that we haven't really gained much from the massive costs of mathematical structure and computational architecture -- we could have done better with just tools from `sklearn`, or even our own hand-built implementations! We'll soon see the infrastructure working to our advantage when we get to problems involving large data sets with complex structure, like text and images. \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
