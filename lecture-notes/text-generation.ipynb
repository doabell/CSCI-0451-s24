{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  Text Generation\n",
        "author: Phil Chodrow\n",
        "bibliography: ../refs.bib\n",
        "format: \n",
        "  html: \n",
        "    code-fold: false\n",
        "    cache: true\n",
        "    callout-appearance: minimal\n",
        "    cap-location: margin\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/text-classification.ipynb\" target=\"_parent\">Open these notes in Google Colab</a>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/text-classification-live.ipynb\" target=\"_parent\">Open the live version in Google Colab</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYn6e_yQkcs"
      },
      "source": [
        "\n",
        "*Major components of this set of lecture notes are based on the [Text Classification](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) tutorial from the PyTorch documentation*. \n",
        "\n",
        "## Deep Text Classification and Word Embedding\n",
        "\n",
        "In this set of notes, we'll discuss the problem of *text classification*. Text classification is a common problem in which we aim to classify pieces of text into different categories. These categories might be about:\n",
        "\n",
        "- **Subject matter**: is this news article about news, fashion, finance?\n",
        "- **Emotional valence**: is this tweet happy or sad? Excited or calm? This particular class of questions is so important that it has its own name: sentiment analysis.\n",
        "- **Automated content moderation**: is this Facebook comment a possible instance of abuse or harassment? Is this Reddit thread promoting violence? Is this email spam?\n",
        "\n",
        "We saw text classification previously when we first considered the problem of vectorizing pieces of text. We are now going to look at a somewhat more contemporary approach to text using *word embeddings*. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 705,
      "metadata": {
        "id": "6ehlyVuF5k9Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchsummary import summary\n",
        "\n",
        "# for embedding visualization later\n",
        "import plotly.express as px \n",
        "import plotly.io as pio\n",
        "\n",
        "# for VSCode plotly rendering\n",
        "# pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
        "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
        "\n",
        "pio.templates.default = \"plotly_white\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import spacy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Our Task\n",
        "\n",
        "Today, we are going to see whether we can teach an algorithm to understand and reproduce the pinnacle of cultural achievement; the benchmark against which all art is to be judged; the mirror that reveals to humany its truest self. I speak, of course, of *Star Trek: Deep Space Nine.*\n",
        "\n",
        "<figure class=\"image\" style=\"width:300px\">\n",
        "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/DS9.jpg\" alt=\"\">\n",
        "  <figcaption><i></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "In particular, we are going to attempt to teach a neural  network to generate *episode scripts*. This a text generation task: after training, our hope is that our model will be able to create scripts that are reasonably realistic in their appearance. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 780,
      "metadata": {},
      "outputs": [],
      "source": [
        "## miscellaneous data cleaning\n",
        "\n",
        "start_episode = 20 # Start in Season 2, Season 1 is not very good\n",
        "num_episodes = 5  # only pick this many episodes to train on\n",
        "\n",
        "url = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\n",
        "star_trek_scripts = pd.read_json(url)\n",
        "\n",
        "cleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\n",
        "cleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\n",
        "text = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\n",
        "for char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n",
        "    text = text.replace(char, \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 781,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Last\n",
            "time on Deep Space Nine.  \n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call\n",
            "themselves the Circle. \n",
            "O'BRIEN: What gives them the right to mess up our station? \n",
            "ODO: They're an extremist faction who believe in Bajor for the\n",
            "Bajorans. \n",
            "SISKO: I can't loan you a Starfleet runabout without knowing where you\n",
            "plan on taking it. \n",
            "KIRA: To Cardassia Four to rescue a Bajoran prisoner of war. \n",
            "(The prisoners are rescued.) \n",
            "KIRA: Come on. We have a ship waiting. \n",
            "JARO: What you \n"
          ]
        }
      ],
      "source": [
        "print(text[0:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 782,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    L = [s.split() for s in text.split(\"\\n\")]\n",
        "    # return [w for l in L for w in l]\n",
        "    out = L[0]\n",
        "    for i in range(1, len(L)):\n",
        "        out += [\"\\n\"]\n",
        "        out += L[i] \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 783,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 783,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer(\"Last \\n time on Deep Space Nine. \\n SISKO: This\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 784,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 'is')"
            ]
          },
          "execution_count": 784,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "WINDOW = 10 # predict next word from 10 previous words\n",
        "word_seq = tokenizer(text)\n",
        "\n",
        "predictors = []\n",
        "targets    = []\n",
        "\n",
        "for i in range(len(word_seq) - WINDOW - 1):\n",
        "    predictors.append(\" \".join(word_seq[i:(i+WINDOW)]))\n",
        "    targets.append(word_seq[WINDOW+i])\n",
        "    \n",
        "i = 0\n",
        "len(tokenizer(predictors[0])), targets[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 785,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataSet(Dataset):\n",
        "    def __init__(self, predictors, targets):\n",
        "        self.predictors = predictors\n",
        "        self.targets = targets\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.predictors[index], self.targets[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 786,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = TextDataSet(predictors, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 787,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "def yield_tokens(data_iter):\n",
        "    for text, word in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(data), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 788,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 788,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer(next(iter(data))[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 789,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define data loader\n",
        "\n",
        "def collate_batch(batch):\n",
        "    text_list, next_word_list = [], []\n",
        "    for (text, next_word) in batch:\n",
        "        processed_text = vocab(tokenizer(text))\n",
        "        text_list.append(processed_text)\n",
        "        next_word_list.append(vocab([next_word]))\n",
        "    next_word_list = torch.tensor(next_word_list, dtype=torch.int64).squeeze()\n",
        "    text_list = torch.tensor(text_list)\n",
        "    return text_list.to(device), next_word_list.to(device)\n",
        "\n",
        "data_loader = DataLoader(data, batch_size=8, shuffle=False, collate_fn=collate_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 837,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_list, next_word_list = next(iter(data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 838,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3730"
            ]
          },
          "execution_count": 838,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 866,
      "metadata": {},
      "outputs": [],
      "source": [
        "# word embedding + LSTM?? feels like a lot, may need to adjust sequence size...\n",
        "# or pivot to letters\n",
        "from torch import nn\n",
        "\n",
        "class TextGenModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, window):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim*window, hidden_size = 100, num_layers = 1, batch_first = True)\n",
        "        self.fc   = nn.Linear(100, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x, (hn, cn) = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        return(x)\n",
        "        \n",
        "EMBEDDING_DIM = 5\n",
        "TGM = TextGenModel(len(vocab), EMBEDDING_DIM, WINDOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 867,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(8.5928, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 867,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = next(iter(data_loader))\n",
        "loss_fn(TGM(X), y.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 868,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "def train(dataloader):\n",
        "    # keep track of some counts for measuring accuracy\n",
        "    total_acc, total_count, total_loss = 0, 0, 0\n",
        "    log_interval = 1000\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (text_seq, next_word) in enumerate(dataloader):\n",
        "\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # form prediction on batch\n",
        "        preds = TGM(text_seq)\n",
        "        # evaluate loss on prediction\n",
        "        loss = loss_fn(preds, next_word)\n",
        "        # compute gradient\n",
        "        loss.backward()\n",
        "        # take an optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        # for printing accuracy\n",
        "        total_acc   += (preds.argmax(1) == next_word).sum().item()\n",
        "        total_count += next_word.size(0)\n",
        "        total_loss  += loss.item() \n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| train loss {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_loss/total_count))\n",
        "            total_acc, total_loss, total_count = 0, 0, 0\n",
        "            start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 880,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |  1000/ 3730 batches | train loss    0.610\n",
            "| epoch   1 |  2000/ 3730 batches | train loss    0.663\n",
            "| epoch   1 |  3000/ 3730 batches | train loss    0.684\n",
            "| end of epoch   1 | time: 46.31s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   2 |  1000/ 3730 batches | train loss    0.594\n",
            "| epoch   2 |  2000/ 3730 batches | train loss    0.647\n",
            "| epoch   2 |  3000/ 3730 batches | train loss    0.669\n",
            "| end of epoch   2 | time: 48.70s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   3 |  1000/ 3730 batches | train loss    0.588\n",
            "| epoch   3 |  2000/ 3730 batches | train loss    0.637\n",
            "| epoch   3 |  3000/ 3730 batches | train loss    0.658\n",
            "| end of epoch   3 | time: 45.57s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   4 |  1000/ 3730 batches | train loss    0.581\n",
            "| epoch   4 |  2000/ 3730 batches | train loss    0.627\n",
            "| epoch   4 |  3000/ 3730 batches | train loss    0.647\n",
            "| end of epoch   4 | time: 43.17s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   5 |  1000/ 3730 batches | train loss    0.575\n",
            "| epoch   5 |  2000/ 3730 batches | train loss    0.618\n",
            "| epoch   5 |  3000/ 3730 batches | train loss    0.638\n",
            "| end of epoch   5 | time: 41.74s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   6 |  1000/ 3730 batches | train loss    0.570\n",
            "| epoch   6 |  2000/ 3730 batches | train loss    0.610\n",
            "| epoch   6 |  3000/ 3730 batches | train loss    0.629\n",
            "| end of epoch   6 | time: 36.30s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   7 |  1000/ 3730 batches | train loss    0.563\n",
            "| epoch   7 |  2000/ 3730 batches | train loss    0.602\n",
            "| epoch   7 |  3000/ 3730 batches | train loss    0.621\n",
            "| end of epoch   7 | time: 36.33s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   8 |  1000/ 3730 batches | train loss    0.558\n",
            "| epoch   8 |  2000/ 3730 batches | train loss    0.594\n",
            "| epoch   8 |  3000/ 3730 batches | train loss    0.613\n",
            "| end of epoch   8 | time: 37.65s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch   9 |  1000/ 3730 batches | train loss    0.552\n",
            "| epoch   9 |  2000/ 3730 batches | train loss    0.586\n",
            "| epoch   9 |  3000/ 3730 batches | train loss    0.605\n",
            "| end of epoch   9 | time: 45.10s | \n",
            "-----------------------------------------------------------------\n",
            "| epoch  10 |  1000/ 3730 batches | train loss    0.546\n",
            "| epoch  10 |  2000/ 3730 batches | train loss    0.579\n",
            "| epoch  10 |  3000/ 3730 batches | train loss    0.598\n",
            "| end of epoch  10 | time: 44.17s | \n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(TGM.parameters(), lr=0.0001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(data_loader)\n",
        "    \n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '.format(epoch,\n",
        "                                           time.time() - epoch_start_time))\n",
        "    print('-' * 65)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 881,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preds = TGM(X[[0],:]).flatten()\n",
        "\n",
        "all_words = vocab.get_itos()\n",
        "\n",
        "def sample_from_preds(preds, temp = 1):\n",
        "    probs = nn.Softmax(dim=0)(1/temp*preds)\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(probs, 1)\n",
        "    new_idx = next(iter(sampler))\n",
        "    return new_idx\n",
        "\n",
        "def sample_next_word(text, temp = 1, window = 10):\n",
        "    token_ix = vocab(tokenizer(text)[-window:])\n",
        "    # return token_ix\n",
        "    X = torch.tensor([token_ix], dtype = torch.int64)\n",
        "    # return X\n",
        "    preds = TGM(X).flatten()\n",
        "    new_ix = sample_from_preds(preds, temp)\n",
        "    return all_words[new_ix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 888,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last time on Deep Space Nine. \n",
            " SISKO: This is the\n",
            "--------------------------------------------------\n",
            "Last time on Deep Space Nine. \n",
            " SISKO: This is the emblem \n",
            " BASHIR: \n",
            " BASHIR: I don't \n",
            " BASHIR: \n",
            " BASHIR: I'm, \n",
            " MELORA: I \n",
            " BASHIR: I don't the think \n",
            " BASHIR: [on \n",
            " SISKO: I don't you \n",
            " BASHIR: I don't \n",
            " BASHIR: I \n",
            " O'BRIEN: I don't \n",
            " BASHIR: I don't to think \n",
            " BASHIR: \n",
            " BASHIR: I don't you \n",
            " BASHIR: I don't \n",
            " BASHIR: I \n",
            " MELORA: I don't you \n",
            " BASHIR: I don't \n",
            " BASHIR: I \n",
            " MELORA: I prophecies \n",
            " BASHIR: I don't to think \n",
            " BASHIR: I \n",
            " MELORA: I \n",
            " BASHIR: I don't the think \n",
            " BASHIR: [on \n",
            " BASHIR: I don't you \n",
            " BASHIR: I don't you think \n",
            " BASHIR: [on \n",
            " BASHIR: I \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't to your \n",
            " BASHIR: \n",
            " BASHIR: I don't \n",
            " BASHIR: \n",
            " BASHIR: I'm, \n",
            " O'BRIEN: I \n",
            " BASHIR: I don't the think \n",
            " \n",
            " BASHIR: \n",
            " \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't of \n",
            " MELORA: I \n",
            " BASHIR: I don't \n",
            " BASHIR: Well, \n",
            " MELORA: I don't you \n",
            " BASHIR: I don't you job, \n",
            " BASHIR: \n",
            " BASHIR: I don't \n",
            " BASHIR: \n",
            " BASHIR: I'm, \n",
            " MELORA: I \n",
            " BASHIR: I don't \n",
            " BASHIR: I bureaucracy. \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: You \n",
            " BASHIR: I don't the host of \n",
            " SISKO: I don't \n",
            " BASHIR: I \n",
            " MELORA: I prophecies \n",
            " BASHIR: I don't to think \n",
            " BASHIR: [on \n",
            " BASHIR: I \n",
            " BASHIR: I don't the \n",
            " MELORA: I don't \n",
            " BASHIR: I \n",
            " MELORA: I prophecies \n",
            " BASHIR: I don't \n",
            " BASHIR: [OC]: \n",
            " BASHIR: I don't you \n",
            " DAX: I have \n",
            " BASHIR: I \n",
            " MELORA: I prophecies \n",
            " BASHIR: I don't to think me \n",
            " BASHIR: Not \n",
            " BASHIR: You \n",
            " BASHIR: I don't the think of \n",
            " BASHIR: Not \n",
            " BASHIR: I \n",
            " BASHIR: I don't the think that \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't \n",
            " BASHIR: You \n",
            " BASHIR: I don't you \n",
            " BASHIR: I don't you possibly \n",
            " BASHIR: \n",
            " BASHIR: I don't \n",
            " BASHIR: \n",
            " BASHIR: rumbles) \n",
            " MELORA: I \n",
            " BASHIR: I don't the think \n",
            " BASHIR: [on \n",
            " BASHIR: I don't you shoot. \n",
            " \n",
            " BASHIR: I don't you \n",
            " BASHIR: I don't you think a \n",
            " BASHIR: I don't \n",
            " BASHIR: I don't you \n",
            " DAX: I \n",
            " BASHIR: [on \n",
            " BASHIR: \n",
            " BASHIR: I don't \n",
            " SISKO: \n",
            " BASHIR: I don't \n",
            " BASHIR: \n",
            " BASHIR: I don't \n",
            " BASHIR: \n",
            " BASHIR: rumbles) \n",
            " DAX: I \n",
            " BASHIR: I really \n",
            " BASHIR: Well, \n",
            " BASHIR: I don't you \n",
            " BASHIR: I don't \n",
            " BASHIR: I \n",
            " MELORA: I\n"
          ]
        }
      ],
      "source": [
        "seed = 'Last time on Deep Space Nine. \\n SISKO: This is the'\n",
        "\n",
        "def sample_from_model(seed, n_words, temp, window):\n",
        "    text = seed \n",
        "    for i in range(n_words):\n",
        "        word = sample_next_word(text, temp, window)\n",
        "        text += \" \" + word\n",
        "    return seed, text    \n",
        "\n",
        "\n",
        "synth = sample_from_model(seed, 500, .1, 10)\n",
        "\n",
        "\n",
        "print(synth[0])\n",
        "print(\"-\"*50)\n",
        "print(synth[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 727,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '\\n',\n",
              " 'the',\n",
              " 'to',\n",
              " 'I',\n",
              " 'you',\n",
              " 'a',\n",
              " 'of',\n",
              " 'and',\n",
              " 'SISKO:',\n",
              " 'is',\n",
              " 'in',\n",
              " 'that',\n",
              " 'have',\n",
              " 'be',\n",
              " 'for',\n",
              " 'KIRA:',\n",
              " 'it',\n",
              " 'BASHIR:',\n",
              " 'QUARK:',\n",
              " 'on',\n",
              " \"I'm\",\n",
              " 'ODO:',\n",
              " 'your',\n",
              " 'DAX:',\n",
              " \"O'BRIEN:\",\n",
              " 'You',\n",
              " 'was',\n",
              " 'with',\n",
              " 'we',\n",
              " 'my',\n",
              " 'this',\n",
              " 'The',\n",
              " 'are',\n",
              " 'not',\n",
              " \"don't\",\n",
              " 'me',\n",
              " 'do',\n",
              " 'know',\n",
              " 'can',\n",
              " 'about',\n",
              " 'what',\n",
              " 'all',\n",
              " 'but',\n",
              " 'just',\n",
              " 'as',\n",
              " 'at',\n",
              " 'get',\n",
              " 'going',\n",
              " \"It's\",\n",
              " 'you.',\n",
              " 'like',\n",
              " 'he',\n",
              " 'been',\n",
              " 'if',\n",
              " 'one',\n",
              " 'And',\n",
              " 'his',\n",
              " 'want',\n",
              " 'from',\n",
              " 'What',\n",
              " 'it.',\n",
              " 'think',\n",
              " 'out',\n",
              " 'an',\n",
              " 'But',\n",
              " 'will',\n",
              " 'GARAK:',\n",
              " 'were',\n",
              " 'they',\n",
              " 'We',\n",
              " 'me.',\n",
              " 'would',\n",
              " 'It',\n",
              " \"you're\",\n",
              " \"I'll\",\n",
              " 'our',\n",
              " \"I've\",\n",
              " 'If',\n",
              " 'no',\n",
              " 'see',\n",
              " 'up',\n",
              " \"That's\",\n",
              " 'him',\n",
              " 'has',\n",
              " 'some',\n",
              " 'could',\n",
              " 'JAKE:',\n",
              " 'so',\n",
              " 'by',\n",
              " \"it's\",\n",
              " 'never',\n",
              " 'had',\n",
              " 'them',\n",
              " 'He',\n",
              " 'any',\n",
              " 'into',\n",
              " 'us',\n",
              " 'there',\n",
              " 'when',\n",
              " 'her',\n",
              " 'back',\n",
              " 'here',\n",
              " 'Well,',\n",
              " 'who',\n",
              " 'time',\n",
              " 'here.',\n",
              " \"can't\",\n",
              " 'you,',\n",
              " 'how',\n",
              " \"You're\",\n",
              " 'more',\n",
              " 'take',\n",
              " 'A',\n",
              " 'ROM:',\n",
              " 'or',\n",
              " 'got',\n",
              " 'their',\n",
              " 'make',\n",
              " 'should',\n",
              " 'way',\n",
              " 'very',\n",
              " 'This',\n",
              " 'need',\n",
              " 'How',\n",
              " 'did',\n",
              " '[OC]:',\n",
              " 'tell',\n",
              " 'find',\n",
              " '[on',\n",
              " 'only',\n",
              " 'two',\n",
              " 'That',\n",
              " 'something',\n",
              " 'All',\n",
              " 'come',\n",
              " 'me,',\n",
              " \"I'd\",\n",
              " 'she',\n",
              " \"didn't\",\n",
              " 'Cardassian',\n",
              " 'right.',\n",
              " 'sure',\n",
              " 'go',\n",
              " 'than',\n",
              " 'that.',\n",
              " 'They',\n",
              " 'much',\n",
              " 'over',\n",
              " 'you?',\n",
              " 'good',\n",
              " 'still',\n",
              " 'let',\n",
              " 'Oh,',\n",
              " \"Don't\",\n",
              " 'him.',\n",
              " 'Not',\n",
              " 'My',\n",
              " 'DUKAT:',\n",
              " 'No,',\n",
              " 'even',\n",
              " 'say',\n",
              " 'people',\n",
              " 'might',\n",
              " 'really',\n",
              " \"that's\",\n",
              " 'So',\n",
              " 'little',\n",
              " 'right',\n",
              " 'Do',\n",
              " 'Why',\n",
              " 'thought',\n",
              " 'down',\n",
              " 'Then',\n",
              " 'these',\n",
              " 'must',\n",
              " 'other',\n",
              " 'then',\n",
              " 'help',\n",
              " 'NOG:',\n",
              " 'why',\n",
              " 'better',\n",
              " 'Yes,',\n",
              " 'before',\n",
              " 'Is',\n",
              " 'Sisko',\n",
              " 'ever',\n",
              " \"He's\",\n",
              " 'where',\n",
              " 'it?',\n",
              " 'first',\n",
              " 'believe',\n",
              " 'give',\n",
              " 'us.',\n",
              " '(The',\n",
              " 'There',\n",
              " 'them.',\n",
              " 'through',\n",
              " 'know.',\n",
              " \"won't\",\n",
              " 'kind',\n",
              " 'long',\n",
              " 'off',\n",
              " \"There's\",\n",
              " 'told',\n",
              " 'anything',\n",
              " 'too',\n",
              " 'Bajoran',\n",
              " 'Just',\n",
              " 'time.',\n",
              " \"there's\",\n",
              " 'three',\n",
              " 'Are',\n",
              " 'look',\n",
              " 'am',\n",
              " \"you've\",\n",
              " \"doesn't\",\n",
              " 'KEIKO:',\n",
              " 'station',\n",
              " \"We're\",\n",
              " 'always',\n",
              " 'may',\n",
              " 'Commander,',\n",
              " 'ship',\n",
              " 'Maybe',\n",
              " 'WINN:',\n",
              " 'new',\n",
              " 'No.',\n",
              " \"They're\",\n",
              " 'last',\n",
              " 'trying',\n",
              " \"O'Brien\",\n",
              " \"he's\",\n",
              " 'now.',\n",
              " 'made',\n",
              " 'Now',\n",
              " 'thing',\n",
              " 'BAREIL:',\n",
              " 'No',\n",
              " \"isn't\",\n",
              " 'monitor]:',\n",
              " 'She',\n",
              " 'those',\n",
              " \"you'll\",\n",
              " 'few',\n",
              " 'nothing',\n",
              " 'now',\n",
              " 'put',\n",
              " 'In',\n",
              " 'this.',\n",
              " \"we're\",\n",
              " 'Commander.',\n",
              " 'it,',\n",
              " 'another',\n",
              " 'being',\n",
              " 'station.',\n",
              " 'after',\n",
              " 'said',\n",
              " 'Federation',\n",
              " 'security',\n",
              " \"you'd\",\n",
              " 'Starfleet',\n",
              " \"wouldn't\",\n",
              " 'Cardassians',\n",
              " 'keep',\n",
              " 'talk',\n",
              " 'someone',\n",
              " 'do.',\n",
              " 'When',\n",
              " 'Your',\n",
              " 'hope',\n",
              " 'try',\n",
              " 'Dax',\n",
              " 'wanted',\n",
              " \"We'll\",\n",
              " 'sir.',\n",
              " 'around',\n",
              " 'there.',\n",
              " 'viewscreen]:',\n",
              " 'Kira',\n",
              " 'know,',\n",
              " 'way.',\n",
              " \"What's\",\n",
              " 'man',\n",
              " 'right,',\n",
              " 'lot',\n",
              " 'years',\n",
              " 'COMPUTER:',\n",
              " 'Commander',\n",
              " 'ZEK:',\n",
              " 'able',\n",
              " 'again.',\n",
              " \"they're\",\n",
              " 'Quark.',\n",
              " 'As',\n",
              " 'on.',\n",
              " '[Ops]',\n",
              " 'Of',\n",
              " 'Quark',\n",
              " 'doing',\n",
              " 'five',\n",
              " 'me?',\n",
              " 'because',\n",
              " 'mean',\n",
              " 'that?',\n",
              " \"haven't\",\n",
              " 'own',\n",
              " 'things',\n",
              " 'stay',\n",
              " 'Come',\n",
              " 'Doctor.',\n",
              " 'Thank',\n",
              " 'looking',\n",
              " 'leave',\n",
              " 'next',\n",
              " 'feel',\n",
              " 'life',\n",
              " 'without',\n",
              " 'Chief',\n",
              " 'office]',\n",
              " 'quarters]',\n",
              " 'Bashir',\n",
              " \"We've\",\n",
              " \"Let's\",\n",
              " 'out.',\n",
              " 'use',\n",
              " 'every',\n",
              " 'go.',\n",
              " 'So,',\n",
              " 'same',\n",
              " 'Tell',\n",
              " 'place',\n",
              " 'until',\n",
              " 'PEL:',\n",
              " 'talking',\n",
              " 'hear',\n",
              " 'old',\n",
              " 'MELORA:',\n",
              " 'Now,',\n",
              " 'Odo',\n",
              " 'work',\n",
              " 'Ferengi',\n",
              " 'Go',\n",
              " 'knew',\n",
              " 'used',\n",
              " 'course',\n",
              " 'hundred',\n",
              " \"wasn't\",\n",
              " 'Odo.',\n",
              " 'everything',\n",
              " 'found',\n",
              " 'guess',\n",
              " 'while',\n",
              " 'MARTUS:',\n",
              " 'all.',\n",
              " 'already',\n",
              " 'getting',\n",
              " 'here,',\n",
              " 'left',\n",
              " 'most',\n",
              " 'enough',\n",
              " 'up.',\n",
              " '(Sisko',\n",
              " 'ARJIN:',\n",
              " 'Major.',\n",
              " 'anyone',\n",
              " 'Well',\n",
              " \"we'll\",\n",
              " 'both',\n",
              " 'TAIN:',\n",
              " 'against',\n",
              " 'understand',\n",
              " 'wish',\n",
              " 'NATIMA:',\n",
              " 'ask',\n",
              " 'Did',\n",
              " 'back.',\n",
              " 'came',\n",
              " 'goes',\n",
              " \"what's\",\n",
              " 'Because',\n",
              " 'Doctor',\n",
              " 'Yes.',\n",
              " \"(O'Brien\",\n",
              " 'Quark,',\n",
              " 'coming',\n",
              " 'done',\n",
              " 'kill',\n",
              " 'love',\n",
              " 'one.',\n",
              " 'comes',\n",
              " 'course.',\n",
              " 'power',\n",
              " 'ships',\n",
              " 'what?',\n",
              " 'away',\n",
              " 'many',\n",
              " 'too.',\n",
              " 'Mister',\n",
              " 'quite',\n",
              " 'start',\n",
              " 'under',\n",
              " \"we've\",\n",
              " '(A',\n",
              " 'Bajor',\n",
              " \"She's\",\n",
              " 'day',\n",
              " 'name',\n",
              " 'seems',\n",
              " 'Bajor.',\n",
              " 'Can',\n",
              " 'Let',\n",
              " 'Where',\n",
              " 'run',\n",
              " 'suppose',\n",
              " 'that,',\n",
              " 'Computer,',\n",
              " 'afraid',\n",
              " 'glad',\n",
              " 'her.',\n",
              " \"O'BRIEN\",\n",
              " 'On',\n",
              " 'here?',\n",
              " 'hours.',\n",
              " 'stop',\n",
              " 'MORA:',\n",
              " 'does',\n",
              " 'gets',\n",
              " 'heard',\n",
              " 'in.',\n",
              " 'Odo,',\n",
              " 'To',\n",
              " \"You've\",\n",
              " \"couldn't\",\n",
              " 'means',\n",
              " 'RIKER:',\n",
              " 'care',\n",
              " 'not.',\n",
              " 'twenty',\n",
              " '(Quark',\n",
              " 'Chief.',\n",
              " 'Doctor,',\n",
              " 'For',\n",
              " 'is.',\n",
              " 'Have',\n",
              " 'Sisko.',\n",
              " 'What?',\n",
              " \"[Quark's]\",\n",
              " 'Gamma',\n",
              " 'is,',\n",
              " 'looks',\n",
              " 'part',\n",
              " 'seen',\n",
              " 'remember',\n",
              " '(Bashir',\n",
              " 'HUDSON:',\n",
              " 'great',\n",
              " 'BC:',\n",
              " 'One',\n",
              " 'behind',\n",
              " 'HANEEK:',\n",
              " \"Jem'Hadar\",\n",
              " 'Major,',\n",
              " 'best',\n",
              " 'so.',\n",
              " 'this?',\n",
              " 'Central',\n",
              " 'ISHKA:',\n",
              " 'down.',\n",
              " 'entire',\n",
              " 'to.',\n",
              " 'Klingon',\n",
              " 'Major',\n",
              " 'else',\n",
              " 'seem',\n",
              " 'well',\n",
              " 'Garak',\n",
              " 'ago.',\n",
              " 'each',\n",
              " 'its',\n",
              " 'since',\n",
              " 'such',\n",
              " 'beam',\n",
              " 'exactly',\n",
              " 'having',\n",
              " 'least',\n",
              " '(Kira',\n",
              " 'SISKO',\n",
              " 'now,',\n",
              " 'Look,',\n",
              " \"You'll\",\n",
              " 'are.',\n",
              " 'field',\n",
              " 'happened',\n",
              " 'life.',\n",
              " 'saw',\n",
              " 'well.',\n",
              " 'working',\n",
              " '(Odo',\n",
              " 'DERAL:',\n",
              " \"O'Brien.\",\n",
              " 'send',\n",
              " 'After',\n",
              " 'Get',\n",
              " 'Gul',\n",
              " 'takes',\n",
              " 'took',\n",
              " 'At',\n",
              " 'Who',\n",
              " '[Promenade]',\n",
              " 'between',\n",
              " 'call',\n",
              " 'device',\n",
              " 'maybe',\n",
              " 'show',\n",
              " 'telling',\n",
              " 'Good',\n",
              " '[Infirmary]',\n",
              " 'business',\n",
              " 'him,',\n",
              " 'people.',\n",
              " 'return',\n",
              " 'ten',\n",
              " '-',\n",
              " 'Vedek',\n",
              " 'allow',\n",
              " \"aren't\",\n",
              " 'chance',\n",
              " 'level',\n",
              " 'matter',\n",
              " 'past',\n",
              " 'live',\n",
              " 'waiting',\n",
              " 'warp',\n",
              " '[Corridor]',\n",
              " '[Runabout',\n",
              " 'away.',\n",
              " 'leaves)',\n",
              " \"she's\",\n",
              " 'weapons',\n",
              " 'which',\n",
              " 'wrong',\n",
              " 'VIN:',\n",
              " 'head',\n",
              " 'makes',\n",
              " 'on,',\n",
              " 'set',\n",
              " 'ship.',\n",
              " 'soon',\n",
              " 'wormhole.',\n",
              " 'GHEMOR:',\n",
              " 'bring',\n",
              " 'rest',\n",
              " 'understand.',\n",
              " 'VERAD:',\n",
              " 'along',\n",
              " 'hands',\n",
              " 'probably',\n",
              " 'see.',\n",
              " 'supposed',\n",
              " 'tried',\n",
              " 'turn',\n",
              " 'years.',\n",
              " 'yet.',\n",
              " '(Dax',\n",
              " 'Good.',\n",
              " 'about.',\n",
              " 'almost',\n",
              " 'far',\n",
              " 'hard',\n",
              " 'him?',\n",
              " \"let's\",\n",
              " 'saying',\n",
              " 'upper',\n",
              " 'wait',\n",
              " 'Chief,',\n",
              " 'Garak.',\n",
              " 'SHAKAAR:',\n",
              " 'asked',\n",
              " 'help.',\n",
              " 'home.',\n",
              " 'killed',\n",
              " 'lost',\n",
              " 'once',\n",
              " 'reason',\n",
              " 'sorry',\n",
              " 'DAX',\n",
              " 'KIRA',\n",
              " 'Our',\n",
              " 'Really?',\n",
              " 'Rio',\n",
              " 'WEBB:',\n",
              " 'day.',\n",
              " 'hours',\n",
              " 'side',\n",
              " 'them?',\n",
              " 'thing.',\n",
              " 'transporter',\n",
              " 'within',\n",
              " 'man.',\n",
              " 'Command',\n",
              " 'FEMALE:',\n",
              " 'JENNIFER:',\n",
              " 'Maquis',\n",
              " '[Bridge]',\n",
              " 'brought',\n",
              " 'friend',\n",
              " 'meet',\n",
              " 'mind',\n",
              " 'stand',\n",
              " 'work.',\n",
              " 'ALIXUS:',\n",
              " 'Why?',\n",
              " 'did.',\n",
              " 'expect',\n",
              " 'idea.',\n",
              " 'seven',\n",
              " 'six',\n",
              " 'something.',\n",
              " 'Cardassia',\n",
              " 'Curzon',\n",
              " 'DUKAT',\n",
              " 'MAREEL:',\n",
              " 'SAKONNA:',\n",
              " \"[Commander's\",\n",
              " 'bad',\n",
              " 'subspace',\n",
              " 'taking',\n",
              " 'will.',\n",
              " 'woman',\n",
              " 'wrong.',\n",
              " 'yourself.',\n",
              " 'Dax.',\n",
              " 'FENNA:',\n",
              " 'INTENDANT:',\n",
              " 'computer',\n",
              " 'gives',\n",
              " 'idea',\n",
              " 'right?',\n",
              " 'sorry.',\n",
              " 'system.',\n",
              " 'taken',\n",
              " 'went',\n",
              " 'Defiant',\n",
              " 'I.',\n",
              " 'KANG:',\n",
              " 'about?',\n",
              " 'access',\n",
              " 'change',\n",
              " 'during',\n",
              " 'four',\n",
              " 'hand',\n",
              " 'information',\n",
              " 'mean,',\n",
              " 'met',\n",
              " 'not?',\n",
              " 'open',\n",
              " 'ready',\n",
              " 'runabout',\n",
              " 'sorry,',\n",
              " 'wants',\n",
              " 'Space',\n",
              " 'Excuse',\n",
              " 'His',\n",
              " 'KOR:',\n",
              " 'Take',\n",
              " 'Which',\n",
              " 'asking',\n",
              " 'control',\n",
              " 'do?',\n",
              " 'happen',\n",
              " 'leaves',\n",
              " 'making',\n",
              " 'sensor',\n",
              " 'thousand',\n",
              " 'yourself',\n",
              " 'Deep',\n",
              " '(He',\n",
              " 'GILORA:',\n",
              " \"JEM'HADAR:\",\n",
              " '[Security',\n",
              " 'be.',\n",
              " 'become',\n",
              " 'before.',\n",
              " 'brother.',\n",
              " 'days.',\n",
              " 'energy',\n",
              " 'everyone',\n",
              " \"he'll\",\n",
              " 'hurt',\n",
              " 'important',\n",
              " 'time,',\n",
              " 'willing',\n",
              " '(She',\n",
              " 'Kira.',\n",
              " 'Or',\n",
              " 'PALLRA:',\n",
              " \"They've\",\n",
              " 'With',\n",
              " 'fine.',\n",
              " 'friend.',\n",
              " 'peace',\n",
              " 'thirty',\n",
              " 'whole',\n",
              " 'Dukat',\n",
              " 'JARO:',\n",
              " 'Julian.',\n",
              " 'Perhaps',\n",
              " 'Would',\n",
              " 'Yeah,',\n",
              " 'also',\n",
              " 'better.',\n",
              " 'couple',\n",
              " 'enough.',\n",
              " 'full',\n",
              " 'gave',\n",
              " 'given',\n",
              " 'phaser',\n",
              " 'point',\n",
              " 'sure.',\n",
              " 'Actually,',\n",
              " 'ENTEK:',\n",
              " 'good.',\n",
              " 'myself.',\n",
              " 'personal',\n",
              " 'place.',\n",
              " 'pretty',\n",
              " 'realise',\n",
              " 'says',\n",
              " 'thank',\n",
              " 'way,',\n",
              " 'Bareil',\n",
              " 'Benjamin,',\n",
              " 'CURZON:',\n",
              " 'anything.',\n",
              " 'inside',\n",
              " 'longer',\n",
              " 'mind.',\n",
              " 'plan',\n",
              " 'puts',\n",
              " 'rather',\n",
              " 'sent',\n",
              " \"they'll\",\n",
              " 'trust',\n",
              " 'Benjamin.',\n",
              " 'CHRIS:',\n",
              " 'Dax,',\n",
              " 'Security',\n",
              " 'contact',\n",
              " 'days',\n",
              " 'deal',\n",
              " 'friends',\n",
              " 'medical',\n",
              " 'room',\n",
              " 'speak',\n",
              " 'system',\n",
              " 'thinking',\n",
              " 'using',\n",
              " 'Cardassia.',\n",
              " 'Demilitarised',\n",
              " 'GRILKA:',\n",
              " 'Give',\n",
              " 'Jadzia',\n",
              " 'Look',\n",
              " 'May',\n",
              " 'Obsidian',\n",
              " 'SEYETIK:',\n",
              " 'ahead.',\n",
              " 'all,',\n",
              " 'big',\n",
              " 'close',\n",
              " 'do,',\n",
              " 'enters)',\n",
              " 'hoping',\n",
              " 'minute.',\n",
              " 'order',\n",
              " 'play',\n",
              " 'sit',\n",
              " 'together',\n",
              " 'turns',\n",
              " 'whatever',\n",
              " 'Even',\n",
              " 'Garak,',\n",
              " 'Hey,',\n",
              " 'I?',\n",
              " 'boy',\n",
              " 'certain',\n",
              " 'eight',\n",
              " 'happy',\n",
              " 'hate',\n",
              " \"he'd\",\n",
              " 'job',\n",
              " 'minutes.',\n",
              " 'see,',\n",
              " 'station,',\n",
              " 'KRIM:',\n",
              " 'Nagus',\n",
              " 'Very',\n",
              " 'figure',\n",
              " 'government',\n",
              " 'no.',\n",
              " 'suggest',\n",
              " 'them,',\n",
              " 'trouble',\n",
              " 'well,',\n",
              " \"would've\",\n",
              " 'Bajorans',\n",
              " 'By',\n",
              " \"It'll\",\n",
              " 'Nothing',\n",
              " 'Quadrant.',\n",
              " 'Romulans',\n",
              " 'Starfleet.',\n",
              " 'cloaking',\n",
              " 'end',\n",
              " 'hold',\n",
              " 'main',\n",
              " 'real',\n",
              " 'sound',\n",
              " 'year',\n",
              " 'Bashir.',\n",
              " 'Dominion',\n",
              " 'Lieutenant.',\n",
              " 'MAKBAR:',\n",
              " 'Rule',\n",
              " 'Three',\n",
              " 'case',\n",
              " 'comm.',\n",
              " \"it'll\",\n",
              " 'quarters',\n",
              " 'reading',\n",
              " 'running',\n",
              " 'there?',\n",
              " 'this,',\n",
              " 'want.',\n",
              " 'worry',\n",
              " '(Garak',\n",
              " 'Cardassians.',\n",
              " 'JOSEPH:',\n",
              " 'KOVAT:',\n",
              " 'Miles',\n",
              " 'Two',\n",
              " 'appreciate',\n",
              " 'begin',\n",
              " 'destroy',\n",
              " 'die.',\n",
              " 'different',\n",
              " 'else.',\n",
              " 'fight',\n",
              " 'finally',\n",
              " 'happened?',\n",
              " 'join',\n",
              " 'myself',\n",
              " 'seemed',\n",
              " 'small',\n",
              " 'walk',\n",
              " 'word',\n",
              " 'wrong?',\n",
              " 'Any',\n",
              " 'COLYUS:',\n",
              " 'Quadrant',\n",
              " 'ULANI:',\n",
              " 'again,',\n",
              " 'door',\n",
              " 'feeling',\n",
              " 'hell',\n",
              " 'home',\n",
              " 'miss',\n",
              " 'onto',\n",
              " 'possible.',\n",
              " 'programme',\n",
              " 'risk',\n",
              " 'room]',\n",
              " 'simply',\n",
              " 'sort',\n",
              " 'there,',\n",
              " 'true.',\n",
              " 'Dominion.',\n",
              " 'ERIS:',\n",
              " 'Order',\n",
              " 'Romulan',\n",
              " 'Some',\n",
              " 'TAYA:',\n",
              " \"They'll\",\n",
              " 'Wait',\n",
              " 'anymore.',\n",
              " 'anyway.',\n",
              " 'business.',\n",
              " \"could've\",\n",
              " 'doing?',\n",
              " 'done.',\n",
              " 'explain',\n",
              " 'food',\n",
              " 'knows',\n",
              " 'outside',\n",
              " 'promise',\n",
              " 'prove',\n",
              " 'quarters.',\n",
              " 'save',\n",
              " 'sense',\n",
              " '(They',\n",
              " 'FALLIT:',\n",
              " 'Like',\n",
              " \"Quark's\",\n",
              " 'RENHOL:',\n",
              " 'RURIGAN:',\n",
              " 'alone.',\n",
              " 'brother',\n",
              " 'cargo',\n",
              " 'communications',\n",
              " 'complete',\n",
              " 'dabo',\n",
              " 'family',\n",
              " 'father',\n",
              " 'half',\n",
              " 'her,',\n",
              " 'himself',\n",
              " 'human',\n",
              " 'job.',\n",
              " 'over.',\n",
              " 'planet',\n",
              " 'question',\n",
              " 'sensors',\n",
              " 'today.',\n",
              " \"we'd\",\n",
              " 'Jadzia.',\n",
              " 'Jake,',\n",
              " \"O'Brien,\",\n",
              " 'Please',\n",
              " 'Prophets',\n",
              " 'Whatever',\n",
              " \"You'd\",\n",
              " 'cannot',\n",
              " 'dead',\n",
              " 'doubt',\n",
              " 'fact,',\n",
              " 'felt',\n",
              " 'fifty',\n",
              " 'have.',\n",
              " 'less',\n",
              " 'luck.',\n",
              " 'minutes',\n",
              " 'move',\n",
              " 'night',\n",
              " 'night.',\n",
              " 'playing',\n",
              " 'report',\n",
              " 'then.',\n",
              " 'us,',\n",
              " 'Believe',\n",
              " \"E'TYSHRA:\",\n",
              " 'Infirmary.',\n",
              " 'KOLOTH:',\n",
              " 'Ops.',\n",
              " 'Please,',\n",
              " 'These',\n",
              " 'aboard',\n",
              " 'again',\n",
              " 'am.',\n",
              " 'attack',\n",
              " 'bar',\n",
              " 'can.',\n",
              " 'decided',\n",
              " 'five.',\n",
              " 'forty',\n",
              " 'forward',\n",
              " 'immediately.',\n",
              " 'know?',\n",
              " 'latinum',\n",
              " 'log,',\n",
              " 'no,',\n",
              " 'picking',\n",
              " 'problem.',\n",
              " 'seconds.',\n",
              " 'seeing',\n",
              " 'shut',\n",
              " 'sounds',\n",
              " 'together.',\n",
              " 'turned',\n",
              " 'up,',\n",
              " 'worry,',\n",
              " 'An',\n",
              " 'Bareil.',\n",
              " 'Ben.',\n",
              " 'Does',\n",
              " 'Ferengi.',\n",
              " 'Jake',\n",
              " 'Klingons',\n",
              " 'LWAXANA:',\n",
              " 'agree',\n",
              " 'bit',\n",
              " 'check',\n",
              " 'died',\n",
              " 'enters',\n",
              " 'gone',\n",
              " 'he?',\n",
              " 'leave.',\n",
              " 'long.',\n",
              " 'number',\n",
              " 'off.',\n",
              " ...]"
            ]
          },
          "execution_count": 727,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNqg/CAncHESRiKP7ztpmQi",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
