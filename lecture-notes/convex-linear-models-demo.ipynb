{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import make_blobs\n",
    "plt.rcParams[\"figure.figsize\"] = (4, 3)\n",
    "np.seterr(all='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(0, 5, 101)\n",
    "plt.plot(z, -np.log(1/(1 + np.exp(-z)))) \n",
    "labs = plt.gca().set(xlabel = r\"$\\hat{y}$\", ylabel = r\"$-\\log \\sigma(\\hat{y})$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig-cap: Note that this data is **not** linearly separable. The perceptron algorithm wouldn't even have converged for this data set, but logistic regression will do great. \n",
    "\n",
    "p_features = 3\n",
    "\n",
    "X, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n",
    "\n",
    "fig = plt.scatter(X[:,0], X[:,1], c = y)\n",
    "xlab = plt.xlabel(\"Feature 1\")\n",
    "ylab = plt.ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a constant feature to the feature matrix\n",
    "X_ = np.append(X, np.ones((X.shape[0], 1)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define some functions to compute the empirical risk: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement: (WE ARE CODING TOGETHER HERE!!)\n",
    "# - predict\n",
    "# - sigmoid\n",
    "# - logistic_loss\n",
    "# - empirical_risk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the function that will solve the empirical risk minimization problem for us. We're going to use the `scipy.optimize.minimize` function, which is a built-in function for solving minimization problems. Soon, we'll study how to solve minimization problems from scratch. \n",
    "\n",
    "The `scipy.optimize.minimize` function requires us to pass it a single function that accepts a vector of parameters, plus an initial guess for the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "def find_pars(X, y):\n",
    "    \n",
    "    p = X.shape[1]\n",
    "    w0 = np.random.rand(p) # random initial guess\n",
    "    \n",
    "    # perform the minimization\n",
    "    result = minimize(lambda w: empirical_risk(X, y, w, logistic_loss), \n",
    "                      x0 = w0) \n",
    "    \n",
    "    # return the parameters\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try it and take a look at the parameters we obtained. Because the final column of `X_` is the constant column of 1s, the final entry of `w` is interpretable as the intercept term `b`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "w = find_pars(X_, y)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, we can plot the linear classifier that we learned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "fig = plt.scatter(X[:,0], X[:,1], c = y)\n",
    "xlab = plt.xlabel(\"Feature 1\")\n",
    "ylab = plt.ylabel(\"Feature 2\")\n",
    "\n",
    "f1 = np.linspace(-3, 3, 101)\n",
    "\n",
    "plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the logistic loss is convex, we are guaranteed that this solution is the unique best solution (as measured by the logistic loss). There is no other possible set of parameters that would lead to a better result (again, as measured by the logistic loss). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "debe06cc0f9553f110b64dc3926c05df82dae2145b852c8422b9c04315589dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
