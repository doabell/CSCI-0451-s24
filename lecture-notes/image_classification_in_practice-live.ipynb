{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  More on Image Classification\n",
        "author: Phil Chodrow\n",
        "bibliography: ../refs.bib\n",
        "format: \n",
        "  html: \n",
        "    code-fold: false\n",
        "    cache: true\n",
        "    callout-appearance: minimal\n",
        "    cap-location: margin\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "[Open these notes in Google Colab.](https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/image_classification_in_practice.ipynb) \n",
        "\n",
        "[Open the live version of these notes in Google Colab.](https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/image_classification_in_practice-live.ipynb) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPi1VklCGwe0"
      },
      "source": [
        "In this lecture, we'll wrap up our discussion of deep image classification with consideration of some of the practicalities that go in to designing and training models. We'll focus especially on some methods that can improve the performance of image classifiers without necessarily increasing the size of the training data set. These are *data augmentation* and *transfer learning*. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bAPlL6KJD_u"
      },
      "outputs": [],
      "source": [
        "# for data retrieval\n",
        "from io import BytesIO\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# for managing image data\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "\n",
        "# various torch stuff\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# for timing the training loop\n",
        "import time\n",
        "\n",
        "torch.random.manual_seed(12345) # for reproducibility\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7koMqVYHRzc"
      },
      "source": [
        "\n",
        "# Task for Today\n",
        "\n",
        "Can you teach a machine learning algorithm to distinguish between pictures of dogs and pictures of cats?\n",
        "\n",
        "According to this helpful diagram below, one way to do this is to attend to the visible emotional range of the pet:\n",
        "\n",
        "![](https://static.boredpanda.com/blog/wp-content/uploads/2017/09/funny-cats-vs-dogs-comics-200-59c380533523b__700.jpg)\n",
        "\n",
        "Unfortunately, using this method requires that we have access to multiple images of the same individual. We will consider a setting in which we have only one image for pet. Can we reliably distinguish between cats and dogs in this case?\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "We'll start by downloading a data set containing images of cats and dogs. The result is a folder in our working directory called `cats_and_dogs_filtered`. This folder contains a `train` directory consisting of training images and a `validation` directory with validation images. Within the `train` directory are `cats` and `dogs` subdirectories containing images of each kind, and similarly for the `validation` directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQRif9c9CjxY"
      },
      "outputs": [],
      "source": [
        "URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "with urlopen(URL) as zipped:\n",
        "    with ZipFile(BytesIO(zipped.read())) as zfile:\n",
        "        zfile.extractall('')\n",
        "\n",
        "train_dir = \"cats_and_dogs_filtered/train\"\n",
        "val_dir   = \"cats_and_dogs_filtered/validation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-gZ7qdrKdGW"
      },
      "source": [
        "Now that we've acquired the data, let's create a Torch `Dataset` object. The nice developers at `Torch` have implemented an `ImageFolder` constructor that will create a `Dataset` for us. We only need to supply the directory of the training (and validation) data sets, as well as an optional `transform` that should be applied to each image prior to being fed into the model. Let's start by just specifying that each image should have the `transforms.ToTensor()` function applied to it, which, as you might guess, will convert the image into a `Tensor`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bdFMDPDGxtR",
        "outputId": "105ed6c0-22d2-44bb-f500-0e5b3a4a57f6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFsK2BJXK59u"
      },
      "source": [
        "Now we're ready to visualize a few entries in our data: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "1mhDcZNXI6iI",
        "outputId": "11058957-47a8-4962-e920-e6c3671eba01"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def viz_data(data, n_rows = 3, n_cols = 4):\n",
        "    classes = (\"cat\", \"dog\")\n",
        "    fig, axarr = plt.subplots(n_rows, n_cols, figsize = (10, 7))\n",
        "    for i, ax in enumerate(axarr.ravel()):\n",
        "        \n",
        "        j = np.random.randint(0, 2000)\n",
        "        # returns batch_size images with their labels\n",
        "        X, y = train_data[j]\n",
        "\n",
        "        # populate a row with the images in the batch\n",
        "        for j in range(n_cols):\n",
        "            img = np.moveaxis(X.numpy(), 0, 2) \n",
        "            ax.imshow(img )\n",
        "            ax.axis(\"off\")\n",
        "            ax.set(title = classes[int(y)])\n",
        "\n",
        "viz_data(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpK6rAu-LPCY"
      },
      "source": [
        "Although the classification task of \"cat\" vs. \"dog\" is simpler than last time, we can also see that the dat is somewhat messier. In particular, the images appear to come from a larger variety of different contexts. An important technical problem is that the images are of different *sizes*. In the supervized learning context this corresponds to a different number of features for each image, which is not workable for modeling. So, we need to resize our images to ensure that they all have the same number of features. This is handled by the `Resize` transform. The `Normalize` transform services to make the pixel values more comparable to each other (more formally, it scales the pixel values so that each image has mean pixel value 0 and variance 1). \n",
        "\n",
        "Let's define both our training and validation data sets with this new set of transforms: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi1t3vOdGSbU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_data = ImageFolder(train_dir, transform = transform)\n",
        "val_data = ImageFolder(val_dir, transform = transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReS-S0MWMumj"
      },
      "source": [
        "If we now take a look, we can see that the resulting images are now all the same size, although some of them are a little blurry and others might be cropped in slightly strange ways. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "ect-w7EBKSux",
        "outputId": "3b33f682-f7e1-46d4-f357-23fd5e34675a"
      },
      "outputs": [],
      "source": [
        "viz_data(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Q_SOpfNM_l"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Now that we've prepared our data, we are ready to construct a model. First let's wrap our data sets in data loaders to handle batching and randomization during training: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eoJcrp7Hofy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xGKdPEWN2Ws"
      },
      "source": [
        "For a first model, let's use a manual convolutional neural network with three convolutional layers. This network is very similar to the one that we introduced last lecture. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e172eHEDJU7L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q5V4Kj4N_zx"
      },
      "source": [
        "Let's inspect our model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqKIVCTKXSEm",
        "outputId": "f886168f-0d09-4466-c4a5-5c2b897d9973"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (3, 64, 64)\n",
        "summary(model, INPUT_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAUP_zv_OEg1"
      },
      "source": [
        "This is plenty of parameters, but again not as many as we might expect from a model with a similar number of layers with more fully-connected (`Linear`) layers. \n",
        "\n",
        "Now we can define a training loop and train our model. As usual, the training loop contains just a few important computational steps: \n",
        "\n",
        "1. Extract a batch of data from the `data_loader`. \n",
        "2. Zero out the previously-computed gradients. \n",
        "3. Compute the model predictions on the predictor data in the batch. \n",
        "4. Compute the loss of these predictions relative to the ground-truth labels. \n",
        "5. Compute gradients of the batch loss with respect to the model parameters. \n",
        "6. Take a step in the direction of the gradient, possibly after some modifications (e.g. in the Adam algorithm). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4AsxJL4K3dt"
      },
      "outputs": [],
      "source": [
        "def train(model,  data_loader, optimizer, k_epochs = 1, print_every = 2000):\n",
        "\n",
        "    begin = time.time()\n",
        "    # loss function is cross-entropy (multiclass logistic)\n",
        "    loss_fn = nn.CrossEntropyLoss() \n",
        "\n",
        "    # optimizer is Adam, which does fancier stuff with the gradients\n",
        "    \n",
        "    for epoch in range(k_epochs): \n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "\n",
        "            # extract a batch of training data from the data loader\n",
        "            X, y = data\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # zero out gradients: we're going to recompute them in a moment\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # compute the loss (forward pass)\n",
        "            y_hat = model(X)\n",
        "            loss = loss_fn(y_hat, y)\n",
        "\n",
        "            # compute the gradient (backward pass)\n",
        "            loss.backward()\n",
        "\n",
        "            # Adam uses the gradient to update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # print the epoch, number of batches processed, and running loss \n",
        "            # in regular intervals\n",
        "            if i % print_every == print_every - 1:    \n",
        "                print(f'[epoch: {epoch + 1}, batches: {i + 1:5d}], training loss: {running_loss / print_every:.3f}')\n",
        "                running_loss = 0.0\n",
        "    end = time.time()\n",
        "    print(f'Finished training in {round(end - begin)}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuFrNO_CO0tE"
      },
      "source": [
        "Now let's try training our model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_ARHnjbO24y",
        "outputId": "250f3f6d-0259-4944-d36f-47fd0bb3e810"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_crOYqp4O3m_"
      },
      "source": [
        "These are not *very* impressive training loss scores, as the loss we would get for a model that put equal weight on each possible label (essentially shrugging its shoulders) would be "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "925bPQckO9Ff",
        "outputId": "c7512b9d-abb0-4294-afed-1227826ce131"
      },
      "outputs": [],
      "source": [
        "print(np.log(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UNmJSA6PBEE"
      },
      "source": [
        "However, we can still test the model and see whether it might have learned anything that enables better-than-random predictions: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTJNS43rLa_D",
        "outputId": "fe7733a8-fc9f-438e-fe77-b0478a9cc2ed"
      },
      "outputs": [],
      "source": [
        "def test(model, data_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # torch.no_grad creates an environment in which we do NOT store the \n",
        "    # computational graph. We don't need to do this because we don't care about \n",
        "    # gradients unless we're training\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            X, y = data\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            # run all the images through the model\n",
        "            y_hat = model(X)\n",
        "\n",
        "            # the class with the largest model output is the prediction\n",
        "            _, predicted = torch.max(y_hat.data, 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "    print(f'Test accuracy: {100 * correct // total} %')\n",
        "\n",
        "test(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NkK-446PeRT"
      },
      "source": [
        "So, our model did learn some signal that was useful in making predictions on the validation set, even though that wasn't totally visible from the values of the training loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0DXwpcaIqS"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "One of the most common problems in applied machine learning is: \n",
        "\n",
        "> *I don't have enough data to train my model.* \n",
        "\n",
        "Sometimes, in this situation, we are simply out of luck: if there's not enough data then you need to train a simpler model, or go get more data. In some settings---especially in the setting of images---there are some techniques we can try. The first one is data augmentation. \n",
        "\n",
        "Data augmentation techniques usually exploit certain so-called *invariances* in image data. Here's an example of \"rotation invariance.\" Suppose I show you a picture of a cat: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "CkeSUPWnQQZB",
        "outputId": "58df5267-5b59-4d93-ee5a-a133ea4149db"
      },
      "outputs": [],
      "source": [
        "X, y = train_data[14]\n",
        "plt.imshow(np.moveaxis(X.numpy(), 0, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PyEWEiwQrTX"
      },
      "source": [
        "Now suppose that I *rotate* the image a bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "OFFGw800Qc59",
        "outputId": "a27b51e9-fb92-4de6-d5d9-2d6017134848"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.imshow(np.moveaxis(rotated.numpy(), 0, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4hBHEDfQ3Be"
      },
      "source": [
        "Well, that rotated image is *still an image of a cat*, just rotated a little bit. So, the correct label of the image is \"rotation-invariant\": rotating the image doesn't change the correct label. I could similarly flip the image upside-down, or crop it, or manipulate it in many other ways while still preserving it's \"cat\"-ness. So, and this is a very special kind of circumstances, I can (sort of) *get more data for free* by rotating, flipping, and otherwise manipulating my images. \n",
        "\n",
        "This cluster of techniques is called *data augmentation*. Theoretically, data augmentation is not *just* a hack to get \"more\" data; it also helps models to generalize by ensuring that they are really learning features of \"cats\" and not just \"cats that are standing the right way up.\" \n",
        "\n",
        "A nice feature of Torch is that we can build these transforms right into our training data set. We don't apply data augmentation to our validation data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QzI1_k8aNYC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_data = ImageFolder(train_dir, transform = augmentation_transform)\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SMDg8MxRuhC"
      },
      "source": [
        "Let's take a look at some of our transformed data: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "HPBRX3y9aqiA",
        "outputId": "b0a98983-a4fd-466f-8fc7-db3ba75438f6"
      },
      "outputs": [],
      "source": [
        "classes = (\"cat\", \"dog\")\n",
        "\n",
        "n_rows = 3\n",
        "n_cols = 5\n",
        "\n",
        "fig, axarr = plt.subplots(n_rows, n_cols, figsize = (10, 7))\n",
        "\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "    # returns batch_size images with their labels\n",
        "        X, y = train_data[i]\n",
        "\n",
        "        img = np.moveaxis(X.numpy(), 0, 2) \n",
        "        axarr[i, j].imshow(img )\n",
        "        axarr[i, j].axis(\"off\")\n",
        "        axarr[i, j].set(title = classes[int(y)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpgxC2rxR083"
      },
      "source": [
        "There are many other possible transformations, but we'll stick with just those for now. Let's go ahead and train! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NhCeDMGasay",
        "outputId": "0bf15040-0066-4249-c6d9-2a0bb16a226c"
      },
      "outputs": [],
      "source": [
        "model = ConvNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train(model, train_loader, optimizer, k_epochs = k_epochs, print_every = 50)\n",
        "test(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8XxUYRWR4-z"
      },
      "source": [
        "Why did this model do worse than the last one even though we introduced a cool new technique? One way to think about this is that applying data augmentation increases the \"effective size\" of the data. However, we haven't given the model any more epochs for training. So, the model still only sees each image 5 times, just randomly modified. The model can't learn as much as it needs to from each image in this small number of epochs, and so it performs worse. It is likely that, with more epochs, this model would eventually be able to outperform the previous one. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg6podE9eBsD"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "*Transfer learning* is a fancy term that was invented to make it sound respectable to mooch off the work and computatinoal power of others. \n",
        "\n",
        "This is not quite fair. When we do *transfer learning,* we use a pre-trained model, modify it, and train it a little bit more for our own task. Generally speaking, models that have been trained on tasks similar to the ones we are doing now are likely to have the best performance. \n",
        "\n",
        "For example, `ImageNet` is a well-known class of models trained for image classification tasks. `Torch.models` allows you to simply create an instance of an `ImageNet` model: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO1ZXYU3eDFR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIstX4WsTU3S"
      },
      "source": [
        "We can immediately see that this model is *much* more complex than anything we have worked with before. There are over 11M parameters! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU1xJLYfh7Tf",
        "outputId": "658ac764-0dc8-49a0-b311-fb12dc2daafe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPB7FoM2TdL0"
      },
      "source": [
        "One important point we can discover from the summary is that the final `Linear` layer, which serves as the model output, has 1,000 components. This is because the model was originally trained to classify images with 1,000 different possible labels. This won't work for us, since we only have two categories. What we'll do is: \n",
        "\n",
        "1. Assume that the patterns that `ImageNet` has learned are stored in the many hidden layers and\n",
        "2. Replace the output layer with our own output layer with just two output components. \n",
        "\n",
        "In this model, the output layer is stored in the `fc` instance variable. So, let's just overwrite it, making sure to include the correct number of input features; "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqwm19nAi8GW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6-lBX5rUBZN"
      },
      "source": [
        "Now we (hopefully) have a model that maintains much of `ImageNet`'s ability to detect patterns in images, but which will work for our purposes of binary classification. Let's go ahead and train it! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrBxe0RLieX1",
        "outputId": "e347182b-7991-470d-fe27-10e0bae43cf4"
      },
      "outputs": [],
      "source": [
        "# train with the complete model\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train(model, train_loader, optimizer, k_epochs = k_epochs, print_every = 50)\n",
        "test(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7ZXNyOuUQNL"
      },
      "source": [
        "After just two training epochs, our `ImageNet`-based classifier is already able to do substantially better on the validation set than our hand-crafted model did under either training approach. \n",
        "\n",
        "One way to think about this: we only have the data (and compute power) we have, but transfer learning can allow us to benefit from the data (and compute power) that others have used in their own projects. \n",
        "\n",
        "## Fixing Parameters\n",
        "\n",
        "But wait! In the last experiment, we still needed to train all 11M parameters of our modified `ImageNet`. But maybe we don't even need to train the parameters of the intermediate hidden layers. Let's instead train our model by insisting that only the parameters of the final hidden layer are going to be updated, with all the other parameters staying the same. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqwj0jQbh95g"
      },
      "outputs": [],
      "source": [
        "# instead train only the parameters of the final layer\n",
        "# can be around 50% faster\n",
        "\n",
        "model = models.resnet18(weights='IMAGENET1K_V1')\n",
        "model = model.to(device)\n",
        "\n",
        "# no gradients for any of the model parameters, so no updates\n",
        "\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfgy1MLpVG39",
        "outputId": "01f3809a-843d-438f-f2f4-3d0d712b1dc8"
      },
      "outputs": [],
      "source": [
        "summary(model, INPUT_SHAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg7K_eZJVLPn"
      },
      "source": [
        "Now, although our model still has the same number of parameters, the number of *trainable* parameters is much smaller. This means that we can get away with much less computation when it comes time to perform gradient updates. In particular, when using automatic differentiation it typically requires about as much compute to form the gradient (the backward step) as it does to compute the loss (the forward step). So, if we skip most of the backward step, we can hope for our training to be roughly twice as fast. This is usually the case for very large models. Our model isn't quite big enough to realize that large a speedup. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5WKM7tGe6o0",
        "outputId": "59d9cc3b-9726-43a4-de1e-6d8aa34807aa"
      },
      "outputs": [],
      "source": [
        "train(model, train_loader, optimizer, k_epochs = k_epochs, print_every = 50)\n",
        "test(model, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9XKgyMPWC3Z"
      },
      "source": [
        "It's important to keep in mind that none of these models were even *close* to being fully trained. Running a complete training process would be too much to show in a live demo! But the results here should be taken as illustrative of some of the benefits of the demonstrated modeling techniques, rather than scientific comparisons.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../assets/img/memes/class-cats.jpeg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNELcJc4BoZC15jplKdrWw3",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
