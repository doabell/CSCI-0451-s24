---
title: |
  Introduction to Deep Learning
author: Phil Chodrow
bibliography: ../refs.bib
format: 
  html: 
    code-fold: false
    cache: true
    callout-appearance: minimal
    cap-location: margin
---

::: {.hidden}
$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vp}{\mathbf{p}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vz}{\mathbf{z}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\mY}{\mathbf{Y}}
\newcommand{\mR}{\mathbf{R}}
\newcommand{\mI}{\mathbf{I}}
\newcommand{\mB}{\mathbf{B}}
\newcommand{\mU}{\mathbf{U}}
\newcommand{\mW}{\mathbf{W}}
\newcommand{\mZ}{\mathbf{Z}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\mP}{\mathbf{P}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\one}[1]{\mathbb{1}\left[ #1 \right]}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\mA}{\mathbf{A}}
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\prob}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\E}{\mathbb{E}}
\newcommand{\dd}[2]{\frac{\partial #1}{\partial #2}}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$

:::

# The Problem of Features

Let's begin by recalling and slightly expanding the empirical risk minimization framework that we've developed throughout this course. In the simplest approach to empirical risk minimization, we began with a matrix of features $\mX \in \R^{n\times p}$ and a vector of targets $\vy \in \R^n$. We defined a linear predictor function $\hat{y} = f(\vx) = \bracket{\vw, \vx}$ which we interpreted as producing predictions of the value of $y$. We then defined a loss function $\ell: \R\times \R \rightarrow \R$ that told us the quality of the prediction $\hat{y}$ by comparing it to a true target $y$. Our learning problem was to find $\vw$ by minimizing the *empirical risk*: the mean (or sum) of the risk across all data points:  

$$
\hat{\vw} = \argmin_{\vw \in \R^p} \sum_{i = 1}^n \ell(\bracket{\vw, \vx_i}, y_i)\;.
$$

We solved this problem using gradient descent. 

When we extended to the setting of multiple label classification (e.g. three penguin species), we modified this setup slightly. We let $\mY \in \R^{n\times \ell}$ be a matrix of targets, where each of the $\ell$ columns represents a possible category, $y_{i\ell} = 1$ means that observation $i$ has label $\ell$. We then needed to replace the prediction rule $f(\vx) = \bracket{\vw, \vx}$ with a matrix-vector multiplication $f(\vx) = \vx_i\mW $, where $\mW \in \R^{p \times \ell}$. This produced a vector $\hat{\vy}$ which we could compare to $\vy$ using an appropriately modified loss function. 

$$
\hat{\mW} = \argmin_{\mW \in \R^{p \times \ell}} \sum_{i = 1}^n \ell(\vx_i\mW , \vy_i)\;.
$$

However, we soon discovered a limitation: this method can only discover *linear* patterns in data, e.g. linear decision boundaries for classification or linear trends for regression. But most interesting patterns in data are *nonlinear*. We addressed this limitation using *feature engineering*: define a feature map $\phi: \R^p \rightarrow \R^{q}$ and then solve the modified problem 

$$
\hat{\mW} = \argmin_{\mW \in \R^{q\times \ell}} \sum_{i = 1}^n \ell(\phi(\vx_i)\mW, \vy_i)\;.
$$

## Sidebar: Notation Update

Before we proceed further, let's make a notation update. It is about to become very convenient for us to abuse mathematical notation slightly by "vectorizing functions over rows of matrices." What this means is that, if $f:\R^p \rightarrow \R^q$ we will define things like this: [This kind of definition is sometimes called *vectorizing* the function $f$, not to be confused with our discussion of vectorizing data in a [different lecture](vectorization.qmd).]{.aside}

$$
\begin{aligned}
f(\mX) &\triangleq \left[\begin{matrix} f(\vx_1) \\ f(\vx_2) \\ \vdots \\ f(\vx_n) \end{matrix}\right] 
\end{aligned}
$$

So, evaluating $f:\R^p \rightarrow \R^q$ on $\mX\in \R^{n\times p}$ produces a matrix $f(\mX) \in \R^{n\times q}$. 

Similarly, we'll define 

$$
\begin{aligned}
    \phi(\mX) \triangleq \left[\begin{matrix}\phi(\vx_1) \\ \phi(\vx_2) \\ \vdots \\ \phi(\vx_n)\end{matrix}\right] \quad \text{and} 
    \quad \ell(\hat{\mY}, \mY) &\triangleq \left(\begin{matrix} \ell(\hat{\vy}_1, \vy_1) \\ \ell(\hat{\vy}_2, \vy_2) \\ \vdots \\ \ell(\hat{\vy}_n, \vy_n) \end{matrix}\right)\;.
\end{aligned}
$$

This allows us to write our prediction rule compactly as 
$$
\begin{aligned}
\hat{\mY} = f(\mX) = \left[\begin{matrix} f(\vx_1) \\ f(\vx_2) \\ \vdots \\ f(\vx_n) \end{matrix}\right] 
       = \left[\begin{matrix} \phi(\vx_1)\mW \\ \phi(\vx_2)\mW \\ \vdots \\ \phi(\vx_n)\mW \end{matrix}\right] 
       = \left[\begin{matrix} \phi(\vx_1) \\ \phi(\vx_2) \\ \vdots \\ \phi(\vx_n) \end{matrix}\right] \mW 
       = \phi(\mX)\mW 
\end{aligned}
$$

Let's also define $\cL(\hat{\mY}, \mY) = \sum_{i = 1}^n \ell(\hat{\vy}_i, \vy_i)$. Then, we can write our generalized empirical risk as 

$$
R = \cL(\phi(\mX)\mW, \mY)\;,
$$

and our learning problem is 

$$
\hat{\mW} = \argmin_{\mW} \; \cL(\phi(\mX)\mW, \mY)\;,
$$

## The Network View

We can express the process of obtaining a prediction from logistic regression using a *computational graph*. Here's an example of a computational graph for logistic regression in which we have two input features and wish to perform 3-way classification by outputing for each input $\vx$ a vector $\vp$ of probabilities for each label: 

```{mermaid}
flowchart LR
  subgraph input
    direction TB  
    x_1 
    x_2
  end

  subgraph feature_map[features]
    f_1
    f_2
  end

  subgraph raw predictions[raw predictions/logits]
    y_hat_1
    y_hat_2 
    y_hat_3
  end

  subgraph label_preds[label probabilities]
    p_1
    p_2
    p_3
  end

  x_1--phi-->f_1
  x_2--phi-->f_2

  f_1 --w_1--> y_hat_1
  f_2 --w_1--> y_hat_1
  f_1 --w_2--> y_hat_2
  f_2 --w_2--> y_hat_2
  f_1 --w_3--> y_hat_3
  f_2 --w_3--> y_hat_3

  y_hat_1 & y_hat_2 & y_hat_3 --> softmax

  softmax --> p_1 & p_2 & p_3
```



## Back to Recap

We have a problem that we never addressed in a fully satisfactory way. On the one hand, in order to describe complex, nonlinear patterns in our data, we want a *lot* of features. [Kernel methods like [kernel logistic](../assignments/blog-posts/blog-post-kernel-logistic.qmd) regression or support vector machine can actually use infinitely many features!]{.aside} However, the more features we engineer, the more likely we are to encounter overfitting. 

One of the most common approaches to this problem around 15 years ago was to engineer many features but *regularize*, which forced entries of $\vw$ to remain small. This is a useful approach that is still used today, especially in statistics and econometrics. However, when our data points are very complex (audio files, images, text), it might still be very difficult for us to manually engineer the right sets of features to use even in this approach. 

The fundamental idea of *deep learning* is to take a different tack: instead of only learning $\vw$ in the empirical risk minimization problem, we are *also* going to learn the feature map $\phi$. That is, we want to find 

$$
\hat{\mW}, \hat{\phi} = \argmin_{\mW, \phi} \; \cL(\phi(\mX)\mW, \mY)\;,
$$

and our learned predictor is $f(\mX) = \hat{\phi}(\mX)\hat{\mW}$. 

The need to learn the feature map $\phi$ as well as weights $\vw$ makes this problem *much* harder, both mathematically and computationally. A major enabler of the deep learning revolution has been the development of hardware that is up to the task (especially GPUs), as well as algorithms that make good use of this hardware. 

# Hidden Layers

Ok, so we know that we want to figure out a feature map $\phi:\R^p\rightarrow \R^q$ to make the empirical risk small. There's no hope of optimizing over all possible feature maps, so instead we need to parameterize our feature map in some way. Here's what we do: 

Let $\alpha_1:\R \rightarrow \R$ be any *nonlinear* function, and let $\mU_1 \in \R^{p} \rightarrow \R^q$. We'll define: [$\alpha_1$ is often called an *activation function.*]{.aside}

$$
\phi(\mX) = \alpha_1(\mX\mU_1)\;.
$$

Our new prediction rule is 

$$
\hat{\mY} = f(\mX) = \phi(\mX)\mW = \alpha_1(\mX\mU_1)\mW\;, 
$$

and our empirical risk minimization problem is 

$$
\hat{\mW}, \hat{\mU}_1 = \argmin_{\mW \in \R^{q\times \ell}, \mU_1 \in \R^{p\times q}} \; \cL(\alpha_1(\mX\mU_1)\mW, \mY)\;.
$${#eq-single-layer}

@eq-single-layer is an example of a machine learning problem for a neural network with a single ***hidden layer***. The hidden layer refers to the "layer" of computation $\alpha_1(\mX\mU_1)$. This layer is "hidden" because it's not the input of the model (that would be $\mX$) nor is it the output (that would be $\alpha_1(\mX\mU_1)\mW$). The computational graph is now more complicated: we need to substitute in the explicit computations for $\phi$: 

```{mermaid}
flowchart LR
  subgraph input
    direction TB  
    x_1 
    x_2
  end

  subgraph hidden_activations
    a_1
    a_2
  end

  subgraph feature_map[features]
    f_1
    f_2
  end

  subgraph raw predictions[raw predictions/logits]
    y_hat_1
    y_hat_2 
    y_hat_3
  end

  subgraph label_preds[label probabilities]
    p_1
    p_2
    p_3
  end

  x_1 --u_1-->a_1
  x_1 --u_2-->a_2
  x_2 --u_1-->a_1
  x_2 --u_2-->a_2

  a_1--alpha_1-->f_1
  a_2--alpha_1-->f_2

  f_1 --w_1--> y_hat_1
  f_2 --w_1--> y_hat_1
  f_1 --w_2--> y_hat_2
  f_2 --w_2--> y_hat_2
  f_1 --w_3--> y_hat_3
  f_2 --w_3--> y_hat_3

  y_hat_1 & y_hat_2 & y_hat_3 --> softmax

  softmax --> p_1 & p_2 & p_3
```


As you can see, we have to deal with a lot of matrix multiplications and vectorized function operations. We will also, soon, need to think about how to perform the empirical risk minimization problem, which in turn poses questions about things like how to do gradient descent for this kind of problem. While it's important for us to understand some of the theory, it's not a great use of anyone's time to do large amounts of bookkeeping. This is why specialized deep learning libraries exist to help us along. 

# Deep Penguin Classification

Let's use our friends the penguins as a running example. When working with deep models we often need to construct more complex data structures in order to feed our models to the data. I'm hiding this complexity for today; we'll discuss it in some more detail in a coming lecture. 

```{python}
#| code-fold: true

import torch
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

target_ix_dict = {
  "Adelie Penguin (Pygoscelis adeliae)" : 0,
  "Chinstrap penguin (Pygoscelis antarctica)" : 1,
  "Gentoo penguin (Pygoscelis papua)" : 2
}

class PenguinsDataset(Dataset):
  def __init__(self, train = True):
    if train: 
      url = "https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv"
    else:
      url = "https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv"

    df = pd.read_csv(url)
    df = df.drop(["studyName", "Sample Number", "Individual ID", "Date Egg", "Comments", "Region"], axis = 1)
    df = df[df["Sex"] != "."]
    df = pd.get_dummies(df, columns = [
      "Island", "Stage", "Clutch Completion", "Sex"
    ])
    df = df.dropna()

    self.df = df
    self.transform = lambda x: torch.tensor(x, dtype = torch.float32)
    self.target_ix = lambda x: target_ix_dict[x]
    self.target_transform = lambda x: torch.zeros(
    3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(x), value=1)

  def __len__(self):
    return self.df.shape[0]

  def __getitem__(self, idx):
    features = self.df.drop(["Species"], axis = 1).iloc[idx,:]
    label    = self.df.iloc[idx,:]["Species"]
    features = self.transform(features)
    
    label    = self.target_ix(label)
    label    = self.target_transform(label)

    features = features.to(device)
    label = label.to(device)

    return features, label
```

```{python}
#| code-fold: true

if torch.backends.mps.is_available():
  device = "cpu"
elif torch.cuda.is_available():
  device = "cuda"
else:
  device = "cpu"
```

Here are our data sets. `PenguinsDataset` is a custom class that I implemented above, while `DataLoader` is a utility from `torch` that automatically handles things like batching and randomization for gradient descent. 

```{python}
train = PenguinsDataset()
train_dataloader = DataLoader(train, batch_size=10, shuffle=True)

val = PenguinsDataset(False)
val_dataloader = DataLoader(val, batch_size=10, shuffle=True)
```

Now we're ready to define our model. To see how we do things in torch, let's start by implementing logistic regression. To start, we need to create a predictor model that does the prediction step of logistic regression. If you'll recall, this is nothing more than matrix multiplication. The `nn.Linear` "layer" supplied by `torch` implements this: 

```{python}
from torch import nn

class penguinLogistic(nn.Module):
  def __init__(self):
    super().__init__()

    self.linear = nn.Sequential(
      nn.Linear(14, 3) # (number of features, number of class labels)
    )
  def forward(self, x):
    return self.linear(x)
```

We can optimize this model using the cross-entropy loss and the Adam optimizer. As you'll recall, logistic regression is nothing more than matrix multiplication plus the cross-entropy loss, so this is a logistic regression model! 

```{python}
model = penguinLogistic()
loss_fn = nn.CrossEntropyLoss()

learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```

Now we need to actually perform the optimization. The `torch` package gives us a lot of control over exactly how this happens, and we'll go over the details in a future lecture. 

```{python}
#| code-fold: true

def training_loop(model, train_dataloader, val_dataloader, learning_rate, epochs):

  train_size = len(train_dataloader.dataset)
  val_size = len(val_dataloader.dataset)

  train_loss_history = []
  train_acc_history  = []
  val_loss_history   = []
  val_acc_history    = []

  for t in range(epochs):
    train_loss = 0.0
    train_acc  = 0.0
    val_loss   = 0.0
    val_acc    = 0.0

    for batch, (X, y) in enumerate(train_dataloader):
      
      optimizer.zero_grad()
      pred = model(X)
      fit = loss_fn(pred, y)
      train_loss += fit.item() / train_size
      
      train_acc += (pred.argmax(dim = 1) == y.argmax(dim = 1)).sum() / train_size
      
      # Backpropagation
      
      fit.backward()
      optimizer.step()

    train_loss_history += [train_loss]
    train_acc_history  += [train_acc]

    for batch, (X, y) in enumerate(val_dataloader):
      with torch.no_grad():
        pred = model(X)
        fit = loss_fn(pred, y)
        val_loss += fit.item() / val_size
        val_acc += (pred.argmax(dim = 1) == y.argmax(dim = 1)).sum() / val_size

    val_loss_history += [val_loss]
    val_acc_history  += [val_acc]

    if t % 50 == 0:
      print(f"epoch {t}: val_loss = {val_loss}, val_accuracy = {val_acc}")
    
  return train_loss_history, train_acc_history, val_loss_history, val_acc_history


from matplotlib import pyplot as plt

def plot_histories(tlh, tah, vlh, vah):

  fig, axarr = plt.subplots(1, 2, figsize = (6, 3))

  axarr[0].plot(tlh, label = "train")
  axarr[0].plot(vlh, label = "validation")
  axarr[0].legend()
  axarr[0].set(xlabel = "epoch", title = "loss")
  axarr[0].semilogy()

  axarr[1].plot(tah, label = "train")
  axarr[1].plot(vah, label = "validation")
  axarr[1].legend()
  axarr[1].set(xlabel = "epoch", title = "accuracy")
```

Let's go ahead and train! 

```{python}
tlh, tah, vlh, vah = training_loop(model, train_dataloader, val_dataloader, 1e-4, 500)
plot_histories(tlh, tah, vlh, vah)
```

We can see that our model is doing much better than we would expect from random guessing on this problem, although it may not be competitive with many of the models that you implemented in your analysis of the penguins data set. Further training or tweaks to parameters like batch sizes and learning rates could potentially help improve performance here. 

Let's try adding a hidden layer, as in @eq-single-layer. To do this, we need to add a nonlinearity $\alpha$ and more `Linear` layers. The important points are: 

1. The first dimension of the first linear layer needs to match the number of features of the input. 
2. The final dimension of the last linear layer needs to match the number of possible labels. 
3. The final dimension of each linear layer needs to match the first dimension of the next layer. 

These rules follow directly from the need to make all the matrix multiplications turn out right. 

So, let's try a model with a single layer of 100 hidden units: 

```{python}
from torch import nn
class penguinNN(nn.Module):
  def __init__(self):
    super().__init__()
    
    self.linear_relu_stack = nn.Sequential(
          nn.Linear(14, 100), # U_1
          nn.ReLU(),          # common choice of alpha these days
          nn.Linear(100, 3)   # W
      )

  def forward(self, x):
    return self.linear_relu_stack(x)

model = penguinNN().to(device)
```

We can optimize it using the same approach as before: 

```{python}
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

tlh, tah, vlh, vah = training_loop(model, train_dataloader, val_dataloader, 1e-4, 500)
plot_histories(tlh, tah, vlh, vah)
```

This model has also done well on this task, although still not closer to perfect accuracy. It's also interesting to note that the model's loss and accuracy both vary quite wildly during the training process. This is not uncommon, but may also point to a possible benefit of using a smaller step-size. 

It might appear that we haven't really gained much from the massive costs of mathematical structure and computational architecture -- we could have done better with just tools from `sklearn`, or even our own hand-built implementations! We'll soon see the infrastructure working to our advantage when we get to problems involving large data sets with complex structure, like text and images. 



