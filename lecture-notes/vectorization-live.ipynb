{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "So far in this course, we've considered the general *supervised learning* scenario, in which we are given a feature matrix $\\mX \\in \\R^{n\\times p}$ and a target vector $\\vy \\in \\R^n$. We then solve the empirical risk minimization problem in order to choose model parameters that minimize a loss function on the training data. The exact structure of this loss function depends on things like whether we are doing classification or regression, what our computational resources are, and other considerations. \n",
    "\n",
    "But feature matrices $\\mX$ and target vectors $\\vy$ don't just exist in the world: they are *collected* and *measured*. We can think of data collection and measurement as posing three fundamental questions: \n",
    "\n",
    "- **Data collection**: Which **rows** (observations) exist in $\\mX$ and $\\vy$? \n",
    "- **Measurement**: which **columns** (features) exist in $\\mX$? \n",
    "- **Measurement**: what is the **target** $\\vy$ and how is it measured? \n",
    "\n",
    "Broadly, we can think of the complete machine learning workflow as having phases corresponding to problem definition, data collection + measurement, modeling, and evaluation. Here's roughly how this looks: \n",
    "\n",
    "```{mermaid}\n",
    "flowchart TB\n",
    "\n",
    "    subgraph problem[problem definition]\n",
    "        need[identify need]-->design_collection[design data collection]\n",
    "    end\n",
    "    subgraph measurement[data collection + measurement]\n",
    "        training[training data] \n",
    "        testing[testing data]\n",
    "    end\n",
    "    subgraph modeling\n",
    "        explore[explore data] --> engineer[engineer features]\n",
    "        engineer --> design[design model]\n",
    "    end\n",
    "    subgraph assessment\n",
    "        test --> audit\n",
    "        audit --> deploy\n",
    "        deploy-->evaluate\n",
    "    end\n",
    "    design_collection-->measurement\n",
    "    training --vectorization--> modeling\n",
    "    design --> assessment\n",
    "    testing --vectorization--> assessment\n",
    "    need-->assessment\n",
    "\n",
    "```\n",
    "\n",
    "So far, we've spent most of our time in the \"modeling\" module, especially the last two steps. We've also studied some of the ways to test and audit algorithms. Today we're going to discuss **vectorization**. We can think of vectorization as what happens *between* the collection of raw data and the use of that data as input for models.  \n",
    "\n",
    "::: {.callout-note}\n",
    "\n",
    "::: {#def-vectorization}\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "**Vectorization** is the act of assigning to each data observation a vector $\\vx$, thus forming a feature matrix $\\mX$. Formally, a **vectorization map** is a function $v:\\cD\\rightarrow \\R^p$ such that, if $d \\in \\cD$ is a data observation, then $\\vx = v(d)$ is a set of features corresponding to $d$. \n",
    "\n",
    ":::\n",
    ":::\n",
    "\n",
    "The reason that vectorization is necessary is that **machine learning models only understand numbers**. So, if our data *isn't* numbers, we need to convert it into numbers in order to use it for modeling. \n",
    "\n",
    "## What Data Needs Vectorization?\n",
    "\n",
    "Most of it! \n",
    "\n",
    "- If your data comes to you as a table or matrix containing only numbers, in which each row corresponds to exactly one observation, then you may not need to vectorize. \n",
    "- If your data comes to you in *any other form*, then you need to vectorize. \n",
    "\n",
    "Some data that usually require vectorization:\n",
    "\n",
    "- Images\n",
    "- Text\n",
    "- Audio files\n",
    "- Most genomic data\n",
    "- Etc. etc. \n",
    "\n",
    "There are tons of ways of vectorizing different kinds of data, and we're not going to cover all of them. Instead, we're going to go a little more in depth on **text vectorization**. We'll discuss image vectorization much more when we get to convolutional neural networks. [For your projects, depending on the data you want to work with, you may need to research vectorization schemes appropriate to your data.]{.aside}\n",
    "\n",
    "# Case Study: Sentiment Analysis of COVID-19 Tweets\n",
    "\n",
    "Instead of discussing text vectorization in the abstract, let's jump straight into an example. *Sentiment analysis* describes modeling techniques that aim to describe the emotional valence of text. For example, sentiment analysis is often used to automatically describe text as \"positive\"/\"happy\" or \"negative\"/\"sad\". The function below will download and return a set of training data used for sentiment analysis of tweets related to the COVID-19 pandemic. [I retrieved this data from its [original posting](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification) on Kaggle.]{.aside} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def grab_tweets(data_set = \"train\"):\n",
    "    url = f\"https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/Corona_NLP_{data_set}.csv\"\n",
    "    df = pd.read_csv(url, encoding='iso-8859-1') \n",
    "    df = df[[\"OriginalTweet\", \"Sentiment\"]]\n",
    "    return df\n",
    "    \n",
    "df_train = grab_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>You know itÂs getting tough when @KameronWild...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment\n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral\n",
       "1      advice Talk to your neighbours family to excha...            Positive\n",
       "2      Coronavirus Australia: Woolworths to give elde...            Positive\n",
       "3      My food stock is not the only one which is emp...            Positive\n",
       "4      Me, ready to go at supermarket during the #COV...  Extremely Negative\n",
       "...                                                  ...                 ...\n",
       "41152  Airline pilots offering to stock supermarket s...             Neutral\n",
       "41153  Response to complaint not provided citing COVI...  Extremely Negative\n",
       "41154  You know itÂs getting tough when @KameronWild...            Positive\n",
       "41155  Is it wrong that the smell of hand sanitizer i...             Neutral\n",
       "41156  @TartiiCat Well new/used Rift S are going for ...            Negative\n",
       "\n",
       "[41157 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "\n",
    "## Activity\n",
    "\n",
    "Chat with your group. What are **three questions** you have about how the data was collected? \n",
    "\n",
    ":::\n",
    "\n",
    "## Sketchy Labels\n",
    "\n",
    "These tweets were [labeled manually](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification/discussion/186907) by the original collector of the data. As with any setting in which humans need to make subjective decisions, there is considerable possibility for debate. For example, here is one tweet that was labeld \"**extremely positive**\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WE NEED COVID-19 TESTING FOR EVERYONE TODAY!\n",
      "I have never been afraid to leave my house for a trip to the grocery store in my life. Now I am. I don't want to bring home a virus to my loved ones. It's not me, it's them.\n",
      "#StayHomeSaveLives\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[[40338]][\"OriginalTweet\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges that can cause sketchy labels include:\n",
    "\n",
    "- Speed of labeling (it takes a LONG time to make high-quality labels)\n",
    "- Language familiarity\n",
    "- Ambiguity in the target language\n",
    "- Lots more!\n",
    "\n",
    "Almost always, when working with real-world data sets, we need to keep in mind that not only is our model approximate and our data incomplete, but the data may also be contaminated with *errors* that we aren't really able to control. [See @northcutt2021labelerrors for much more on label errors in common machine learning benchmarks.]{.aside}\n",
    "\n",
    "## Target Vectorization\n",
    "\n",
    "Our aim is to predict the `Sentiment` in terms of the text of the `OriginalTweet`. However, neither the text `OriginalTweet` nor the target `Sentiment` are numbers. So, we need to vectorize. \n",
    "\n",
    "The possible values of the `Sentiment` column are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral',\n",
       "       'Positive'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(df_train[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the target `Sentiment` is simple (although there are multiple ways). We'll construct a new target vector which is `1` if the sentiment is `Positive` or `Extremely Positive` and `0` otherwise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 1*df_train[\"Sentiment\"].str.contains(\"Positive\")\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the predictor `OriginalTweet` is much more complicated, and here we face a number of choices. \n",
    "\n",
    "### Term Frequency (TF) Vectorization\n",
    "\n",
    "In natural language processing (NLP), a data set of text is often called a *corpus*, and each observation is often called a *document*. Here, each document is a tweet. \n",
    "\n",
    "One standard vectorization technique is to construct a *term-document matrix*. In a term-document matrix, each row corresponds to a document and each column corresponds to a \"term\" (usually a word) that is present in the document. The entry $x_{ij}$ of this matrix is the number of terms that term $j$ appears in document $i$, which we'll call $\\mathrm{tf}_{ij}$. To construct a term-document matrix, we can use the `CountVectorizer` from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_df = 0.2, min_df = 0.001, stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `max_df` and `min_df` specify a range of frequencies to include. If a term is present in almost all documents (like \"the\" or \"of\"), then this term may not be a good indication of sentiment. On the other hand, if a term appears in only one or two documents, we probably don't have enough data to figure out whether it matters. Finally, the choice of `stop_words` tells our vectorizer to ignore common English words that are unlikely to carry much emotional meaning, like \"and\" or \"if\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = cv.fit(df_train[\"OriginalTweet\"])\n",
    "counts = cv.transform(df_train[\"OriginalTweet\"])\n",
    "tdm = pd.DataFrame(counts.toarray(), columns = cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youâ</th>\n",
       "      <th>yâ</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 2322 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  10  100  11  12  13  14  15  16  ...  year  years  yes  \\\n",
       "0       0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "1       0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "2       0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "3       0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "4       0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "...    ..  ...  ..  ...  ..  ..  ..  ..  ..  ..  ...   ...    ...  ...   \n",
       "41152   0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "41153   0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "41154   0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "41155   0    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "41156   2    0   0    0   0   0   0   0   0   0  ...     0      0    0   \n",
       "\n",
       "       yesterday  york  young  youtube  youâ  yâ  zero  \n",
       "0              0     0      0        0     0   0     0  \n",
       "1              0     0      0        0     0   0     0  \n",
       "2              0     0      0        0     0   0     0  \n",
       "3              0     0      0        0     0   0     0  \n",
       "4              0     0      0        0     0   0     0  \n",
       "...          ...   ...    ...      ...   ...  ..   ...  \n",
       "41152          0     0      0        0     0   0     0  \n",
       "41153          0     0      0        0     0   0     0  \n",
       "41154          0     0      0        0     0   0     0  \n",
       "41155          0     0      0        0     0   0     0  \n",
       "41156          0     0      0        0     0   0     0  \n",
       "\n",
       "[41157 rows x 2322 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our term-document matrix. Note that most of the entries are 0 because tweets are so short! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below summarizes our entire data prep pipeline, which we'll need for when we get to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tweets(df, vectorizer, train = True):\n",
    "    if train: \n",
    "        vectorizer.fit(df_train[\"OriginalTweet\"])\n",
    "    X = vectorizer.transform(df[\"OriginalTweet\"]) # term-document matrix\n",
    "    y = 1*df[\"Sentiment\"].str.contains(\"Positive\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv, y_train = prep_tweets(df_train, cv, train = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model\n",
    "\n",
    "Let's check on the base rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, always guessing that a tweet is *not* positive would be correct 56% of the time. Let's see if we can beat this using logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philchodrow/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8755011298199578"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_cv = LogisticRegression()\n",
    "LR_cv.fit(X_train_cv, y_train)\n",
    "LR_cv.score(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model achieves 87% accuracy on the training data. \n",
    "\n",
    "### Inverse Document Frequency Weighting \n",
    "\n",
    "Simple term-document matrices are good for some tasks, but in other cases it is useful to downweight terms according to their frequency in the overall training corpus. This allows our models to place greater emphasis on rarer terms, which might be more expressive of strong emotions. \n",
    "\n",
    "In term-frequency-inverse-document-frequency (TF-IDF) weighting, the entry for term $j$ in document $i$ is [Exact details of TF-IDF weightings differ; this is the one implemented by default in `sklearn`.]{.aside} \n",
    "\n",
    "$$\n",
    "\\tilde{\\mathrm{x}}_{ij} = \\overbrace{\\mathrm{tf}_{ij}}^{\\text{Term frequency}}\\times \\underbrace{\\mathrm{idf}_i}_{\\text{inverse document frequency}}\\;. \n",
    "$$\n",
    "\n",
    "Here, the *term frequency* $\\mathrm{tf}_{ij}$ is again the number of times that term $i$ appears in document $j$, while the inverse document frequency $\\mathrm{idf}_i$ is computed with the formula \n",
    "\n",
    "$$\n",
    "\\mathrm{idf}_i = \\log \\frac{1+n}{1+\\mathrm{df}_i} + 1\\;\n",
    "$$\n",
    "with $\\mathrm{df}_i$ being the total number of documents in which term $i$ appears. Finally, each row of $\\tilde{\\mathrm{x}}_{ij}$ is normalized to have unit length: \n",
    "\n",
    "$$\n",
    "x_{ij} = \\frac{x_{ij}}{\\sqrt{\\sum_{j}x_{ij}^2}}\n",
    "$$\n",
    "\n",
    "These $x_{ij}$ are then collected to form the feature matrix $\\mX$. Let's try constructing a model using TF-IDF vectorization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer(max_df = 0.2, min_df = 0.001, stop_words = 'english')\n",
    "X_train_tfidf, y_train = prep_tweets(df_train, tfidfv, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623077483781617"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_tfidf = LogisticRegression()\n",
    "LR_tfidf.fit(X_train_tfidf, y_train)\n",
    "LR_tfidf.score(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our TF-IDF model got a lower training score. At this stage, one good approach would be to choose which vectorization to use (as well as the vectorization parameters) using cross-validation. For now, we'll just go ahead and grab the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = grab_tweets(data_set = \"test\")\n",
    "X_test_cv, y_test = prep_tweets(df_test, vectorizer = cv, train = False)\n",
    "X_test_tfidf, y_test = prep_tweets(df_test, vectorizer = tfidfv, train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-Document Frequency\n",
      "0.8412322274881516\n",
      "TF-IDF\n",
      "0.8367561874670879\n"
     ]
    }
   ],
   "source": [
    "print(\"Term-Document Frequency\")\n",
    "print(LR_cv.score(X_test_cv, y_test))\n",
    "print(\"TF-IDF\")\n",
    "print(LR_tfidf.score(X_test_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, TF-IDF did a little worse than term-document frequency vectorization on the test set. \n",
    "\n",
    "## Model Inspection\n",
    "\n",
    "Let's take a moment to learn more about how our term-document frequency-based model looks at the data. One good way to do this is by looking at the confusion matrices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false negative rate is higher than the true positive rate, suggesting that our model tends to tilt negative. Let's take a look at some tweets that our model labeled as negative even though the label was positive: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we might have some further questions for the producer of this data set about how he did the labeling: don't some of these tweets look like they \"really\" should be negative? \n",
    "\n",
    "\n",
    "## Word-Based Sentiment Analysis\n",
    "\n",
    "A nice feature of linear models like logistic regression is that we can actually check the coefficient for each word in the model. This coefficient can give us important information about which words the model believes are most positive  or most negative. One easy way to get at this information is to construct a data frame with the coefficients and the words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.063696</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015816</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.035407</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002059</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.214862</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317</th>\n",
       "      <td>0.444522</td>\n",
       "      <td>young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>-0.446678</td>\n",
       "      <td>youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>-0.343226</td>\n",
       "      <td>youâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>0.274800</td>\n",
       "      <td>yâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>-0.115663</td>\n",
       "      <td>zero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2322 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          coef     word\n",
       "0    -0.063696       00\n",
       "1    -0.015816      000\n",
       "2    -0.035407       10\n",
       "3     0.002059      100\n",
       "4    -0.214862       11\n",
       "...        ...      ...\n",
       "2317  0.444522    young\n",
       "2318 -0.446678  youtube\n",
       "2319 -0.343226     youâ\n",
       "2320  0.274800       yâ\n",
       "2321 -0.115663     zero\n",
       "\n",
       "[2322 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({\"coef\" : LR_cv.coef_[0], \"word\" : cv.get_feature_names_out()})\n",
    "\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can obtain positive and negative words by sorting. Here are some of the good ones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.832914</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>3.470211</td>\n",
       "      <td>won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>3.327824</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>3.304033</td>\n",
       "      <td>hand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>3.265563</td>\n",
       "      <td>bonus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>3.261603</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>3.153919</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>3.145394</td>\n",
       "      <td>enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3.088850</td>\n",
       "      <td>dedicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>3.066329</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coef       word\n",
       "223   3.832914       best\n",
       "2280  3.470211        won\n",
       "849   3.327824     friend\n",
       "945   3.304033       hand\n",
       "241   3.265563      bonus\n",
       "916   3.261603      great\n",
       "1541  3.153919   positive\n",
       "701   3.145394      enjoy\n",
       "565   3.088850  dedicated\n",
       "2303  3.066329        wow"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(\"coef\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, here are some of the negative ones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common use for these coefficients is to assign sentiment scores to sentences. Here's a function that does this. It works by first stripping the punctuation and capitalization from a string, and then looking up each of its individual words in a dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation \n",
    "\n",
    "d = {coef_df[\"word\"].loc[i] : coef_df[\"coef\"].loc[i] for i in coef_df.index}\n",
    "\n",
    "def sentiment_of_string(s):\n",
    "    no_punc = s\n",
    "    for punc in punctuation:\n",
    "        no_punc = no_punc.replace(punc, \"\")\n",
    "    \n",
    "    words = no_punc.lower().split()\n",
    "    return np.mean([d[word] for word in words if word in d ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is the basis of [The Hedonometer](https://hedonometer.org/timeseries/en_all/?from=2021-09-23&to=2023-03-22), a large-scale Twitter sentiment analysis tool from our friends at the University of Vermont. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "::: {callout-warning}\n",
    "\n",
    "## Activity\n",
    "\n",
    "There is a very important kind of information that is *not* captured by term-document matrices, even with inverse-document-frequency weighting. Consider the following two sentences: \n",
    "\n",
    "1. \"I like pears, not apples.\"\n",
    "2. \"I like apples, not pears.\"\n",
    "\n",
    "Would these sentences have different representations in a term-document matrix?\n",
    "\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
