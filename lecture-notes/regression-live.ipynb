{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  Least-Squares Linear Regression\n",
        "author: Phil Chodrow\n",
        "bibliography: ../refs.bib\n",
        "format: \n",
        "  html: \n",
        "    code-fold: false\n",
        "    cache: true\n",
        "    callout-appearance: minimal\n",
        "    cap-location: margin\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.hidden}\n",
        "$$\n",
        "\\newcommand{\\R}{\\mathbb{R}}\n",
        "\\newcommand{\\cP}{\\mathcal{P}}\n",
        "\\newcommand{\\vx}{\\mathbf{x}}\n",
        "\\newcommand{\\vp}{\\mathbf{p}}\n",
        "\\newcommand{\\vy}{\\mathbf{y}}\n",
        "\\newcommand{\\vz}{\\mathbf{z}}\n",
        "\\newcommand{\\vd}{\\mathbf{d}}\n",
        "\\newcommand{\\mX}{\\mathbf{X}}\n",
        "\\newcommand{\\mR}{\\mathbf{R}}\n",
        "\\newcommand{\\mW}{\\mathbf{W}}\n",
        "\\newcommand{\\mY}{\\mathbf{Y}}\n",
        "\\newcommand{\\mZ}{\\mathbf{Z}}\n",
        "\\newcommand{\\vw}{\\mathbf{w}}\n",
        "\\newcommand{\\vr}{\\mathbf{r}}\n",
        "\\newcommand{\\vq}{\\mathbf{q}}\n",
        "\\newcommand{\\mP}{\\mathbf{P}}\n",
        "\\newcommand{\\vzero}{\\mathbf{0}}\n",
        "\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n",
        "\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n",
        "\\newcommand{\\one}[1]{\\mathbb{1}\\left[ #1 \\right]}\n",
        "\\newcommand{\\cL}{\\mathcal{L}}\n",
        "\\newcommand{\\cD}{\\mathcal{D}}\n",
        "\\newcommand{\\cM}{\\mathcal{M}}\n",
        "\\newcommand{\\mA}{\\mathbf{A}}\n",
        "\\newcommand{\\vtheta}{\\boldsymbol{\\theta}}\n",
        "\\newcommand{\\vsigma}{\\boldsymbol{\\sigma}}\n",
        "\\newcommand{\\norm}[1]{\\lVert #1 \\rVert}\n",
        "\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n",
        "\\newcommand{\\prob}[1]{\\mathbb{P}\\left[#1\\right]}\n",
        "\\newcommand{\\E}{\\mathbb{E}}\n",
        "\\newcommand{\\dd}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
        "\n",
        "\\usepackage{amsmath}\n",
        "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
        "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
        "$$\n",
        "\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far in this course, we've focused exclusively on *classification* tasks: how to predict a *categorical label* for each data point. The other important task we need to consider is *regression*, in which we predict a real number for each data point based on its features. Here's the stereotypical example: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "w0 = -0.5\n",
        "w1 =  0.7\n",
        "\n",
        "n = 100\n",
        "x = np.random.rand(n, 1)\n",
        "y = w1*x + w0 + 0.1*np.random.randn(n, 1)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "labels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at this data, we can see an apparent linear trend that we would like to use in order to make prediction on new data points.  \n",
        "\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "We're going to focus on *least-squares linear regression*. The nice thing about least-squares linear regression is that it falls perfectly into our framework of convex linear models. In least-squares linear regression, we still make predictions of the form $\\hat{y}_i = \\bracket{\\vw, \\vx_i}$, since these are exactly linear predictions! The loss function is $\\ell(\\hat{y}, y) = (\\hat{y} - y)^2$, the *squared error*, which is convex. The empirical risk minimization problem is \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{\\vw} &= \\argmin_{\\vw} \\; L(\\vw) \\\\ \n",
        "          &= \\sum_{i = 1}^n \\ell(\\hat{y}_i, y_i) \\\\ \n",
        "          &= \\argmin_{\\vw} \\sum_{i = 1}^n \\left(\\bracket{\\vw, \\vx_i} - y_i \\right)^2\\;. \n",
        "\\end{aligned}\n",
        "$$\n",
        "It's useful to write this in a more compact way using matrix-vector notation: the loss function $L(\\vw)$ can be written \n",
        "\n",
        "[Reminder: $\\mX \\in \\R^{n\\times p}$, $\\vw \\in \\R^{p}$, $\\mX\\vw \\in \\R^n$, which is the same dimension as $\\vy$. ]{.aside}\n",
        "$$\n",
        "L(\\vw) = \\norm{\\mX\\vw - \\vy}_2^2\\;.\n",
        "$$\n",
        "\n",
        "So, we want to solve the problem \n",
        "\n",
        "$$\n",
        "\\hat{\\vw} = \\argmin_{\\vw} \\; L(\\vw) = \\argmin_{\\vw} \\; \\norm{\\mX\\vw - \\vy}_2^2\\;.\n",
        "$${#eq-least-squares}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Solution Methods\n",
        "\n",
        "There are a *lot* of ways to solve @eq-least-squares. Let's start by taking the gradient with respect to $\\hat{\\vw}$. Using the multivariate chain rule, this is \n",
        "\n",
        "$$\n",
        "\\nabla L(\\vw) = 2\\mX^T(\\mX\\vw - \\vy)\\;.\n",
        "$${#eq-gradient}\n",
        "\n",
        "One way to approach the linear regression problem is with gradient descent: repeat the iteration \n",
        "\n",
        "$$\n",
        "\\vw^{(t+1)} \\gets \\vw^{(t)} - 2\\alpha \\mX^T(\\mX\\vw^{(t)} - \\vy)\n",
        "$$\n",
        "\n",
        "to convergence. As it turns out, there's also an explicit formula involving a matrix inversion that we can obtain by using the condition $\\nabla L(\\vw) = \\vzero$ which must hold at the minimum. Plugging in our expression for $L(\\vw)$, we get   \n",
        "\n",
        "$$\n",
        "\\vzero = \\mX^T(\\mX\\hat{\\vw} - \\vy)\\;.\n",
        "$$\n",
        "\n",
        "To start solving for $\\hat{\\vw}$, we can move $\\mX^T\\vy$ to the other side: \n",
        "\n",
        "$$\n",
        "\\mX^T\\mX\\hat{\\vw} = \\mX^T\\vy\\;.\n",
        "$$\n",
        "\n",
        "[This requires that there are at least $p$ linearly independent rows of $\\mX$. In particular, $\\mX$ must have at least as many rows as it has columns.]{.aside}\n",
        "Now, *provided that the matrix $\\mX^T\\mX$ is of full rank*,  we can multiply both sides by $(\\mX^T\\mX)^{-1}$ to obtain \n",
        "$$\n",
        "\\hat{\\vw} = (\\mX^T\\mX)^{-1}\\mX^T\\vy\\;,\n",
        "$${#eq-regression-explicit}\n",
        "which is an explicit formula for $\\hat{\\vw}$. \n",
        "\n",
        "Let's see if we can use this to compute predictions for our fake data above. In order for this formula to work, we need to ensure that $\\mX$ is padded with a vector of ones. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad(X):\n",
        "    return np.append(X, np.ones((X.shape[0], 1)), 1)\n",
        "\n",
        "X = pad(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use the formula: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test this out on our fake data: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not bad! \n",
        "\n",
        "::: {.callout-warning} \n",
        "\n",
        "## Activity 1: Computational Complexity of Exact Least-Squares Regression\n",
        "\n",
        "Multiplying a $k\\times \\ell$ matrix with an $\\ell \\times k$ matrix using the standard algorithm has time complexity $k \\ell^2$. Inverting a $k\\times k$ matrix (when the inverse exists) has time complexity $k^3$. Left-multiplying a $k\\times k$ matrix and a $k$-vector has time complexity $k^2$. \n",
        "\n",
        "With these facts in mind, describe the time complexity of computing $\\hat{\\vw}$ using @eq-regression-explicit in terms of the number of data points $n$ and the number of features $p$. What would the computational bottleneck when the number of data points $n$ is very large? What about what the number of features $p$ is very large? \n",
        "\n",
        "[As a reminder, we are talking about the formula $\\hat{\\vw} = (\\mX^T\\mX)^{-1}\\mX^T\\vy\\;,$.]{.aside}\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-warning} \n",
        "\n",
        "## Activity 2: Computational Complexity of Gradient Descent\n",
        "\n",
        "As you'll implement in your blog post, gradient descent can actually be implemented via the iteration \n",
        "\n",
        "$$\n",
        "\\vw^{(t+1)} = \\vw^{(t)} - \\alpha (\\mP \\vw^{(t)} - \\vq)\\;,\n",
        "$$\n",
        "\n",
        "where $\\mP$ is a $p \\times p$ matrix and $\\vq$ is a $p$-vector. [The trick is to precompute some matrix products]{.aside} What is the time complexity of a single iteration of gradient descent? \n",
        "\n",
        "Of course, a full analysis of the time complexity of the gradient descent algorithm as a whole requires knowing how many iterations are necessary to achieve acceptable accuracy, which is a much harder problem. \n",
        "\n",
        ":::\n",
        "\n",
        "## Scoring Linear Models\n",
        "\n",
        "A good, simple way to score linear models is by using the loss function directly: smaller values are better! In `scikit-learn`, regression models are instead scored using an affine transformation of the loss function called the *coefficient of determination*. The coefficient of determination is `1` when the model fits the data perfectly with no errors. It can be arbitrarily negative (e.g. `-8.7`) if the model fits the data very poorly. \n",
        "\n",
        "For a quick illustration, here's the `scikit-learn` implementation of linear regression: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's evaluate our model on similar, unseen data: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 100\n",
        "x_val = np.random.rand(n, 1)\n",
        "y_val = w1*x_val + w0 + 0.1*np.random.randn(n, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As usual, gaps between the training and validation scores suggest the *possibility* of overfitting, although further investigation is required to see whether improvement on validation data is possible. \n",
        "\n",
        "## Incorporating Features\n",
        "\n",
        "Sometimes (most of the time), the patterns we are looking for in our data are not actually linear. For example: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = 2*np.pi*np.random.rand(n, 1)\n",
        "y = np.sin(x) + 0.2*np.random.randn(n, 1)\n",
        "\n",
        "plt.scatter(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like in the case of classification, we can use *feature maps* to learn nonlinear patterns. For example: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feature matrix $\\Phi$ now plays the role of $\\mX$, and we can use the same formula as earlier: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predictions are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(x, y)\n",
        "\n",
        "x_lin = np.linspace(0, 2*np.pi, 1001)[:,np.newaxis]\n",
        "PHI_lin = p.fit_transform(x_lin)\n",
        "y_trend = PHI_lin@w_hat\n",
        "\n",
        "plt.plot(x_lin, y_trend, color = \"black\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hmmm, this is not a very impressive fit. Let's wrap this process in a function and do a few experiments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def poly_viz(deg, ax):\n",
        "    p = PolynomialFeatures(deg)\n",
        "    PHI = p.fit_transform(x)\n",
        "    w_hat = np.linalg.inv(PHI.T@PHI)@PHI.T@y\n",
        "    x_lin = np.linspace(x.min(), x.max(), 1001)[:,np.newaxis]\n",
        "    PHI_lin = p.fit_transform(x_lin)\n",
        "    y_trend = PHI_lin@w_hat\n",
        "    ax.scatter(x, y)\n",
        "    ax.plot(x_lin, y_trend, color = \"black\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in classification, the use of polynomial features makes us susceptible to overfitting, and validation or cross-validation should be used in order to select a good degree. \n",
        "\n",
        "### Kernel Regression\n",
        "\n",
        "*Kernel methods* offer a theoretically-grounded approach for dealing with nonlinearity in both classification and regression. [You can implement a kernel classification method in [this blog post](../assignments/blog-posts/blog-post-kernel-logistic.qmd).]{.aside} In kernel methods, the feature vector corresponding to each data point $\\vx_i$ is actually in terms of all the other points: \n",
        "\n",
        "$$\n",
        "\\phi(\\vx_i) = \\left(\\begin{matrix}\n",
        "                k(\\vx_i, \\vx_1) \\\\ \n",
        "                k(\\vx_i, \\vx_2) \\\\ \n",
        "                \\cdots \\\\ \n",
        "                k(\\vx_i, \\vx_n) \\\\ \n",
        "              \\end{matrix}\\right)\n",
        "$$\n",
        "\n",
        "Here, $k:\\R^{p} \\times \\R^{p} \\rightarrow \\R$ is a special function called a *kernel function*. Usually the kernel function is a measure of how similar two points are. A very common and useful kernel is the radial basis function (RBF) kernel \n",
        "\n",
        "$$\n",
        "k(\\vx_1, \\vx_2) = e^{-\\gamma \\norm{\\vx_1 - \\vx_2}^2}\\;.\n",
        "$$\n",
        "\n",
        "This function is largest when $\\vx_1 = \\vx_2$, and decreases as these two vectors become farther apart. The idea of kernel methods is that the prediction can be expressed as a weighted sum of the target values at nearby points. \n",
        "\n",
        "Kernel methods have extremely beautiful mathematics behind them and are fun to implement, but for today we can just show the implementation in `scikit-learn`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different values of `gamma` will result in more or less \"wiggly\" fits. `gamma` should be tuned using validation or cross validation in order to protect against overfitting.  \n",
        "\n",
        "## Activity\n",
        "\n",
        "\n",
        "The following code produces a random feature matrix $\\mX$ and weight vector $\\vw$: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.random.rand(10, 3)\n",
        "X = pad(X)\n",
        "w = np.random.rand(X.shape[1])\n",
        "\n",
        "y = X@w + np.random.randn(X.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Implement a `predict` function that computes `y_hat` from `X` and `w` (*hint: don't overthink it*). \n",
        "2. Implement a `score` function that accepts `X`, `y`, and `w` as arguments and computes the *coefficient of determination* of the predictions. The coefficient of determination is \n",
        "$$\n",
        "c = 1 - \\frac{\\sum_{i = 1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i = 1}^n (\\bar{y}_i - y_i)^2}\\;,\n",
        "$$\n",
        "where  $\\bar{y} = \\frac{1}{n} \\sum_{i = 1}^n$. \n",
        "\n",
        "You can modify these functions and use them in the [blog post on linear regression](../assignments/blog-posts/blog-post-bias-allocative.qmd). \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
