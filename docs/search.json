[
  {
    "objectID": "lecture-notes/intro-classification.html",
    "href": "lecture-notes/intro-classification.html",
    "title": "Introduction to Classification and Auditing",
    "section": "",
    "text": "In this lecture, we’ll make our first acquaintance to the classification task. Classification is a form of supervised machine learning. Here’s the big-picture version of the supervised ML task.\nWe are given:\n\nA set of observations of predictor variables. We’ll call the \\(i\\)th such observation \\(\\mathbf{x}_i\\). We write it this way because \\(\\mathbf{x}_i\\) is usually a vector of multiple variables, often called features or covariates. We often collect these observations into a matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), where \\(n\\) is the number of observations and \\(p\\) is the total number of features.\nA set of observations of a single target variable. We’ll call the \\(i\\)th such observation \\(y_i\\). We write it this way because (at least in this course) \\(y_i\\) will always be a scalar number, rather than a vector. We can collect these observations into a (column) vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\).\nWe can refer to a single observation as a pair \\((\\mathbf{x}_i, y_i)\\).\n\nBig picture, the supervised machine learning task is to use \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to find a function \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) with the property that\n\\[\nf(\\mathbf{x}) \\approx y\n\\]\nWhat does it mean for \\[f(\\mathbf{x}) \\approx y\\]? This requires mathematical fleshing-out that we’ll do very soon.\nfor new observations \\((\\mathbf{x}, y)\\). We can think of the function \\(f\\) as an expression of the (unknown) relationship between the features \\(\\mathbf{x}\\) and the target \\(y\\). If we can find an approximation of that pattern, then we’ll have the ability to make predictions.\nWe often use \\(\\hat{y} = f(\\mathbf{x})\\) to denote the predicted value for \\(y\\) based on \\(\\mathbf{x}\\). So, we want to choose \\(f\\) so that \\(\\hat{y} \\approx y\\).\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/intro-classification.html#classification",
    "href": "lecture-notes/intro-classification.html#classification",
    "title": "Introduction to Classification and Auditing",
    "section": "Classification",
    "text": "Classification\nWe use the vector \\(\\mathbf{y}\\) to hold our observations of the target variable. We have assumed that each observation of the target variable is a real number (i.e. an element of \\(\\mathbb{R}\\)). This looks reasonable for when the thing we want to predictive is a real number (like a stock price or a probability to like a post), but what about when we want to predict a categorical label? In this case, we can simply encode labels using integers: \\(0\\) for one category, \\(1\\) for the next category, \\(2\\) for the one after that, and so on."
  },
  {
    "objectID": "lecture-notes/intro-classification.html#the-compas-recidivism-prediction-algorithm",
    "href": "lecture-notes/intro-classification.html#the-compas-recidivism-prediction-algorithm",
    "title": "Introduction to Classification and Auditing",
    "section": "The COMPAS Recidivism Prediction Algorithm",
    "text": "The COMPAS Recidivism Prediction Algorithm\nCriminal recidivism occurs when a person is convicted of a crime, completes the legal terms of their punishment, and is then convicted of another crime after release. In the American penal system, predictions of recidivism play a role in determining whether or not a defendant will be released on bail before trial or granted parole after serving a portion of a prison sentence. In other words, the belief of the court about whether a person is likely to commit a future crime can have concrete consequences for that person’s current and future freedom. Of course, it’s difficult for a human to predict whether a defendant is likely to commit a future crime. Furthermore, humans are subject to bias. Wouldn’t it be nice if we could use a machine learning algorithm to make this prediction for us?\nIn 2016, the journalism website ProPublica published an investigative story on COMPAS, a machine learning algorithm used to predict recidivism in Broward County, Florida. They obtained data for criminal defendants in Broward County in the years 2013 and 2014. These data include the COMPAS predictions, as well as demographic information (like age, gender, and race) and legal information (e.g. the crime with which the defendant was charged). The data also include an indicator of whether or not the defendant went on to be arrested of a crime within the two years following their initial trial.\nThe COMPAS algorithm actually uses information about the defendant beyond what is shown in this table; here is an example of the survey used for COMPAS to form its prediction.\n\n\n\n\n\n\nActivity\n\n\n\nHere are three concepts:\n\nDemographic data and legal information related to a defendant.\nWhether or not the defendant proceeds to be arrested for a crime within the two years following their initial trial.\nThe COMPAS algorithm.\n\nMatch these three concepts to the three mathematical symbols in the relationship\n\\[f(\\mathbf{x}) \\approx y\\].\n\n\nLet’s look at an excerpt of the data that ProPublica obtained. I have chosen only a subset of the columns and I have filtered out some of the rows as well. The hidden code saves the data in a pandas.DataFrame called df, and then views it.\nClick the little arrow to the right to view the code I used to display this table.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/compas-scores-two-years.csv\")\n\n# filtering as in the original analysis by ProPublica\n# https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n\ndf = df[df.days_b_screening_arrest <= 30]\ndf = df[df.days_b_screening_arrest >= -30]\ndf = df[df.is_recid != -1]\ndf = df[df.c_charge_degree != \"O\"]\ndf = df[df.score_text != \"NA\"]\ndf = df[(df.race == \"African-American\") | (df.race == \"Caucasian\")]\n\ncol_list = df.columns\n\ndf[\"compas_prediction\"] = 1*(df.score_text != \"Low\")\ndf = df.reset_index()\ncols = [\"race\", \"age\", \"compas_prediction\", \"two_year_recid\"]\ndf = df[cols]\n\ndf\n\n\n\n\n\n\n  \n    \n      \n      race\n      age\n      compas_prediction\n      two_year_recid\n    \n  \n  \n    \n      0\n      African-American\n      34\n      0\n      1\n    \n    \n      1\n      African-American\n      24\n      0\n      1\n    \n    \n      2\n      Caucasian\n      41\n      1\n      1\n    \n    \n      3\n      Caucasian\n      39\n      0\n      0\n    \n    \n      4\n      Caucasian\n      27\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5273\n      African-American\n      30\n      0\n      1\n    \n    \n      5274\n      African-American\n      20\n      1\n      0\n    \n    \n      5275\n      African-American\n      23\n      1\n      0\n    \n    \n      5276\n      African-American\n      23\n      0\n      0\n    \n    \n      5277\n      African-American\n      33\n      0\n      0\n    \n  \n\n5278 rows × 4 columns\n\n\n\nThe column compas_prediction is the COMPAS algorithm’s prediction of whether the individual will be arrested again.\n\n0 means “no:” according to COMPAS, the individual does not have an elevated risk to be arrested for a crime within the next two years.\n1 means “yes:” according to COMPAS, the individual does have an elevated risk to be arrested for a crime within the next two years.\n\nwhere 0 means The column two_year_recid records the actual outcome: 0 means “no,” the individual was not arrested within the next two years, while 1 means “yes,” the individual was arrested within the next two years.\nThere are a number of other columns that I have omitted, including the defendant name, the severity of the criminal charge, whether or not the charge is for a violent crime, presence of a prior record, sex, and other information.\n\n\n\n\n\n\nDiscussion\n\n\n\nTake some time to look at the excerpted data set and my description of it. What questions do you have when you look at the data? Try to find at least two questions about:\n\nHow the data was collected/gathered/presented by me.\nWhat patterns might be present in the data? What concerns might you have that you would want to check?"
  },
  {
    "objectID": "lecture-notes/intro-classification.html#evaluating-classification-algorithms",
    "href": "lecture-notes/intro-classification.html#evaluating-classification-algorithms",
    "title": "Introduction to Classification and Auditing",
    "section": "Evaluating Classification Algorithms",
    "text": "Evaluating Classification Algorithms\nWas COMPAS successful at making its predictions? There are lots of ways to assess this.\n\nOverall Accuracy\nOne way is the overall accuracy of the predictions: how often was it the case that the predictions were correct? The code below computes the proportion of the time that the COMPAS prediction matched reality:\n\ndf[\"accurate\"] = df[\"compas_prediction\"] == df[\"two_year_recid\"]\ndf[\"accurate\"].mean()\n\n0.6582038651004168\n\n\nIs this a good result? We can compare it to the performance of a hypothetical algorithm that simply always predicted that the individual would not reoffend.\n\n\nCode\n(1-df[\"two_year_recid\"]).mean()\n\n\n0.5295566502463054\n\n\nThis is an example of comparing against a base rate. There’s no formal definition of a base rate, but you can think of it as the performance of the best approach to the problem that doesn’t involve anything fancy. Here, the base rate is 53% and the accuracy of COMPAS is 66%, indicating that the COMPAS algorithm is significantly outperforming the base rate."
  },
  {
    "objectID": "lecture-notes/intro-classification.html#classification-rates",
    "href": "lecture-notes/intro-classification.html#classification-rates",
    "title": "Introduction to Classification and Auditing",
    "section": "Classification Rates",
    "text": "Classification Rates\nWhile accuracy is a useful metric for classification problems, it’s useful to break it down in more detailed ways. In the case of binary classification, there are four cases:\nRecall that \\(\\hat{y}\\) is just another name for \\(f(\\mathbf{x})\\), the predicted value of \\(y\\) based on \\(\\mathbf{x}\\).\n\nIf \\(y = 1\\) and \\(\\hat{y} = 1\\), we have a true positive.\nIf \\(y = 0\\) and \\(\\hat{y} = 1\\), we have a false positive.\nIf \\(y = 1\\) and \\(\\hat{y} = 0\\), we have a false negative.\nIf \\(y = 0\\) and \\(\\hat{y} = 0\\), we have a true negative.\n\nThe false positive rate is the fraction of all negative events for which the prediction is positive:\nHere \\(\\mathbb{1}\\) is the indicator function that is 1 if its arguments all evaluate to true and 0 otherwise.\n\\[\\mathrm{FPR}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\sum_{i=1}^n \\mathbb{1}(\\hat{y}_i = 1, y_i = 0)}{\\sum_{i = 1}^n \\mathbb{1}(y_i = 0)}\\]\nWe can calculate the FPR like this: numpy boolean arrays and pandas boolean columns can be multiplied to do entrywise Boolean and.\n\n\nCode\ndef FPR(y, y_hat):\n    return sum((y_hat == 1)*(y == 0))/sum(y == 0) \n\n\nOne can also define the False Negative Rate, True Positive Rate, and True Negative Rate. Let’s also do the False Negative Rate:\n\n\nCode\ndef FNR(y, y_hat):\n    return sum((y_hat == 0)*(y == 1))/sum(y == 1) \n\n\n\n\nCode\ny = df[\"two_year_recid\"]\ny_hat = df[\"compas_prediction\"]\nFPR(y, y_hat), FNR(y, y_hat)\n\n\n(0.3302325581395349, 0.35481272654047524)\n\n\nIn other words:\n\nOf people who were not arrested within two years, the COMPAS algorithm wrongly predicted that 33% of them would be arrested within two years (but correctly predicted that 67% of them would not be).\nOf people who were arrested within two years, the COMPAS algorithm wrongly predicted that 35% of them would not be arrested within two years (but correctly predicted that 65% of them would be).\n\n\n\nCode\nfor label, fun in {\"False positive rates\": FPR, \"False negative rates\" : FNR}.items():\n    print(label)\n    print(df.groupby(\"race\").apply(lambda df: fun(df[\"two_year_recid\"], df[\"compas_prediction\"])))\n    print(\"\")\n\n\nFalse positive rates\nrace\nAfrican-American    0.423382\nCaucasian           0.220141\ndtype: float64\n\nFalse negative rates\nrace\nAfrican-American    0.284768\nCaucasian           0.496350\ndtype: float64"
  },
  {
    "objectID": "lecture-notes/perceptron.html",
    "href": "lecture-notes/perceptron.html",
    "title": "Introduction to Classification: The Perceptron",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\newcommand{\\one}[1]{\\mathbb{1}\\left[ #1 \\right]}\n\\newcommand{\\cL}{\\mathcal{L}}\n\\newcommand{\\norm}[1]{\\lVert #1 \\rVert}\n\\]\nIn this lecture, we’ll study one of the oldest machine learning algorithms: the perceptron. Invented in 1943 but not actually implemented in hardware until 1958, the perceptron is still relevant today as a fundamental building-block of modern deep neural networks. Indeed, one of the implementations of neural networks in scikit-learn is still called the “multilayer perceptron.”\nWhen first announced, the perceptron algorithm also displayed one of the first examples of AI Hype®. The New York Times uncritically repeated claims by a Navy rep that the perceptron algorithm would be the “embryo” of a computer that would “walk, talk, see, write, reproduce itself, and be conscious of its existence.” As we study and implement the perceptron, you may wish to reflect on what you are doing and decide for yourself whether you believe that you are building the “embryo” of any such capabilities yourself.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/perceptron.html#illustration",
    "href": "lecture-notes/perceptron.html#illustration",
    "title": "Introduction to Classification: The Perceptron",
    "section": "Illustration",
    "text": "Illustration\nNow let’s go ahead and run the perceptron algorithm on some data. First we should set up our feature matrix \\(\\mX\\) and target vector \\(\\vy\\).\n\n\nCode\nX = np.append(np.column_stack((X1, X2)), np.column_stack((X3, X4)), axis = 0) # feature matrix\ny = 2*(np.arange(0, 200) >= 100) - 1 # target vector\n\n\nHere are the first few rows of the feature matrix:\n\n\nCode\nX[0:5, :]\n\n\narray([[-1.0856306 ,  0.64205469],\n       [ 0.99734545, -1.97788793],\n       [ 0.2829785 ,  0.71226464],\n       [-1.50629471,  2.59830393],\n       [-0.57860025, -0.02462598]])\n\n\nAnd here are the corresponding values of the target vector:\n\n\nCode\ny[0:5]\n\n\narray([-1, -1, -1, -1, -1])\n\n\n\n\nCode\nnp.random.seed(123456)\nw = np.random.rand(3)\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    plot_scatter(X1, X2, X3, X4, ax, legend = False)\n    draw_line(w, -10, 10, ax, color = \"black\", linestyle = \"dashed\")\n    w, i, loss = perceptron_update(X, y, w)    \n    ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\")\n    draw_line(w, -10, 10, ax, color = \"black\")\n    ax.set_title(f\"loss = {loss}\")\n    \nplt.tight_layout()\n\n\n\n\n\nFigure 3: Several iterations of the perceptron algorithm. In each panel, the dashed line is the hyperplane corresponding to the previous weight vector \\(\\vw^{(t)}\\), while the solid line is the hyperplane for the updated weight vector \\(\\vw^{t+1}\\). The empty circle is the point \\(i\\) used in the update; only iterations in which \\(i\\) was a mistake are shown, with the exception of the final two iterations (by which the algorithm has converged). The loss is computed as in Equation 4. (The perceptron update itself takes place using a function called perceptron_update whose implementation I have intentionally hidden – you’ll implement a version yourself in a blog post!)"
  },
  {
    "objectID": "lecture-notes/perceptron.html#convergence-of-the-perceptron-algorithm",
    "href": "lecture-notes/perceptron.html#convergence-of-the-perceptron-algorithm",
    "title": "Introduction to Classification: The Perceptron",
    "section": "Convergence of the Perceptron Algorithm",
    "text": "Convergence of the Perceptron Algorithm\nIs the perceptron algorithm guaranteed to terminate? And if so, is it guaranteed to find a weight vector \\(\\tilde{\\vw}\\) that perfectly separates the two data classes?\n\n\n\n\n\n\n\nDefinition 1 (Linear Separability) A data set with feature matrix \\(\\mX \\in \\R^{n\\times k}\\) and target vector \\(y\\in \\{0, 1\\}\\) is linearly separable if there exists a weight vector \\(\\tilde{\\vw}\\) such that, for all \\(i \\in [n]\\),\n\\[\n\\bracket{\\tilde{\\vw}, \\tilde{\\vx}_i} > 0 \\Leftrightarrow y = 1\\;.\n\\]\n\n\n\n\nTake a moment to convince yourself of the following:\n\n\n\n\n\n\n\nProposition 1 (Nonconvergence of perceptron for nonseparable data) \nSuppose that \\((\\mX, \\vy)\\) is not linearly separable. Then, the perceptron update does not converge. Furthermore, at no iteration of the algorithm is it the case that \\(\\cL(\\tilde{\\vw}) = 0\\).\n\n\n\n\nIt’s not as obvious that, if the data is linearly separable, then the perceptron algorithm will converge to a correct answer. Perhaps surprisingly, this is also true:\n\n\n\n\n\n\n\nTheorem 1 (Convergence of perceptron for separable data) Suppose that \\((\\mX, \\vy)\\) is linearly separable. Then:\n\nThe perceptron algorithm converges in a finite number of iterations to a vector \\(\\tilde{\\vw}\\) that separates the data.\n\nDuring the running of the perceptron algorithm, the total number of updates made is no more than\n\n\\[\\frac{2 + r(\\mX)^2}{\\gamma(\\mX, \\vy)}\\;,\\]\nwhere \\(r(\\mX) = \\max_{i \\in [n]} \\norm{\\vx_i}\\) and \\(\\gamma(\\mX, \\vy)\\) is a geometric measure called the margin of how far apart the two label classes are.\n\n\n\n\nFor a proof of Theorem 1, see p. 37-44 of Hardt and Recht (2021)."
  },
  {
    "objectID": "lecture-notes/perceptron.html#close-out-activity",
    "href": "lecture-notes/perceptron.html#close-out-activity",
    "title": "Introduction to Classification: The Perceptron",
    "section": "Close-out Activity",
    "text": "Close-out Activity\n\n\n\n\n\n\nStart Implementing the Perceptron Algorithm\n\n\n\nSuppose that you have a numpy.Array X of features and an np.Array y of binary labels. Assume that X does NOT contain a column of 1s; that is, it corresponds to \\(\\mX\\) rather than \\(\\tilde{\\mX}\\) from the notes above. Additionally, Assume that the labels in y are 0s and 1s rather than -1s and 1s. This is not the mathematically convenient setup, but is the one that is most frequently seen in machine learning software.\nAt the board, write as much code as you can to achieve some of the following tasks. Please work with a partner. It’s ok to pick and choose which of these to try; but you should work together (i.e. in conversation) rather than in parallel.\n\nDetermine n (the number of data points) and p (the number of features) from X.\nModify X into X_ (which contains a column of 1s and corresponds to \\(\\tilde{\\mX}\\)). This one I’ll give you for free:\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nModify y into an array y_ of 0s and 1s, corresponding to the version we use in the notes above.\nInitialize a random weight vector w_ with appropriate dimensions, corresponding to \\(\\tilde{\\vw}^{(0)}\\).\nGenerate a random index i between 0 and n-1.\nExtract the ith row of X_, which corresponds to \\(\\tilde{\\vx}_i\\).\n\nCompute \\(\\ell_i^{(0)} = \\bracket{\\tilde{\\vw}^{(0)}, \\tilde{\\vx}_i}\\).\n\nThese items will give you a head start on the blog post in which you will construct a full implementation of the perceptron algorithm."
  },
  {
    "objectID": "lecture-notes/erm.html",
    "href": "lecture-notes/erm.html",
    "title": "Models, Algorithms, and Learning",
    "section": "",
    "text": "$$\n$$\nIn this lecture, we are going to develop our core theoretical framework for supervised prediction. Supervised means that we are trying to predict something for which there is, or could be, a “ground-truth answer.” Some examples of supervised tasks are:\nIn each of these cases, you could find out whether your prediction was right or wrong just by waiting and checking. Did you predict that a person is likely to commit a crime within the next few years? Wait two years and find out whether you were right.\nIn contrast, unsupervised algorithms generate output for which there is no firm right answer. Unsupervised machine learning tasks aim to do things like “find patterns” or “generate realistic examples.” Large language models (LLMs) like ChatGPT are perhaps the most prominent examples of unsupervised models these days.\nBig picture, the goal of supervised learning is to find a function that takes in some features and uses them to make a prediction that is usually right. Heuristically, we’re looking for a function \\(f\\) that accepts some features \\(x_1,\\ldots,x_p\\) and gives a prediction \\(\\hat{y}\\) that is “close” to the real answer \\(y\\):\n\\[\nf(x_1,\\ldots,x_p) = \\hat{y} \\approx y\\;.\n\\]\nWhat does “\\(\\approx\\)” actually mean in this context? Briefly, we mean that \\(\\hat{y}\\) is usually “close” to \\(y\\) when measured in a certain way.\nThe goal of supervised learning is to “train” a “model” that will make “good” “predictions” on “data.” We need to cash out each of these terms.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/erm.html#data-generating-distributions",
    "href": "lecture-notes/erm.html#data-generating-distributions",
    "title": "Models, Algorithms, and Learning",
    "section": "Data Generating Distributions",
    "text": "Data Generating Distributions\nIntuitively, a data generating distribution is an expression of our expectations of what the world looks like. For example, let’s suppose we are trying to predict whether someone enjoys skiing. So, we do a survey in which we ask two questions:\n\nOn a scale from 1 to 10, how much do you enjoy being outdoors in cold weather?\nDo you enjoy skiing? (yes/no)\n\nHere, we have a single feature \\(x\\in [10]\\) and a binary outcome \\(y \\in \\{0,1\\}\\). We might imagine that when \\(x\\) is larger, it is more likely for \\(y\\) to be equal to 1. Here’s a probabilistic model that expresses this idea:\n\\[\n\\begin{align}\n    \\mathbb{P}\\left[X = x\\right] &= 1/10 \\quad \\forall x \\in [10] \\\\\n    \\mathbb{P}\\left[Y = 1 | X\\right] &= \\frac{x}{11}\\;.\n\\end{align}\n\\]\nThis probabilistic model is an example of a data generating distribution. The reason it’s called this is that you could “generate” a data point from it: first pick a random value of \\(x\\) uniformly between 1 and 10. Then, to generate \\(y\\), flip a weighted coin with probability of heads equal to \\(x/11\\).\nHere’s some “data” sampled from this probabilistic model:\n\n\nCode\nimport numpy as np \nfrom matplotlib import pyplot as plt \n\nn = 100\nx = np.random.randint(1, 11, n)\ny = np.random.rand(n) < x/11\n\n\nf = plt.scatter(x, y + 0.2*np.random.rand(n) - 0.1, alpha = 0.5)\nl = plt.xlabel(\"Enjoys cold weather\") \nl = plt.ylabel(\"Enjoys skiing\")\n\n\n\n\n\nFigure 1: Data sampled from the probabilistic model of enjoyment of skiing based on enjoyment of cold weather. The vertical axis has been jittered so for legibility.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Data Generating Distribution) An observation generating distribution is a probability distribution describing the likelihood of a single observation \\((\\mathbf{x}, y)\\). We’ll usually call this distribution \\(p_\\mathcal{D}\\), so that the likelihood of realizing the pair \\((\\mathbf{x}, y)\\) is \\(p_\\mathcal{D}(\\mathbf{x}, y)\\).\nA data generating distribution is a probability distribution describing the likelihood of a feature matrix-target vector pair \\((\\mathbf{X}, \\mathbf{y})\\) with \\(n\\) observations. In this class we’ll always assume that the observations are independent and identically distributed (i.i.d.) according to \\(p_\\mathcal{D}\\), and so the data generating distribution can be written\n\\[\nP_\\mathcal{D}(\\mathbf{X}, \\mathbf{y}) = \\prod_{i = 1}^n p_\\mathcal{D}(\\mathbf{x}_i, y_i)\\;.\n\\]\n\n\n\n\nIf we knew the data distribution, then it would be easy to make good predictions. In the supervised learning framework, we don’t usually assume that we know the probability model, but we do usually assume that there is one. Our job is to help our models find the patterns encoded in the data generating distribution.\nRecall that in the case of the perceptron, we assumed that we were dealing with linearly separable data like the ones shown below:\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom hidden.perceptron import perceptron_update, draw_line\n\nnp.random.seed(123)\n\nplt.rcParams[\"figure.figsize\"] = (4, 4)\n\nX1 = np.random.normal(0, 1, 100)\nX2 = np.random.normal(0, 1, 100)\n\nX3 = np.random.normal(0, 1, 100)*.5+3\nX4 = np.random.normal(0, 1, 100)*.5+3\n\nfig, ax = plt.subplots(1, 1)\n\ndef plot_scatter(X1, X2, X3, X4, ax, legend = True):\n    \n    s = ax.scatter(X1, X2, color = \"#ED90A4\", alpha = 0.5, label = r\"$y_i = -1$\")\n    s = ax.scatter(X3, X4, color = \"#00C1B2\", alpha = 0.5, label = r\"$y_i = 1$\")\n    l = ax.set(xlabel = r\"$x_{i1}$\")\n    l = ax.set(ylabel = \"$x_{i2}$\")\n    if legend:\n        l = ax.legend()\n    \nplot_scatter(X1, X2, X3, X4, ax)\n\n\n\n\n\nFigure 2: 200 data points in the 2d plane, each of which has one of two labels.\n\n\n\n\n\n\n\n\n\n\nActivity 1\n\n\n\nCan you write down some ideas for an observation generating distribution that would generate data that looks like this? Feel free to use a weight vector \\(\\mathbf{w}\\), a bias \\(b\\), and anything else you might need."
  },
  {
    "objectID": "lecture-notes/erm.html#empirical-risk-minimization",
    "href": "lecture-notes/erm.html#empirical-risk-minimization",
    "title": "Models, Algorithms, and Learning",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe are now prepared to formulate a fundamental paradigm for prediction problems in machine learning.\n\n\n\n\n\n\n\nDefinition 8 (Empirical Risk Minimization) An empirical risk minimization problem involves:\n\nA choice of model family \\(\\mathcal{M} = \\{f_{\\boldsymbol{\\theta}}\\}\\).\nA choice of loss function \\(\\ell: \\mathbb{R}\\times \\mathbb{R}\\rightarrow \\mathbb{R}\\).\n\nA data set \\((\\mathbf{X}, \\mathbf{y})\\).\n\nThe empirical risk problem is then to find the parameter vector \\(\\boldsymbol{\\theta}\\) (which corresponds to a choice of model) that minimizes the empirical risk:\n\\[\n\\begin{align}\n\\hat{\\boldsymbol{\\theta}} &= \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\theta}} \\; \\hat{R}(f_\\boldsymbol{\\theta})  \\\\\n&= \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\theta}} \\; \\frac{1}{n} \\sum_{i = 1}^n \\ell(f_\\boldsymbol{\\theta}(\\mathbf{x}_i), y_i)\\;.\n\\end{align}\n\\]\n\n\n\n\nA large number of classification and regression algorithms that we study in this course are instances of empirical risk minimization."
  },
  {
    "objectID": "lecture-notes/erm.html#back-to-perceptron",
    "href": "lecture-notes/erm.html#back-to-perceptron",
    "title": "Models, Algorithms, and Learning",
    "section": "Back to Perceptron",
    "text": "Back to Perceptron\nWe are now able to situate the perceptron algorithm within the framework of empirical risk minimization:\n\nThe model family \\(\\mathcal{M}\\) is the set of all functions of the form \\(f_{\\mathbf{w}, b}(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle + b\\). We can take the parameter vector to be \\(\\boldsymbol{\\theta} = (\\mathbf{w}, b)\\).\nThe per-observation loss function used is the so-called 0-1 loss function given by\n\n\\[\n\\ell(\\hat{y}, y) = 1 - \\mathbb{1}\\left[ \\hat{y}y > 0 \\right]\\;.\n\\]\n\nThe empirical risk is\n\n\\[\n\\hat{R}(f_{\\mathbf{w}, b}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\hat{y}_i,y_i) = \\sum_{i= 1}^n \\left[1 - \\mathbb{1}\\left[ \\hat{y}_iy_i > 0 \\right]\\right]\\;.\n\\]\n\nThe perceptron algorithm attempts to find a pair \\((\\mathbf{w}, b)\\) that reduces the empirical risk by updating the parameters using one data point at a time.\nThe perceptron convergence theorem for separable data says that if there exists a pair \\((\\mathbf{w}, b)\\) such that \\(\\hat{R}(f_{\\mathbf{w}, b}) = 0\\), then the perceptron algorithm is guaranteed to find such a pair after a number of steps that can be bounded in terms of the data."
  },
  {
    "objectID": "lecture-notes/erm.html#closeout-activity",
    "href": "lecture-notes/erm.html#closeout-activity",
    "title": "Models, Algorithms, and Learning",
    "section": "Closeout Activity",
    "text": "Closeout Activity\nConsider the problem of predicting the flip of a biased coin based only on prior flips. We’re going to treat this as an inference problem: we’d like to offer a predicted probability \\(\\hat{p}\\) that the coin will come up heads, based on prior information. This \\(\\hat{p}\\) plays the role of the prediction \\(\\hat{y}\\). We represent a heads coin flip as \\(y = 1\\) and a tails coin flip as \\(y = 0\\). We assume that the coin has a true bias \\(p \\in [0,1]\\) that we are not able to observe. This \\(p\\) characterizes the observation generating distribution: \\(p_{\\mathcal{D}}(y = 1) = p\\) and \\(p_{\\mathcal{D}}(y = 1) = 1-p\\)\nFor this activity, we’ll use the cross-entropy loss:\n\\[\n\\ell(\\hat{p}, y) = - y \\log \\hat{p} - (1-y) \\log (1-\\hat{p})\\;.\n\\]\nThe risk is \\(R(\\hat{p}) = \\mathbb{E}_{\\mathcal{D}}[\\ell(\\hat{p}, y)]\\), where \\(y\\) is distributed according to \\(p_\\mathcal{D}\\). The empirical risk if we have observed \\(n\\) coin flips is \\(\\hat{R}(\\hat{p}) = \\sum_{i = 1}^n \\ell(\\hat{p}, y_i)\\).\n\n\n\n\n\n\nEmpirical and Actual Risk\n\n\n\n\nCompute the risk \\(R(\\hat{p})\\) explicitly in terms of \\(p\\), treating \\(y\\) as random.\n\nHint: what is \\(\\mathbb{E}_{\\mathcal{D}}[y]\\)?\n\nProve that the value of \\(\\hat{p}\\) that minimizes the empirical risk is \\(\\hat{p} = \\frac{1}{n}\\sum_{i = 1}^n y_i\\), the fraction of the observed times that we observe heads.\n\nHint: take the derivative of \\(\\hat{R}(\\hat{p})\\) with respect to \\(\\hat{p}\\) and set the result equal to 0.\n\n\n\n\n\n\n\n\n\n\nExperiment\n\n\n\nThe following code simulates 100 random flips of a biased coin and computes the minimizer \\(\\hat{p}\\) of the empirical risk after each flip. It then plots the empirical and actual risk to show how these evolve as we observe more data. However, there are two lines missing: the computation of the empirical and actual risks.\nIt then computes arrays of the empirical risk R_hat and actual risk R associated the estimates \\(p_hat\\). For example, if \\(\\hat{p}_k\\) is the prediction after \\(k\\) observations, then the corresponding entry of \\(\\hat{R}_k\\) should have value \\(\\frac{1}{k} \\sum_{i = 1}^k \\left[-y \\log \\hat{p}_k - (1- y)\\log (1-\\hat{p}_k)\\right]\\) (hint: np.log. Second hint: simplify this expression a little using \\(\\hat{p}_k\\). ).\n\n# imports \nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# simulate flips. The array flips now contains 0s and 1s. \nn_flips = 100\np = 0.7 # bias of coin\nflips = 1*(np.random.rand(n_flips) < p)\n\n# minimizing value of p_hat after each flip\np_hat = np.cumsum(flips)/np.arange(1, len(flips)+1) \n\nR_hat = # empirical risk: fill in\nR =     # true risk:      fill in\n\n# construct the plot \nplt.rcParams[\"figure.figsize\"] = (6, 3)\nfig, axarr = plt.subplots(1, 2)\naxarr[0].plot(R_hat, label = \"Empirical risk\")\naxarr[0].plot(R, label = \"Risk\")\naxarr[0].legend()\naxarr[0].set(xlabel = \"iteration\", ylabel = \"risk (nats)\")\n\naxarr[1].plot(p_hat, label = \"Prediction\", color = \"grey\")\naxarr[1].plot(p*np.ones(n_flips), label = \"True value\", color = \"black\")\naxarr[1].set(xlabel = \"iteration\", ylabel = r\"prediction $p$\")\nplt.tight_layout()\n\naxarr[1].legend()\nYour result should look something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nTry running your experiment a few times. Comment qualitatively: how accurate is the empirical risk as an estimate of the true risk? How accurate is \\(\\hat{p}\\) as an estimate of \\(p\\)? In this case, how many coin flips would you want to see before you felt comfortable with your predictive model? Why?"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Index of Assignments",
    "section": "",
    "text": "Blog Posts\n\n\n\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Implementing the Perceptron Algorithm\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 22, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Implementation\n                                | \n                            Navigation\n                                | \n                            Experimentation\n            \n        \n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Empirical Risk Minimization For Classification\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 5, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement empirical risk minimization for two important classification algorithms: logistic regression and the primal support vector machine.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Theory\n                                | \n                            Implementation\n                                | \n                            Experimentation\n            \n        \n            \n        \n\n\nNo matching items\n\n\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-erm.html",
    "href": "assignments/blog-posts/blog-post-erm.html",
    "title": "Empirical Risk Minimization For Classification",
    "section": "",
    "text": "1 Introduction\nLet’s do empirical risk minimization!\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html",
    "href": "assignments/blog-posts/blog-post-penguins.html",
    "title": "Application Project: Classifying Palmer Penguins",
    "section": "",
    "text": "© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-medical-bias.html",
    "href": "assignments/blog-posts/blog-post-medical-bias.html",
    "title": "Bias in a Medical Recommender System",
    "section": "",
    "text": "© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html",
    "href": "assignments/blog-posts/blog-post-perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#source-code",
    "href": "assignments/blog-posts/blog-post-perceptron.html#source-code",
    "title": "Implementing the Perceptron Algorithm",
    "section": "3.1 Source Code",
    "text": "3.1 Source Code\nYour class should have the following methods:\n\nPerceptron.fit(X, y) is the primary method. This method has no return value. If p is a Perceptron, then after p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector \\(\\tilde{\\vw} = (\\vw, -b)\\) in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period (see Perceptron.score(X, y) below.)\nPerceptron.predict(X) should return a vector \\(\\hat{\\vy} \\in \\{0,1\\}^n\\) of predicted labels. These are the model’s predictions for the labels on the data.\nPerceptron.score(X, y) should return the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification.\n\nFeel free to add any other methods or functions that you find helpful while implementing.\n\nImplementing fit()\nTo implement fit(), it’s convenient to consider a modified version of \\(\\mX\\): \\(\\tilde{\\mX} = [\\mX, \\mathbf{1}]\\), where \\(\\mathbf{1} \\in \\R^n\\) is a column-vector of \\(1\\)s. The reason this is handy is that if we also define \\(\\tilde{\\vw} = (\\vw, -b)\\), then we can write our classification rule as\n\\[\n\\hat{y}_i = \\mathbb{1}(\\langle \\tilde{\\vw}, \\tilde{\\vx}_i\\rangle \\geq 0)\\;.\n\\]\nThis is mathematically convenient and makes it much easier for us to code up our algorithms.\nWith these definitions, the perceptron algorithm proceeds as follows:\n\nFirst, initialize a random initial weight vector \\(\\tilde{\\vw}^{(0)}\\).\nThen, until termination:\n\n\nPick a random index \\(i \\in [n]\\).\nCompute\n\n\\[\n\\tilde{\\vw}^{(t+1)} = \\tilde{\\vw}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\vw}^{(t)}, \\tilde{\\vx}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\vx}_i\\;.\n\\tag{1}\\]\nIn this expression, \\(\\tilde{y}_i = 2y_i - 1\\) is a convenient version of \\(y_i\\) that takes values \\(-1\\) and \\(1\\) instead of \\(0\\) and \\(1\\).\nThis update is performed until either a user-specified maximum number of steps is reached or until the score (accuracy) reaches 1.0.\nNote that in an iteration in which \\(\\tilde{y}_i \\langle \\tilde{\\vw}^{(t)}, \\tilde{\\vx}_i\\rangle \\geq 0\\), nothing happens. Take a moment to check that this occurs when the current weight vector \\(\\tilde{\\vw}^{(t)}\\) correctly classifies the tuple \\((\\vx_i, y_i)\\).\n\n\nOther Specifications\nYou should be able to replicate the demo in Section 2 with your source code. Feel free to use that demo as a test case – your source code may be in good shape when you are able to fully replicate the results. For perfect replication, you’ll need to include the call to np.random.seed() immediately after importing your packages.\nAn excellent solution will have exactly one for-loop, of the form:\nfor _ in range(max_steps):\n  # perform the perceptron update and log the score in self.history\nThat is, you should not do any loops over the data! Use vectorized numpy operations and matrix-vector multiplication.\nYou should also not use if statements to perform comparisons between numbers.\n\n\nFor a hint on how you can avoid doing this, you can reflect on the following two code snippets:\nprint((1 < 2)*2)\nprint((1 > 2)*2)\n\n\n\n\n\n\nPlease include informative docstrings for Perceptron.fit(), Perceptron.predict(), and Perceptron.score().\n\n\n\nA concise solution should likely be no more than 60 lines of compact Python code (excluding comments and docstrings)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#experiments",
    "href": "assignments/blog-posts/blog-post-perceptron.html#experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "3.2 Experiments",
    "text": "3.2 Experiments\nPlease perform experiments (with visualizations) that illustrate the following:\n\nUsing 2d data like the data in the example, if the data is linearly separable then the perceptron algorithm converges to weight vector \\(\\tilde{\\vw}\\) describing a separating line (provided that the maximum number of iterations is large enough).\n\nPlease show visualizations of the data, the separating line, and the evolution of the accuracy over training. It’s also fine for you to use the loss instead of the accuracy if you’d prefer.\n\n\nFor 2d data, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\tilde{\\vw}\\), but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n\nPlease show visualizations of the data, the line in the final iteration, and the evolution of the score over training.\n\nThe perceptron algorithm is also able to work in more than 2 dimensions! Show an example of running your algorithm on data with at least 5 features. You don’t need to visualize the data or the separating line, but you should still show the evolution of the score over the training period. Include a comment on whether you believe that the data is linearly separable based on your observation of the score."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#writing",
    "href": "assignments/blog-posts/blog-post-perceptron.html#writing",
    "title": "Implementing the Perceptron Algorithm",
    "section": "3.3 Writing",
    "text": "3.3 Writing\nIn crafting your blog post, please include the following components:\n\nAt the very top of your blog post, a link to your source code (perceptron.py) on your GitHub repo.\nA brief walk-through of your implementation of the perceptron update (Equation 1) in your source code. Quote the function which you use to perform the update. It’s not necessary to walk the user through every single aspect of your solution class.\nFull code and English descriptions for all the experiments you perform.\nAt the end of your blog post, please address the following question:\n\n\nWhat is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points \\(n\\)? What about the number of features \\(p\\)?\n\nYou only need to consider this question in the context of a single update. The question of how many updates are required to converge is a trickier one that you don’t have to discuss in your blog post."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html",
    "href": "assignments/blog-posts/blog-post-optimization.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/mid-course.html",
    "href": "assignments/process/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice. I recommend JupyterLab for this one, but you can pick.\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nBriefly review the goals you set for yourself in our goal-setting activity at the beginning of the course. You can find your goals on Gradescope.\nIn the The Data section, replace the blanks with brief responses.\nIn the What You Learned and Reflecting on Goals sections, write down your reflections on your learning, achievement, and presence in CSCI 0451 in the provided markdown cells.\nTake some time to reflect on your responses so far. When you’re ready, review the soundbytes describing letter grades.\nTake some time to reflect on your responses so far. When you’re ready, propose the letter grade that you feel best reflects your learning, participation, and achievement in CSCI 0451 so far.\nOptionally, respond to the last prompt with some thoughts on how the semester is going and what we might do to help you meet your goals for the course.\nSubmit the notebook as a PDF on Gradescope.\n\nWe’ll discuss your reflection and your proposed letter grade during our end-of-semester conference.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf end-of-course.ipynb\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/mid-course.html#the-data",
    "href": "assignments/process/mid-course.html#the-data",
    "title": "Mid-Course Reflection",
    "section": "The Data",
    "text": "The Data\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\nPresence in Class\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) ____\nHow often have you taken notes on the core readings ahead of the class period? ____\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ____\nHow many times have you actually presented the daily warm-up to your team? ____\nHow many times have you asked your team for help while presenting the daily warm-up? ____\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? ____\nHow often have you helped a teammate during the daily warm-up presentation? ____\n\n\n\nPresence Outside of Class\n\nHow often have you attended Student Hours or Peer Help? ____\nHow often have you asked for or received help from your fellow students? ____\nHave you been regularly participating in a study group outside class? ____\nHow often have you posted questions or answers in Slack? ____\n\n\n\nAssignments and Effort\n\nHow many blog posts have you submitted? ____\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: ____\nRevisions useful: ____\nRevisions encouraged: ____\nIncomplete: ____\n\nRoughly how many hours per week have you spent on this course outside of class? ____"
  },
  {
    "objectID": "assignments/process/mid-course.html#what-youve-learned",
    "href": "assignments/process/mid-course.html#what-youve-learned",
    "title": "Mid-Course Reflection",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\n[your response here]"
  },
  {
    "objectID": "assignments/process/mid-course.html#reflecting-on-goals",
    "href": "assignments/process/mid-course.html#reflecting-on-goals",
    "title": "Mid-Course Reflection",
    "section": "Reflecting on Goals",
    "text": "Reflecting on Goals\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\nBlog Posts\n[your response here]\n\n\nCourse Presence (Participation)\n[your response here]\n\n\nProject\n[your response here]\n\n\nOther\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\n[your response here]"
  },
  {
    "objectID": "assignments/process/mid-course.html#grade-and-goals",
    "href": "assignments/process/mid-course.html#grade-and-goals",
    "title": "Mid-Course Reflection",
    "section": "Grade and Goals",
    "text": "Grade and Goals\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\nWhat a Grade Sounds Like\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of ____"
  },
  {
    "objectID": "assignments/process/mid-course.html#optional-how-to-improve",
    "href": "assignments/process/mid-course.html#optional-how-to-improve",
    "title": "Mid-Course Reflection",
    "section": "(Optional:) How to Improve?",
    "text": "(Optional:) How to Improve?\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\n[your response here]"
  },
  {
    "objectID": "assignments/process/ideas.html",
    "href": "assignments/process/ideas.html",
    "title": "Ideas for Goal-Setting",
    "section": "",
    "text": "If you’re not sure what goals you might want to set for yourself when setting your goals for the course, here are a few ideas to help you get started. Don’t limit yourself to just these ideas! They are just here to show you some possibilities and get your creative juices flowing. Note that these are not requirements and you do not have to do all of these in order to demonstrate learning in the course.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/ideas.html#blog-posts",
    "href": "assignments/process/ideas.html#blog-posts",
    "title": "Ideas for Goal-Setting",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nSubmit at least 10 blog posts during the semester.\nSubmit the first draft of no more than two blog posts after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo “above and beyond” in at least two blog posts by implementing more complex algorithms, discussing more advanced theory, or performing experiments significantly beyond what is requested.\nPropose and complete a blog post on an not covered in lecture, especially one related to your areas of focused interest.\nPropose and complete a blog post on a topic related to algorithmic bias and social responsibility."
  },
  {
    "objectID": "assignments/process/ideas.html#course-presence",
    "href": "assignments/process/ideas.html#course-presence",
    "title": "Ideas for Goal-Setting",
    "section": "Course Presence",
    "text": "Course Presence\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\n“Pass” at most once when asked to lead the warmup activity for my group.\nAsk questions or make suggestions for the warmup presenter in most weeks.\nPropose questions ahead of time for our guest speaker.\nOrganize a study group outside of class time to work on blog posts or other course work.\nAttend a study group outside of class time.\nFrequently attend Peer Help or Student Hours (after preparing questions and working examples).\nRegularly post questions or answers on the course Slack workspace."
  },
  {
    "objectID": "assignments/process/ideas.html#project",
    "href": "assignments/process/ideas.html#project",
    "title": "Ideas for Goal-Setting",
    "section": "Project",
    "text": "Project\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nImplement algorithms or write automated checks of algorithms written by teammates.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nLead creation of the final project presentation.\nTake the lead in delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "assignments/process/goal-setting.html",
    "href": "assignments/process/goal-setting.html",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice. I recommend JupyterLab for this one, but you can pick.\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nIn each of the spaces provided, write down some goals describing what you believe success will look like for you in CSCI 0451. I’ve offered some ideas to help you get started in case you’re not sure, but you shouldn’t feel constrained by these.\nYou may want to look at the end-of-course reflection activity in which you’ll look back on your goals and propose a letter grade that reflects your learning, participation, and achievement in the course. You might especially want to look at the data that I’ll ask you to log and what a grade sounds like.\nSubmit the notebook as a PDF on Gradescope.\n\nI’ll respond to your submission with feedback on your goals. I may ask you to display more or less ambition in some of your goals; in that case, I’ll ask you to revise and resubmit.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf goal-setting.ipynb\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/goal-setting.html#what-youll-learn",
    "href": "assignments/process/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\n[your response here]"
  },
  {
    "objectID": "assignments/process/goal-setting.html#what-youll-achieve",
    "href": "assignments/process/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n[your response here]\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n[your response here]\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n[your response here]"
  },
  {
    "objectID": "assignments/process/end-of-course.html",
    "href": "assignments/process/end-of-course.html",
    "title": "End-Of-Course Reflection",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice. I recommend JupyterLab for this one, but you can pick.\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nBriefly review the goals you set for yourself in our goal-setting activity at the beginning of the course. You can find your goals on Gradescope.\nIn the The Data section, replace the blanks with brief responses.\nIn the What you Learned and Reflecting on Goals sections, write down your reflections on your learning, achievement, and presence in CSCI 0451 in the provided markdown cells.\nTake some time to reflect on your responses so far. When you’re ready, move on to propose the letter grade that you feel best reflects your learning, participation, and achievement in CSCI 0451.\nSubmit the notebook as a PDF on Gradescope.\n\nWe’ll discuss your reflection and your proposed letter grade during our end-of-semester conference.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf end-of-course.ipynb\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/end-of-course.html#the-data",
    "href": "assignments/process/end-of-course.html#the-data",
    "title": "End-Of-Course Reflection",
    "section": "The Data",
    "text": "The Data\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often did you attend class,” good answers include “almost always,” “I missed three times,” “about 75% of the time,” “not as often as I wanted,” etc.\n\nPresence in Class\n\nHow often did you attend class? (e.g. “almost always,” “I missed three times,” etc.) ____\nHow often did you take notes on the core readings ahead of the class period? ____\nHow often were you prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ____\nHow many times did you actually present the daily warm-up to your team? ____\nHow many times did you ask your team for help while presenting the daily warm-up? ____\nHow often did you learn something new from a teammate’s presentation of the daily warm-up? ____\nHow often did you help a teammate during the daily warm-up presentation? ____\nDid you contribute a question for our guest speaker? ____\n\n\n\nPresence Outside of Class\n\nHow often did you attend Student Hours or Peer Help? ____\nHow often did you ask for or receive help from your fellow students? ____\nDid you regularly participate in a study group outside class? ____\nHow often did you post questions or answers in Slack? ____\n\n\n\nAssignments and Effort\n\nHow many blog posts did you submit? ____\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: ____\nRevisions useful: ____\nRevisions encouraged: ____\nIncomplete: ____\n\nRoughly how many hours per week did you spend on this course outside of class? ____"
  },
  {
    "objectID": "assignments/process/end-of-course.html#what-you-learned",
    "href": "assignments/process/end-of-course.html#what-you-learned",
    "title": "End-Of-Course Reflection",
    "section": "What You Learned",
    "text": "What You Learned\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what did you do in order to pursue your interest?\n[your response here]"
  },
  {
    "objectID": "assignments/process/end-of-course.html#reflecting-on-goals",
    "href": "assignments/process/end-of-course.html#reflecting-on-goals",
    "title": "End-Of-Course Reflection",
    "section": "Reflecting on Goals",
    "text": "Reflecting on Goals\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways did you meet your goals from the beginning of the course? Be specific: explain what the goal was and what you did to meet it.\nIn what ways did you not meet your goals from the beginning of the course? Be specific: explain what the goal was and what the gap was between what you aspired to and what happened.\nIf there’s any context you want to share about how you fared relative to your goals, please do!\n\n\nBlog Posts\n[your response here]\n\n\nCourse Presence (Participation)\n[your response here]\n\n\nProject\n[your response here]\n\n\nOther\nIs there anything else that you want to share with me about what you learned, how you participated, or what you achieved in CSCI 0451?\n[your response here]"
  },
  {
    "objectID": "posts/2023-01-01-test.html",
    "href": "posts/2023-01-01-test.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "This is some code that I’m writing, but also some math that I’m doing.\n\\[x = - y\\]\n\nfrom matplotlib import pyplot as plt\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "sol/perceptron/perceptron-experiment.html",
    "href": "sol/perceptron/perceptron-experiment.html",
    "title": "",
    "section": "",
    "text": "n = 100\nX = np.random.rand(n, 2) - .5\nw_true = np.array([.5, .5])\ny = 1*(X@w_true > 0)\n\n\ndf = pd.DataFrame(np.append(X, y[..., None], 1), columns=[\"x1\", \"x2\", \"y\"])\nf = sns.relplot(data = df, x = \"x1\", y = \"x2\", hue = \"y\")\n\n\n\n\n\nreload(perceptron)\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 10000)\nplt.plot(p.history)\n\n\n\n\n\np.w\n\narray([2.11140182, 2.63862367, 0.39340574])\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "You can access this page using go/cs-451.\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "slides/welcome.html#section",
    "href": "slides/welcome.html#section",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is the theory and practice of algorithmically learning patterns in data."
  },
  {
    "objectID": "slides/welcome.html#section-1",
    "href": "slides/welcome.html#section-1",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…automated consumer recommendations for content and shopping."
  },
  {
    "objectID": "slides/welcome.html#section-2",
    "href": "slides/welcome.html#section-2",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…generating realistic synthetic text, images, and code.\n\n\n\n\n\nAsk chatGPT to condemn itself in the tone of Shakespeare and it looks hilarious. pic.twitter.com/T785FbGmUX\n\n— Deqing Fu (@DeqingFu) December 5, 2022"
  },
  {
    "objectID": "slides/welcome.html#section-3",
    "href": "slides/welcome.html#section-3",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…predictions and recommendations for life-changing decisions: housing, healthcare, criminal justice."
  },
  {
    "objectID": "slides/welcome.html#section-4",
    "href": "slides/welcome.html#section-4",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…search engines, smart homes, computer vision, speech-to-text, scientific discovery, driver assistance systems…"
  },
  {
    "objectID": "slides/welcome.html#section-5",
    "href": "slides/welcome.html#section-5",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Can you list the times in which you interacted with a machine learning system yesterday?"
  },
  {
    "objectID": "slides/welcome.html#section-6",
    "href": "slides/welcome.html#section-6",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "How is this course going to go?"
  },
  {
    "objectID": "slides/welcome.html#section-7",
    "href": "slides/welcome.html#section-7",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "CSCI 0451 is….\nCoding\n\nNumerical array programming\nObject-oriented interfaces\nExperiments and visualization\n\nMath\n\nLinear algebra\nOptimization (\\(\\implies\\) calculus)\nA bit of probability\n\nReading, writing, discussion\n\nTechnical methods\nBias, fairness, and impact of ML"
  },
  {
    "objectID": "slides/welcome.html#section-8",
    "href": "slides/welcome.html#section-8",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Rough, tentative plan for the semester\n\n\nFundamentals of prediction (~6 weeks)\n\nCore math concepts\nOptimization (“the algorithms”)\nHow to help models generalize\nFormal definitions of bias and fairness\n\nUnsupervised methods (~1 week)\n\nClustering\nDimensionality reduction\n\nDeep Learning (~4 weeks)\n\nImage classification\nText generation\nWord embedding\n\nProject Presentations (~1 week)"
  },
  {
    "objectID": "slides/welcome.html#section-9",
    "href": "slides/welcome.html#section-9",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Most Days\n\n\nWarmup Activity\n\nComplete ahead of time.\nReinforces content from readings and connects them to lecture.\nPresent in groups of 4-5.\nRandom presenter presents to the group.\n\nLecture\n\nMath\nLive-coding\nYour questions and ideas!\n\nClose-Out Activity\n\nIn same groups as warmup.\nPractices content from lecture."
  },
  {
    "objectID": "slides/welcome.html#section-10",
    "href": "slides/welcome.html#section-10",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Activities and assignments\n\n\nBlog Posts\n\nAim for ~1 per week.\nInvolves implementation, experiments, and discussion.\nPublished on your blog.\n\nDaily Warmup Activities\n\nRelatively quick when you’ve done the readings.\nOne (random) person each day will present to 4-5 classmates.\nConnects readings to lecture.\n\nProject\n\nIn groups of your choosing.\nWork on it throughout the semester, presentations in last week.\nWe’ll have activities etc. to help you pick a path."
  },
  {
    "objectID": "slides/welcome.html#section-11",
    "href": "slides/welcome.html#section-11",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Blog Posts\n\nImplement algorithms in source (.py) files.\nPerform experiments in Jupyter notebooks.\nCreate figures, add expository prose, etc.\nRender your notebooks into a blog using the Quarto publishing engine.\nHost source code and rendered blog on GitHub."
  },
  {
    "objectID": "slides/welcome.html#section-13",
    "href": "slides/welcome.html#section-13",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Feedback on blog posts via Hypothes.is"
  },
  {
    "objectID": "slides/welcome.html#section-14",
    "href": "slides/welcome.html#section-14",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Feedback on source code"
  },
  {
    "objectID": "slides/welcome.html#section-15",
    "href": "slides/welcome.html#section-15",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Readings and Warmups\nDo them! Some readings are optional."
  },
  {
    "objectID": "slides/welcome.html#section-16",
    "href": "slides/welcome.html#section-16",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "ML, Ethics, Bias, and Fairness\nAlgorithmic bias is the tendency of automated systems to reproduce structural privilege and oppression, especially in relation to race, gender, and sexuality.\nMost systems that impact people in any way have at least a risk of algorithmic bias. Active mitigation is usually needed.\n\n\n\n\n\nSave the Date\n\nMonday April 24th\nDr. Timnit Gebru will be virtually visiting our class for a Q&A session and giving a talk at 7pm.\nDr. Gebru is one of the world’s leading experts in intersectional bias in AI."
  },
  {
    "objectID": "slides/welcome.html#section-17",
    "href": "slides/welcome.html#section-17",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading"
  },
  {
    "objectID": "slides/welcome.html#section-18",
    "href": "slides/welcome.html#section-18",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading\n\n\nInitialization:\n\n\nYou set goals for your learning and achievement (in week 2).\n\n\nMain Loop:\n\n\nYou attend class, participate in activities, and complete assignments.\n\nYou get feedback on your assignments from me and the TAs, and you revise.\nYou reflect on your learning and achievement at different points throughout the course.\n\n\nAt End Of Course:\n\n\nYou propose a letter grade that reflects your learning and achievement, and discuss it with me.\n\n\nIndividual assignments don’t get scores, points, or grades–just feedback."
  },
  {
    "objectID": "slides/welcome.html#section-19",
    "href": "slides/welcome.html#section-19",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading\n \n\n\n\n\n\n\n\n\n\nOpportunity\nChallenge\n\n\n\n\nNo points, no averages\nYou can focus on feedback and set your own goals.\nYou need to motivate based on your interest in the class\n\n\nResubmit assignments\nOne of the best ways to learn\nNeed to read feedback and prioritize time for revisions\n\n\nCan skip assignments\nNo busy work – work on what’s valuable to you.\nStill need to work enough to learn and meet your goals\n\n\nNo hard due-dates\nDon’t ask for extensions, take the time you need\nNeed to keep yourself on pace to achieve your goals"
  },
  {
    "objectID": "slides/welcome.html#section-20",
    "href": "slides/welcome.html#section-20",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What a Grade Sounds Like…\nA: I am ready to take the theory, techniques, and ideas of this course into my endeavours outside this classroom: future classes, projects, hobbies, career.\nB: With help or review, I might be able to take some of what I learned outside this classroom.\nC: I showed up and did stuff, but I don’t really see any ways to take what I learned outside this classroom.\nD-F: I didn’t really show up or do much.\n\n\nWork Expected Work Expected\nI am very likely to accept your proposed grade in the course if you EITHER:\n\nComplete most assignments to a high standard (including revisions) OR\nWork for ~10 productive hours per week outside of class OR\nDo some of the assignments I give you and also some other things (that you propose) that are relevant to the course learning goals."
  },
  {
    "objectID": "slides/welcome.html#section-21",
    "href": "slides/welcome.html#section-21",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What is something that makes you feel excited or empowered about collaborative grading?\nWhat is something that makes you feel nervous or confused about collaborative grading?"
  },
  {
    "objectID": "slides/welcome.html#section-22",
    "href": "slides/welcome.html#section-22",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What I expect from you now"
  },
  {
    "objectID": "slides/welcome.html#section-23",
    "href": "slides/welcome.html#section-23",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "CS Stuff\nYou can write moderately-complex, object-oriented software.\nYou are comfortable reading software documentation and researching how to perform a task that you haven’t seen before.\nYou know what a terminal is and how to perform simple operations at the command line.\nYou have experience debugging your code and you are ready to do it a lot more.\n\n\nMath Stuff\nYou remember most of MATH 0200 and CSCI 0200:\n\nMatrix multiplication and inner products\nEverything about \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\).\n\nVisualizing linear spaces.\nEigenvalues, eigenvectors, positive-definite matrices.\nDerivatives, critical points of functions.\nSample spaces, probability distribution functions.\nRandom variables, mean and variance.\nConditional probability and expectations.\n\nYou are ready to look up what you don’t remember."
  },
  {
    "objectID": "slides/welcome.html#section-24",
    "href": "slides/welcome.html#section-24",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "NYT, 1957\n \n\n\n\n\n\n\n\nWhat We Are Actually Talking About\n\n\n\n\n\n\\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1}(y_i \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle < 0)y_i \\mathbf{x}_i\\]"
  },
  {
    "objectID": "slides/welcome.html#section-25",
    "href": "slides/welcome.html#section-25",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "NYT, 2022\n \n\n\n\n\nWhat We Are Actually Talking About\n\n\nxkcd"
  },
  {
    "objectID": "slides/welcome.html#section-26",
    "href": "slides/welcome.html#section-26",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Most of all, I expect that you are ready to make thoughtful decisions to guide your own learning in this course."
  },
  {
    "objectID": "slides/welcome.html#section-27",
    "href": "slides/welcome.html#section-27",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Based on what you know about the course so far, what are some ways that success might look like for you?"
  },
  {
    "objectID": "collaboration.html",
    "href": "collaboration.html",
    "title": "Collaboration And Academic Honesty",
    "section": "",
    "text": "This is a page of general principles and guidelines that apply in courses I (Phil Chodrow) teach at Middlebury College. It is lightly adapted from the handout “Collaborating on Mathematics” by the Harvey Mudd Department of Mathematics, which I discovered in a Tweet by Francis Su.\nIn any case in which the guidelines and principles on this page conflict with the policies of a specific course, the policies of the specific course should be followed. For example, if the course syllabus says that collaboration is not permitted on homeworks, then collaboration is not permitted on homeworks, regardless of anything written here.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "collaboration.html#why-collaborate",
    "href": "collaboration.html#why-collaborate",
    "title": "Collaboration And Academic Honesty",
    "section": "Why Collaborate?",
    "text": "Why Collaborate?\nMost scientists and engineers don’t work on their own; they work with colleagues and students while doing and publishing research. Increasingly, open problems in science and engineering require multiple skill sets and areas of expertise. Because of this, the need to collaborate will only increase in the future. This is why several of CS@Midd’s learning goals are explicitly focused on communication and collaboration. We want our students to have strong professional and communication skills, to be able to function well as part of a team, and to be able to work and communicate with diverse groups of people."
  },
  {
    "objectID": "collaboration.html#collaborating-on-homework-and-other-individually-assessed-assignments",
    "href": "collaboration.html#collaborating-on-homework-and-other-individually-assessed-assignments",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaborating on Homework and Other Individually-Assessed Assignments",
    "text": "Collaborating on Homework and Other Individually-Assessed Assignments\n\n(COLLABORATION IS A LIFE SKILL) Understand that working with others and asking for assistance are not signs of weakness or deficiency; rather, they are essential life skills important for making progress in any discipline, including computer science. Our department wants you to develop these skills. If you’re too shy to come to my office hours or to join a group of people working on their homework, ask a friend to come with you.\n(COLLABORATIONS BENEFIT FROM DIVERSITY) Open yourself up to working with people whom you don’t know (yet). You might find someone you work really well with and who doesn’t think exactly like you do. A wide range of experiecnes and backgrounds is beneficial in problem solving, although it may be helpful to find folks who can work on assignments during the same time of day and at roughly the same pace. If you’re having trouble finding people to work with, I can help!\n(COLLABORATIONS ARE INCLUSIVE) Believe that everyone has something meaningful to contribute (you included), and that you have something to learn from each person. This can be a difficult state of mind to achieve, but critical for healthy, effective collaboration. Here are some practical consequences:\n\nIn any group setting, listen carefully for everyone’s contributions. Don’t dismiss or ignore what someone says, and don’t move on until you’ve considered it carefully. If what is said doesn’t make sense to you, that doesn’t necessarily mean it’s incorrect–the person might just have a way of approaching the problem that is different and not yet clear to you. Furthermore, even ideas that ultimately turn out to be incomplete or incorrect are often still useful building blocks towards a successful approach.\nFind ways to verbally validate the ideas of others. For example: “One really neat feature of Zenith’s approach to part (b) is that it also works with a small modification for part (c).”\nIf someone in the group hasn’t spoken for a while, ask for their ideas or opinions. Conversely, if you find yourself talking a lot, take a step back and allow someone else to contribute to the discussion.\n\n(COLLABORATIONS REQUIRE PREPARATION) Don’t seek help from others on a probem before you’ve had time to think about it yourself, try at least one approach, and formulate the obstacle as clearly as you can. But at the same time, if you find yourself frustrated with a problem and you’re not making progress, don’t wait too long before you look for help from your classmates, your tutors, or me.\n(COLLABORATIONS GENERATE DEEPER UNDERSTANDING) Don’t be satisfied with only producing the correct final result; use your collaboration to push each other to understand:\n\nWhy does this approach work?\nWhat alternative approaches would also have worked?\nWhat are some of the merits and drawbacks of these different approaches?\n\n(COLLABORATIONS ARE EMPOWERING) Good collaborations empower people towards further growth.\n\nWhen you’re working on a problem with others and you find a path before everyone else, avoid ruining the experience of discovery for others. Conversely, if you haven’t figured out something yet and want to enjoy the discovery for yourself, don’t let someone else ruin your joy.\nIf someone asks you for help, don’t just tell them the answer or start showing them a solution method. Listen carefully to their question. Ask for more information if they aren’t being specific enough. If they say “I don’t know where to start,” ask them to tell you about their understanding of what the question is asking and which parts of it seem most puzzling. Ask guiding questions to help them discover ideas for themselves. In these situations, you have the opportunity to learn how to help others learn – this is an invaluable life skill.\n\n(COLLABORATIONS ACKNOWLEDGE CONTRIBUTORS) Whenever you’ve received help on a homework assignment from a classmate, a friend, a tutor, or me, acknowledge the support and briefly describe how it helped you in your assignment.\n\nThe reason I ask you to acknowledge tutors and myself is actually different from the reason I ask you to acknowledge classmates and friends. For classmates and friends, it’s about cultivating transparency and integrity. The primary reason I want you to acknowledge tutors and myself is that the exercise of explicitly remembering and reflecting on your learning journey is part of metacognition, a valuable set of practices that will help you succeed in this class, in college, and in your long-term career."
  },
  {
    "objectID": "collaboration.html#collaborating-on-group-projects",
    "href": "collaboration.html#collaborating-on-group-projects",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaborating on Group Projects",
    "text": "Collaborating on Group Projects\n\n(COLLABORATIONS SET GOOD EXPECTATIONS) Establish clear expectations and ways of communicating with each other to avoid misunderstandings. When, where, and how often will you meet? How can you reach each other in case of an emergency?\n(COLLABORATION IS NOT DIVISION OF LABOR) Collaboration is not the same as splitting up a problem into pieces and then slapping the completed pieces together.\n\nIdentify the parts of the problem that need to be completed together and the parts that can be completed individually.\nWork toward a final product that everyone is happy with and that represents the contributions of everyone on the group.\nDon’t just divide up the work based on who might have the most experience or skill with each part of the problem. Let those who want to develop their skills also have a chance to work on pieces that are unfamiliar to them\n\n(COLLABORATIONS ARE EQUITABLE) Aim for each person to contribute a fair and equitable amount of effort and/or time to the group’s deliverables.\n(COLLABORATIONS RESOLVE CONFLICT QUICKLY) Resolve any misunderstands between the team members quickly. Don’t let those misunderstandings fester into distrust, resentment, or anger. Don’t be afraid to ask your professor for help in resolving interpersonal conflict in your team. While this can feel uncomfortable, often these kinds of situations are important opportunities for everyone to learn more about how to coexist as collaborative, whole humans."
  },
  {
    "objectID": "collaboration.html#collaboration-is-a-skill",
    "href": "collaboration.html#collaboration-is-a-skill",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaboration is a Skill",
    "text": "Collaboration is a Skill\nYou might imagine that you already know whether you need to collaborate and how to do it. And indeed, there’s a lot you know already! But collaboration is a skill, and like other skills it rewards practice and growth. Effective collaboration involves perspective-taking, empathy, respect, and clear communication. We hope that you will find that the benefits of collaboration far outweigh its challenges."
  },
  {
    "objectID": "collaboration.html#collaboration-and-the-middlebury-honor-code",
    "href": "collaboration.html#collaboration-and-the-middlebury-honor-code",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaboration and the Middlebury Honor Code",
    "text": "Collaboration and the Middlebury Honor Code\nThe Middlebury Honor Code’s preamble states that:\n\nThe students of Middlebury College believe that individual undergraduates must assume responsibility for their own integrity on all assigned academic work…The Middlebury student body, then, declares its commitment to an honor system that fosters moral growth and to a code that will not tolerate academic dishonesty in the College community.\n\nIn any assignment in which you receive a grade individually (homeworks, exams), the purpose of the grade is to measure your learning and achievement. When you turn in such an assignment, you implicitly represent that work as work that you are able to complete yourself under the stated conditions (which may include getting help or working with others). If you cannot complete some work under the stated collaboration conditions, it is dishonest to turn in that work.\nWhen working individually, it is your responsibility to uphold the Code’s standards of integrity and academic honesty. When working in a group, it is additionally your responsibility to ensure that your group as a whole upholds these standards.\nIf you have a question about whether some form of collaboration is permitted, just ask!\n\nWhat Happens if I Observe an Honor Violation?\n\nWe all fail to uphold our highest moral aspirations at times. If you show lack of integrity or academic honesty, that doesn’t mean you’re a bad person. It means that you’re under pressure and chose the course of action that looked like the most workable one to you at the time.\nThat said, if you show lack of integrity or academic honesty, that’s an indicator that you have an opportunity for some very important growth.\nIt is part of my job to help you achieve that growth. I take this part of my job very seriously. In order to help you on your journey, I will connect both of us with the Middlebury Community Standards Office. Office leadership will help us all find a path that helps you grow toward integrity and honesty.\nThis is an awkward and uncomfortable process for everyone involved. You don’t want this."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software and Setup",
    "section": "",
    "text": "After following this set of instructions, you will be all ready to go for participation in CSCI 0451.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "software.html#install-anaconda",
    "href": "software.html#install-anaconda",
    "title": "Software and Setup",
    "section": "1. Install Anaconda",
    "text": "1. Install Anaconda\nInstall and configure Anaconda Python by following these instructions. Choose the installer appropriate for your operating system."
  },
  {
    "objectID": "software.html#create-the-ml-0451-environment",
    "href": "software.html#create-the-ml-0451-environment",
    "title": "Software and Setup",
    "section": "2. Create the ml-0451 Environment",
    "text": "2. Create the ml-0451 Environment\n\n\n\nThe Environments tab, with the Create button on the bottom.\nAn environment is a separate installation of Python that exists independently of any other versions of Python on your computer. Using environments allows us to have fine-grained control over which version of Python we use, which additional packages are installed, etc.\nTo create an environment in Anaconda, first open the Anaconda Navigator program. Then, navigate to the Environments tab. There, you’ll find the current existing environments, including the default base(root) environment. Click the Create button to create a new environment.\nIn the resulting dialog box:\n\nName your environmnent ml-0451.\nEnsure that the installed Python is some version of Python 3.9 (it’s ok if your version number differs in the last two digits from the one shown in the example).\n\n\n\n Configuring the ml-0451 environment."
  },
  {
    "objectID": "software.html#install-packages",
    "href": "software.html#install-packages",
    "title": "Software and Setup",
    "section": "Install Packages",
    "text": "Install Packages\nYou will need to install several packages to the ml-0451 environment. Note that you need to do this even if you previously installed these packages to another version of Python on your laptop.\nTo add packages to the environment, first ensure that the environment is selected (it will be highlighted in green). Then, on the righthand menu, search for the package you want to install. You may need to change the box on the top left from “Installed” to “Not Installed” in order to view packages that you have not installed yet.\n\nInstall the following packages:\n\nnb_conda\nnumpy\nmatplotlib\npandas\nscikit-learn\nseaborn\n\nI may ask you to install additional packages later on, or you may find it useful to install packages yourself in order to deal with problems or projects. You’ll follow this same process to install them to the ml-0451 environment."
  },
  {
    "objectID": "software.html#launch-jupyterlab",
    "href": "software.html#launch-jupyterlab",
    "title": "Software and Setup",
    "section": "Launch JupyterLab",
    "text": "Launch JupyterLab\nNow back on the Home tab, launch the JupyterLab app. You may need to install it first. Create a notebook using the ml-0451 environment as a kernel.\n\n\n Creating a notebook using the ml-0451 environment as the kernel.\nNext, type the following code into the grey code cell that appears in the notebook:\nimport sklearn as sk\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nprint(\"I did it!\")\nFinally, run the cell (cmd + enter on Mac or ctrl + enter on Windows). If you get no errors, only the output of the print statement, then you did it!\n\n\n If you see this then you did it!"
  },
  {
    "objectID": "software.html#alternative-vscode-and-others",
    "href": "software.html#alternative-vscode-and-others",
    "title": "Software and Setup",
    "section": "Alternative: VSCode and Others",
    "text": "Alternative: VSCode and Others\nJupyterLab is probably the easiest way for you to get up and coding. The reason is that it supports two ways of working with Python:\n\nNotebooks, which allow us to combine code, text, and outputs, including data visualizations.\nText files, like .py files, which are best for holding complex, reusable source code.\n\nThere are other editors that support these as well. My personal favorite is Visual Studio Code (often called VSCode), and you’re likely to see me using it in class. It’s fine for you to use VSCode or any other editor, but please note that I’ll only troubleshoot Anaconda + JupyterLab. That is, you can use VSCode, but you’ll be “on your own” in terms of getting up and running. That said, the documentation on working with notebooks in VSCode is pretty good."
  },
  {
    "objectID": "software.html#optional-github-desktop",
    "href": "software.html#optional-github-desktop",
    "title": "Software and Setup",
    "section": "Optional: GitHub Desktop",
    "text": "Optional: GitHub Desktop\nIf you are comfortable working with git from the command line, you can continue to do this! If you are unfamiliar with git, I recommend that you download and install the GitHub Desktop graphical client. You will need to connect it to your GitHub account."
  },
  {
    "objectID": "software.html#test-drive-quarto",
    "href": "software.html#test-drive-quarto",
    "title": "Software and Setup",
    "section": "Test Drive Quarto",
    "text": "Test Drive Quarto\nChange modify the About page of your blog by modifying the file about.qmd. You can do things like change text or change the profile picture (it doesn’t have to be of yourself). Once you’ve made these changes, open a terminal in the location of your blog and type the command\nquarto preview\nAfter a few moments, a web browser window should pop up with a preview of your blog. If you navigate over to the About tab, you should see your changes."
  },
  {
    "objectID": "software.html#finalize-and-publish",
    "href": "software.html#finalize-and-publish",
    "title": "Software and Setup",
    "section": "Finalize and Publish",
    "text": "Finalize and Publish\nIn the terminal, use ctrl + c to stop the preview process. Then type the command\nquarto render\nThis time you won’t see a preview, but that’s ok! Over in git or GitHub Desktop, check all the new and modified files that have been generated, add a short message, and commit them to the main branch. Then, push your commit. This sends your files back to GitHub.com, where it will be published. After a minute or two, navigate back over to the URL housing your website and check that your changes have been made."
  },
  {
    "objectID": "warmup-exercises.html",
    "href": "warmup-exercises.html",
    "title": "Warmup Exercises",
    "section": "",
    "text": "Classification Rates\n\nPart 1\nCOVID-19 rapid tests have approximately an 80% sensitivity rate, which means that, in an individual who truly has COVID-19, the probability of a rapid test giving a positive result is roughly 80%.  On the other hand, the probability of a rapid test giving a positive result for an individual who truly does not have COVID-19 is 5%. Suppose that approximately 4% of the population are currently infected with COVID-19. These numbers are mostly made-up.Example 2.3.1 of Murphy, page 46, has a good review of the relevant probability and the definition of each of the rates below.\nWrite a Python function called rate_summary that prints the following output, filling in the correct values for each of the specified rates:\ns = 0.8           # test sensitivity\nf = 0.02          # probability of positive test if no COVID\nprevalence = 0.05 # fraction of population infected\n\nrate_summary(s, f, current_infection)\nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\n\nPart 2\n\nSuppose that scientists found an alternative rapid test which had a 75% sensitivity rate with a 0% chance of a positive test on someone who is truly not infected. Would you suggest replacing the old rapid tests with these alternative tests? Why? \nWhat if the alternative test had an 85% sensitivity rate and a 10% chance of a positive test on someone who is truly not infected?\n\nYou don’t necessarily need to use your function from the previous part in this part.\n\nPart 3\nIt’s all well and good to do the math, but what about when we actually have data? Write a function called rate_summary_2 that accepts two columns of a pandas.DataFrame (or equivalently two one-dimensional numpy.arrays of equal length). Call these y and y_pred. Assume that both y and y_pred are binary arrays (i.e. arrays of 0s and 1s). y represents the true outcome, whereas y_pred represents the prediction from an algorithm or test. Here’s an example of the kind of data we are thinking about:\n\nimport pandas as pd\n\nurl = \"https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/toy-classification-data.csv\"\ndf = pd.read_csv(url)\n\ndf.head() # just for visualizing the first few rows\n\n\n\n\n\n  \n    \n      \n      y\n      y_pred\n    \n  \n  \n    \n      0\n      0\n      0\n    \n    \n      1\n      1\n      0\n    \n    \n      2\n      1\n      0\n    \n    \n      3\n      0\n      1\n    \n    \n      4\n      0\n      0\n    \n  \n\n\n\n\nYou should be able to use your function like this:\n# y is the true label, y_pred is the prediction\nrate_summary_2(df[\"y\"], df[\"y_pred\"]) \nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\nHints\nAn excellent solution for this part will not use any for-loops. Computing each of the four rates can be performed in a single compact line of code. To begin thinking of how you might do this, you may want to experiment with code like the following:\ndf[[\"y\"]] == df[[\"y_pred\"]]\ndf[[\"y\"]].sum(), df[[\"y\"]].sum()\n\n\n\n\nPerceptron\n\nPart 1\nSketch the line in \\(\\R^2\\) described by the equation\n\\[\n\\bracket{\\vw, \\vx}  =  w_0\\;,\n\\tag{1}\\]\nwhere \\(\\vw = \\paren{1, -\\frac{1}{2}}^T \\in \\R^2\\) and \\(w_0 = \\frac{1}{2}\\).\n\n\nPart 2\nWrite a quick Python function called perceptron_classify(w, w_0, x). w and x should both be 1d numpy arrays of the same length, and w_0 should be a scalar. Your function should return 0 if \\(\\bracket{\\vw, \\vx} < w_0\\) and 1 if \\(\\bracket{\\vw, \\vx} \\geq w_0\\). An excellent solution will use neither a for-loop nor an if-statement.\nVerify that your function works on a few simple examples.\n\n\nPart 3\nConsider a line of the general form of Equation 1. Let’s allow \\(\\vx\\) and \\(\\vw\\) to both be \\(n\\)-dimensional, so that this equation defines a hyperplane in \\(\\R^n\\). Suppose that we wanted to represent the same hyperplane in \\(\\R^n\\) using an equation of the form\n\\[\n\\bracket{\\tilde{\\vw}, \\tilde{\\vx}} = 0\\;.\n\\tag{2}\\]\nfor some \\(\\tilde{\\vw} \\in \\R^{n+1}\\). Define \\(\\tilde{\\vx} = (\\vx, 1)\\). How could you define \\(\\tilde{\\vw}\\) to make Equation 2 equivalent to Equation 1?\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is an advanced elective on the topic of algorithms that learn patterns from data. Artificial intelligence, predictive analytics, computational science, pattern recognition, signal processing, and data science are all disciplines that draw heavily on techniques from machine learning.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "syllabus.html#social-annotation",
    "href": "syllabus.html#social-annotation",
    "title": "Syllabus",
    "section": "Social Annotation",
    "text": "Social Annotation\nA an extremely useful way for you to engage with the readings is to make comments, ask questions, and answer questions about them as you are reading. For this reason, I’ll be providing most links to readings through Hypothes.is. You’ll be able to make marginal comments and view the marginal comments of others. I’ll also regularly be checking on your annotations to see what questions might have come up with the readings."
  },
  {
    "objectID": "syllabus.html#what-will-class-time-look-like",
    "href": "syllabus.html#what-will-class-time-look-like",
    "title": "Syllabus",
    "section": "What Will Class Time Look Like?",
    "text": "What Will Class Time Look Like?\nMy plan is for most class periods to look like a “lecture sandwich:”\n\n10 to 15 minutes of a warmup activity that addresses the recent lectures and readings.\n40-50 minutes of lecture, punctuated by short activities and breaks.\n10-15 minutes of a closing activity that helps us get solid with the day’s content.\n\n\nThe Warmup Activity\nOn most days, we’ll have a warmup activity. The warmup activity will usually ask you to engage with the readings and complete a small amount of work ahead of class time. This could be a short piece of writing, a math problem, or an implementation of a Python function.\nEach day, a few students will be randomly selected to present their work to a small group of peers. It’s ok to ask for help or even pass if you’re not feeling confident in your solution, but you should plan to at least make a good attempt at the warmup before every class period. Your participation on the warmup activity is an important aspect of presence in the course, and I’ll ask you to reflect on it when proposing your course grade."
  },
  {
    "objectID": "syllabus.html#collaborative-grading",
    "href": "syllabus.html#collaborative-grading",
    "title": "Syllabus",
    "section": "Collaborative Grading",
    "text": "Collaborative Grading\nThis course is collaboratively graded.  In a nutshell, this means:You may have also heard the term ungrading to refer to a similar approach.\n\nThere are no points or scores attached to any assignment. When you turn in assignments, you’ll get feedback on how to revise/resubmit, improve or otherwise proceed in the course, but you won’t get “graded.”\nThere also aren’t any firm due dates, although I will give you suggestions on how to maintain a good pace. \nPeriodically throughout the semester, you will complete reflection activities to help you take stock of your learning and achievement in the course. In your final activity at the end of the semester, you’ll make a proposal for your letter grade in the course, and support it with evidence of your learning. You and I will then meet to discuss how the course went for you, using your reflection activity and proposal as a starting point. In this conversation, you and I will agree on your final letter grade for the course, which I will then submit to the registrar.\n\nAll work you wish to be considered toward your achievement in the course needs to be submitted by the end of Finals Week.Reflection activities:\n\nReflective goal-setting.\nMid-course reflection.\nEnd-of-course reflection and grade proposal."
  },
  {
    "objectID": "syllabus.html#why-collaborative-grading",
    "href": "syllabus.html#why-collaborative-grading",
    "title": "Syllabus",
    "section": "Why Collaborative Grading?",
    "text": "Why Collaborative Grading?\nBecause grading is broken! Traditional points-based grading is ineffective at both (a) accurately measuring student learning and (b) motivating students to learn. I broadly agree with Jesse Stommel when he writes:\n\nAgency, dialogue, self-actualization, and social justice are not possible in a hierarchical system that pits teachers against students and encourages competition by ranking students against one another. Grades (and institutional rankings) are currency for a capitalist system that reduces teaching and learning to a mere transaction. Grading is a massive co-ordinated effort to take humans out of the educational process.\n\nI’d prefer to just not give you grades at all. But, Middlebury says I have to, and so my aim is to instead put the process of grading under your control to the greatest extent that I reasonably can."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nThere are three kinds of assessed assignments in this course, plus a mysterious “Other” category.\n\n\n\n\n\nBlog Posts\n\n\n\nBlog posts are the primary way in which you will demonstrate your understanding of course content. Blog posts usually involve: written explanation of some relevant theory; implementation one or more algorithms according to written specifications; performing experiments to test the performance of the implementations; and communicating findings in a professional way. Some blog posts will be more like short essays than problem sets or programming assignments. Your blog posts will be hosted on your own public website (which you will create). This website will serve as your portfolio for the course.\n\n\n\n\n\n\nProject\n\n\n\nYour project is a large-scale undertaking that you will design and complete, usually in a group of 2 or 3, over the course of the semester. Your project should usually involve some combination of data collection, implementation, research of related work, experimentation, deployment, or theory work (but not necessarily all components). Projects are expected to demonstrate deep engagement with both the course content and the problem selected.\n\n\n\n\n\nProcess Reflections\n\n\n\nAt the beginning of the course, you’ll write a process reflection describing your aspirations for the course—what you want to learn and achieve, and how you’d like to be assessed against your goals. We’ll have a second process reflection mid-way through the course that will allow you to reflect on your progress toward your objectives and consider changing direction if needed. At the end of the course, you’ll write a summary reflection on your learning, accomplishment, and engagement with the class. This is also the place where you’ll propose your final letter grade.   I’ll usually give you written feedback on your process reflections. We’ll also meet at the end of the course to discuss your final reflection and agree on your letter grade for the course.\n\n\n\n\n\nOther…?\n\n\n\nYou may have some topic or idea that especially interests you and which you want to explore. If you’d like to work on this topic and use it to demonstrate your learning in the course, you can propose it to me. I may have suggestions or requested modifications before I agree to count the work in your course portfolio."
  },
  {
    "objectID": "syllabus.html#best-by-dates",
    "href": "syllabus.html#best-by-dates",
    "title": "Syllabus",
    "section": "Best-By Dates",
    "text": "Best-By Dates\nWhile we don’t have formal due dates, there is a benefit to keeping yourself on a schedule. It’s best to complete assignments close to the time when we covered the corresponding content in class, and it’s important for your wellbeing not to let work pile up. I’ll provide “best-by” dates for all assignments. These are my recommendations for when you should submit the first versions of these assignments to me for feedback.\n\n\n Image credit: Dr. Spencer Bagley"
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Syllabus",
    "section": "Feedback",
    "text": "Feedback\nI won’t “grade” your individual assignments, but our course team and I will offer you feedback about what I thought was successful and where you can improve. My general expectation is that you will often (though not always) revise your work in response to feedback and resubmit it. Revising in response to feedback is one of the single most effective ways for you to deepen your learning.\nI’ll usually describe the importance of revisions on your assignment using one of the following categories:\n\nNo revisions suggested: you’ve done great work and should focus on the next thing.\nRevisions useful: you have opportunities for improvement on this assignment, but focusing on the next topic or assignment may be a better use of your time—use your judgment.\n\nRevisions encouraged: the best use of your time is to respond to feedback and resubmit, rather than moving on to the next assignment.\nIncomplete: the assignment isn’t sufficiently complete for it to be used as evidence of your learning."
  },
  {
    "objectID": "syllabus.html#what-work-do-you-need-to-do",
    "href": "syllabus.html#what-work-do-you-need-to-do",
    "title": "Syllabus",
    "section": "What Work Do You Need To Do?",
    "text": "What Work Do You Need To Do?\nAt the beginning of the semester, you’ll write a process letter that will outline what you’d like to learn and achieve in the course. It’s ok if you don’t meet all your aspirations by the end of the course. To help guide you in your goal-setting and work-planning, I do have some general expectations.\nI am likely to consider your time in my course to be highly successful if you do at least one of the following things:\nTime spent being stuck doesn’t count as “productive hours” – get help if you need it!\n\nYou complete almost all assignments with a high degree of quality, including revising in response to my feedback.\nYou spend on average 10 productive hours of work time on this course outside of class.\nYou complete many assignments that I give you, and also propose and complete alternative work that demonstrates your learning and achievement."
  },
  {
    "objectID": "syllabus.html#covid-19-considerations",
    "href": "syllabus.html#covid-19-considerations",
    "title": "Syllabus",
    "section": "COVID-19 Considerations",
    "text": "COVID-19 Considerations\n\nMasks Are Required in CSCI 0451\nThe Computer Science Department policy states that:\n\nWe in the Computer Science department value a safe learning and working environment for all. While we can’t eliminate the risks associated with COVID-19, evidence suggests that widespread masking can significantly reduce the transmission and severity of disease. In order to protect the health of our community, the CS department recommends that students and faculty wear masks in CS learning spaces, including classrooms, office hours, and public areas. We acknowledge the College policy gives instructors the final say over classroom masking requirements, and expect all students to respect instructors’ stated policies in each course.\n\nIn alignment with this policy, I require you to wear masks in class and office hours. I encourage you to wear masks during help sessions and at all other times when you are inside 75 Shannon Street.\nIf you arrive in class without a mask, I will offer you one. I will expect you to either wear it or excuse yourself from class that day."
  },
  {
    "objectID": "syllabus.html#academic-integrity-and-collaboration",
    "href": "syllabus.html#academic-integrity-and-collaboration",
    "title": "Syllabus",
    "section": "Academic Integrity and Collaboration",
    "text": "Academic Integrity and Collaboration\n\nAcademic Integrity\nBriefly, academic integrity means that you assume responsibility for ensuring that the work you submit demonstrates your learning and understanding.\nTo be frank, it’s pretty easy to act without integrity (i.e. cheat) in this course. First, there’s a lot of solution code for machine learning tasks in Python online. Second, I’m literally asking you all to post your assignments publicly online. So, there are lots of opportunities to turn in assignments without actually doing the learning that those assignments are designed to offer you.\nI assume that both of us want you to learn some cool stuff. Cheating stops you from doing that, and ultimately wastes both your time and mine. I won’t be vigorously hunting for academic integrity violations, but I may ask you to discuss code or theory with me in class or in our meetings. If I notice you struggling to explain code that you submitted for feedback, I may have questions.\nTrust me. Neither of us want this."
  },
  {
    "objectID": "syllabus.html#collaboration",
    "href": "syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nI love it! Please collaborate in ways that allow you and your collaboration partners to fully learn from and engage with the content. Sharing small snippets of code or math is often helpful to get someone unstuck, but sharing complete function implementations or mathematical arguments is usually counterproductive.\nHere are some general guidelines for how I think about collaboration."
  },
  {
    "objectID": "syllabus.html#general-advice",
    "href": "syllabus.html#general-advice",
    "title": "Syllabus",
    "section": "General Advice",
    "text": "General Advice\nI am always happy to talk with you about your future plans, including internships, research opportunities, and graduate school applications. Because I am a creature of the academy, I am less knowledgeable about industry jobs, although you are welcome to ask about those too. You can drop in during Student Hours or email me to make an appointment."
  },
  {
    "objectID": "syllabus.html#letters-of-recommendation",
    "href": "syllabus.html#letters-of-recommendation",
    "title": "Syllabus",
    "section": "Letters of Recommendation",
    "text": "Letters of Recommendation\nWriting letters of recommendation for students is a fundamental part of my job and something that I am usually very happy to do. Here’s how to ask me for a letter."
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Asking for Help",
    "section": "",
    "text": "Asking for help is a fundamental part of how you will learn in CSCI 0451. Do it often. Here is some wisdom on this topic from the Best Cat On the Internet:\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "help.html#dont-get-stuck",
    "href": "help.html#dont-get-stuck",
    "title": "Asking for Help",
    "section": "Don’t Get Stuck",
    "text": "Don’t Get Stuck\nWe want to productively challenge you, which is different from letting you get stuck. If you’ve spent more than 30 minutes without making any progress or change in your understanding, then that’s likely a sign that you should consult a new reading or other resource, ask for help from a classmate, a Course Assistant, or me."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week 1 \n            \n        \n            \n                    M\n                    \n                    Feb. 13 \n             Welcome!\n   \n        \n        \n             \n                We discuss how the course works and begin our discussion of classification and auditing. \n            \n        \n        \n            \n            \n                    Learning Objectives\n                            Getting Oriented  \n            \n            \n                    Reading\n                           Course syllabus \n                           Collaboration \n                           Why I Don't Grade by Jesse Stommel \n                           Daumé 1.1-1.5 \n            \n            \n                    Notes\n                            Welcome slides\n  \n            \n            \n                    Warmup\n                    Set up your software.\n            \n            \n                    Assignments\n                    No really, set up your software.\n            \n        \n        \n            \n                    W\n                    \n                    Feb. 15 \n             Classification: The Perceptron\n   \n        \n        \n             \n                We study the perceptron algorithm, a historical method that serves as the foundation for many modern classifiers. \n            \n        \n        \n            \n            \n                    Learning Objectives\n                            Theory  \n                            Implementation  \n            \n            \n                    Reading\n                           Daumé 4.1-4.5, 4.7 \n                           Introduction to Numpy from The Python Data Science Handbook by Jake VanderPlas \n                           Linear algebra with Numpy \n                           Hardt and Recht, p. 37-41 \n            \n            \n                    Notes\n                            Lecture notes\n  \n            \n            \n                    Warmup\n                     Perceptron \n            \n            \n                    Assignments\n                    Blog post: perceptron\n            \n        \n            \n             Week 2 \n            \n        \n            \n                    M\n                    \n                    Feb. 20 \n             Models, Loss Functions, and Algorithms\n   \n        \n        \n             \n                We generalize our study of the perceptron to a broader framework for supervised machine learning called empirical risk minimization. Within this framework, we begin to study logistic regression and the primal support vector machine for binary classification.\n            \n        \n        \n            \n            \n                    Learning Objectives\n                            Theory  \n                            Implementation  \n            \n            \n                    Reading\n                           Daumé 2.1-2.7 \n                           Daumé 7.1-7.3 \n            \n            \n                    Notes\n                            TBD  \n            \n            \n                    Warmup\n                    TBD\n            \n            \n            \n        \n        \n            \n                    W\n                    \n                    Feb. 22 \n             More On Classification and Auditing\n   \n        \n        \n             \n                We continue our discussion of classification and introduce algorithmic bias. \n            \n        \n        \n            \n            \n                    Learning Objectives\n                            Social Responsibility  \n                            Navigation  \n                            Experimentation  \n            \n            \n                    Reading\n                           Machine Bias from ProPublica \n                           Data Manipulation with Pandas from The Python Data Science Handbook by Jake VanderPlas \n                           Example 2.3.1 from Murphy \n            \n            \n                    Notes\n                            Lecture notes\n  \n            \n            \n                    Warmup\n                     Classification rates \n            \n            \n                    Assignments\n                    Reflective goal-setting ACTUAL REQUIRED DUE DATE: 2/24\n            \n        \n            \n             Week 3 \n            \n        \n            \n                    M\n                    \n                    Feb. 27 \n             Optimization\n   \n        \n        \n             \n                We discuss standard mathematical methods for empirical risk minimization, including gradient descent and stochastic gradient descent. We also recontextualize the perceptron algorithm as stochastic subgradient descent for the primal support vector machine. \n            \n        \n        \n            \n            \n                    Learning Objectives\n                            Theory  \n                            Implementation  \n            \n            \n                    Reading\n                           TBD \n            \n            \n                    Notes\n                            TBD  \n            \n            \n                    Warmup\n                    TBD\n            \n            \n                    Assignments\n                    Blog post: empirical risk minimization\n            \n        \n        \n\nNo matching items\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "© Phil Chodrow, 2023"
  }
]