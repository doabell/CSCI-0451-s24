[
  {
    "objectID": "lecture-notes/gd-demo.html",
    "href": "lecture-notes/gd-demo.html",
    "title": "",
    "section": "",
    "text": "import numpy as np \nfrom scipy.optimize import minimize\nnp.seterr(all='ignore') \nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n    \nnp.random.seed(123)\n\n\n# add a constant feature to the feature matrix\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef predict(X, w):\n    return X@w\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef logistic_loss(y_hat, y): \n    return -y*np.log(sigmoid(y_hat)) - (1-y)*np.log(1-sigmoid(y_hat))\n\ndef empirical_risk(X, y, loss, w):\n    y_hat = predict(X, w)\n    return loss(y_hat, y).mean()\n\n\n# pick random w and see the classifier \nw = .5 - np.random.rand(p_features)\n\nloss = empirical_risk(X_, y, logistic_loss, w)\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n# compute better w with gradient descent\n\nfrom hidden.logistic import gradient\n\nalpha = .001 # learning rate\n\ndone = False       # initialize for while loop\nprev_loss = np.inf # handy way to start off the loss\n\nhistory = []\n\n# main loop\nwhile not done: \n    w -= alpha*gradient(w, X_, y)                      # gradient step\n    new_loss = empirical_risk(X_, y, logistic_loss, w) # compute loss\n    \n    history.append(new_loss)\n    # check if loss hasn't changed and terminate if so\n    if np.isclose(new_loss, prev_loss):          \n        done = True\n    else:\n        prev_loss = new_loss\n\n\n\nloss = empirical_risk(X_, y, logistic_loss, w)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\naxarr[1].plot(history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/regression-live.html",
    "href": "lecture-notes/regression-live.html",
    "title": "Least-Squares Linear Regression",
    "section": "",
    "text": "$$\n$$\nSo far in this course, we’ve focused exclusively on classification tasks: how to predict a categorical label for each data point. The other important task we need to consider is regression, in which we predict a real number for each data point based on its features. Here’s the stereotypical example:\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nLooking at this data, we can see an apparent linear trend that we would like to use in order to make prediction on new data points.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/regression-live.html#mathematical-formulation",
    "href": "lecture-notes/regression-live.html#mathematical-formulation",
    "title": "Least-Squares Linear Regression",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nWe’re going to focus on least-squares linear regression. The nice thing about least-squares linear regression is that it falls perfectly into our framework of convex linear models. In least-squares linear regression, we still make predictions of the form \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\), since these are exactly linear predictions! The loss function is \\(\\ell(\\hat{y}, y) = (\\hat{y} - y)^2\\), the squared error, which is convex. The empirical risk minimization problem is\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) \\\\\n          &= \\sum_{i = 1}^n \\ell(\\hat{y}_i, y_i) \\\\\n          &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\sum_{i = 1}^n \\left(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - y_i \\right)^2\\;.\n\\end{aligned}\n\\] It’s useful to write this in a more compact way using matrix-vector notation: the loss function \\(L(\\mathbf{w})\\) can be written\n \\[\nL(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2\\;.\n\\]Reminder: \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times p}\\), \\(\\mathbf{w}\\in \\mathbb{R}^{p}\\), \\(\\mathbf{X}\\mathbf{w}\\in \\mathbb{R}^n\\), which is the same dimension as \\(\\mathbf{y}\\).\nSo, we want to solve the problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2\\;.\n\\tag{1}\\]"
  },
  {
    "objectID": "lecture-notes/regression-live.html#solution-methods",
    "href": "lecture-notes/regression-live.html#solution-methods",
    "title": "Least-Squares Linear Regression",
    "section": "Solution Methods",
    "text": "Solution Methods\nThere are a lot of ways to solve Equation 1. Let’s start by taking the gradient with respect to \\(\\hat{\\mathbf{w}}\\). Using the multivariate chain rule, this is\n\\[\n\\nabla L(\\mathbf{w}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\tag{2}\\]\nOne way to approach the linear regression problem is with gradient descent: repeat the iteration\n\\[\n\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - 2\\alpha \\mathbf{X}^T(\\mathbf{X}\\mathbf{w}^{(t)} - \\mathbf{y})\n\\]\nto convergence. As it turns out, there’s also an explicit formula involving a matrix inversion that we can obtain by using the condition \\(\\nabla L(\\mathbf{w}) = \\mathbf{0}\\) which must hold at the minimum. Plugging in our expression for \\(L(\\mathbf{w})\\), we get\n\\[\n\\mathbf{0}= \\mathbf{X}^T(\\mathbf{X}\\hat{\\mathbf{w}} - \\mathbf{y})\\;.\n\\]\nTo start solving for \\(\\hat{\\mathbf{w}}\\), we can move \\(\\mathbf{X}^T\\mathbf{y}\\) to the other side:\n\\[\n\\mathbf{X}^T\\mathbf{X}\\hat{\\mathbf{w}} = \\mathbf{X}^T\\mathbf{y}\\;.\n\\]\n Now, provided that the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is of full rank, we can multiply both sides by \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) to obtain \\[\n\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\;,\n\\tag{3}\\] which is an explicit formula for \\(\\hat{\\mathbf{w}}\\).This requires that there are at least \\(p\\) linearly independent rows of \\(\\mathbf{X}\\). In particular, \\(\\mathbf{X}\\) must have at least as many rows as it has columns.\nLet’s see if we can use this to compute predictions for our fake data above. In order for this formula to work, we need to ensure that \\(\\mathbf{X}\\) is padded with a vector of ones.\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nX = pad(x)\n\nNow we can use the formula:\n\nw_hat = np.linalg.inv(X.T@X)@X.T@y\nw_hat\n\narray([[ 0.6946368],\n       [-0.4953097]])\n\n\nLet’s test this out on our fake data:\n\nplt.scatter(x, y)\ny_hat = X@w_hat\nplt.plot(x, y_hat, color = \"black\")\n\n\n\n\nNot bad!\n\n\n\n\n\n\nActivity 1: Computational Complexity of Exact Least-Squares Regression\n\n\n\nMultiplying a \\(k\\times \\ell\\) matrix with an \\(\\ell \\times k\\) matrix using the standard algorithm has time complexity \\(k \\ell^2\\). Inverting a \\(k\\times k\\) matrix (when the inverse exists) has time complexity \\(k^3\\). Left-multiplying a \\(k\\times k\\) matrix and a \\(k\\)-vector has time complexity \\(k^2\\).\nWith these facts in mind, describe the time complexity of computing \\(\\hat{\\mathbf{w}}\\) using Equation 3 in terms of the number of data points \\(n\\) and the number of features \\(p\\). What would the computational bottleneck when the number of data points \\(n\\) is very large? What about what the number of features \\(p\\) is very large?\n\n\n\nAs a reminder, we are talking about the formula \\(\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\;,\\).\n\n\n\n\n\nActivity 2: Computational Complexity of Gradient Descent\n\n\n\nAs you’ll implement in your blog post, gradient descent can actually be implemented via the iteration\n\\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha (\\mathbf{P}\\mathbf{w}^{(t)} - \\mathbf{q})\\;,\n\\]\nwhere \\(\\mathbf{P}\\) is a \\(p \\times p\\) matrix and \\(\\mathbf{q}\\) is a \\(p\\)-vector.  What is the time complexity of a single iteration of gradient descent?\nOf course, a full analysis of the time complexity of the gradient descent algorithm as a whole requires knowing how many iterations are necessary to achieve acceptable accuracy, which is a much harder problem.\n\n\nThe trick is to precompute some matrix products"
  },
  {
    "objectID": "lecture-notes/regression-live.html#scoring-linear-models",
    "href": "lecture-notes/regression-live.html#scoring-linear-models",
    "title": "Least-Squares Linear Regression",
    "section": "Scoring Linear Models",
    "text": "Scoring Linear Models\nA good, simple way to score linear models is by using the loss function directly: smaller values are better! In scikit-learn, regression models are instead scored using an affine transformation of the loss function called the coefficient of determination. The coefficient of determination is 1 when the model fits the data perfectly with no errors. It can be arbitrarily negative (e.g. -8.7) if the model fits the data very poorly.\nFor a quick illustration, here’s the scikit-learn implementation of linear regression:\nLet’s evaluate our model on similar, unseen data:\n\nn = 100\nx_val = np.random.rand(n, 1)\ny_val = w1*x_val + w0 + 0.1*np.random.randn(n, 1)\n\nAs usual, gaps between the training and validation scores suggest the possibility of overfitting, although further investigation is required to see whether improvement on validation data is possible."
  },
  {
    "objectID": "lecture-notes/regression-live.html#incorporating-features",
    "href": "lecture-notes/regression-live.html#incorporating-features",
    "title": "Least-Squares Linear Regression",
    "section": "Incorporating Features",
    "text": "Incorporating Features\nSometimes (most of the time), the patterns we are looking for in our data are not actually linear. For example:\n\nx = 2*np.pi*np.random.rand(n, 1)\ny = np.sin(x) + 0.2*np.random.randn(n, 1)\n\nplt.scatter(x, y)\n\nJust like in the case of classification, we can use feature maps to learn nonlinear patterns. For example:\nThe feature matrix \\(\\Phi\\) now plays the role of \\(\\mathbf{X}\\), and we can use the same formula as earlier:\nThe predictions are\nLet’s visualize:\n\nplt.scatter(x, y)\n\nx_lin = np.linspace(0, 2*np.pi, 1001)[:,np.newaxis]\nPHI_lin = p.fit_transform(x_lin)\ny_trend = PHI_lin@w_hat\n\nplt.plot(x_lin, y_trend, color = \"black\")\n\nHmmm, this is not a very impressive fit. Let’s wrap this process in a function and do a few experiments.\n\ndef poly_viz(deg, ax):\n    p = PolynomialFeatures(deg)\n    PHI = p.fit_transform(x)\n    w_hat = np.linalg.inv(PHI.T@PHI)@PHI.T@y\n    x_lin = np.linspace(x.min(), x.max(), 1001)[:,np.newaxis]\n    PHI_lin = p.fit_transform(x_lin)\n    y_trend = PHI_lin@w_hat\n    ax.scatter(x, y)\n    ax.plot(x_lin, y_trend, color = \"black\")\n\nAs in classification, the use of polynomial features makes us susceptible to overfitting, and validation or cross-validation should be used in order to select a good degree.\n\nKernel Regression\nKernel methods offer a theoretically-grounded approach for dealing with nonlinearity in both classification and regression.  In kernel methods, the feature vector corresponding to each data point \\(\\mathbf{x}_i\\) is actually in terms of all the other points:You can implement a kernel classification method in this blog post.\n\\[\n\\phi(\\mathbf{x}_i) = \\left(\\begin{matrix}\n                k(\\mathbf{x}_i, \\mathbf{x}_1) \\\\\n                k(\\mathbf{x}_i, \\mathbf{x}_2) \\\\\n                \\cdots \\\\\n                k(\\mathbf{x}_i, \\mathbf{x}_n) \\\\\n              \\end{matrix}\\right)\n\\]\nHere, \\(k:\\mathbb{R}^{p} \\times \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) is a special function called a kernel function. Usually the kernel function is a measure of how similar two points are. A very common and useful kernel is the radial basis function (RBF) kernel\n\\[\nk(\\mathbf{x}_1, \\mathbf{x}_2) = e^{-\\gamma \\lVert \\mathbf{x}_1 - \\mathbf{x}_2 \\rVert^2}\\;.\n\\]\nThis function is largest when \\(\\mathbf{x}_1 = \\mathbf{x}_2\\), and decreases as these two vectors become farther apart. The idea of kernel methods is that the prediction can be expressed as a weighted sum of the target values at nearby points.\nKernel methods have extremely beautiful mathematics behind them and are fun to implement, but for today we can just show the implementation in scikit-learn:\nDifferent values of gamma will result in more or less “wiggly” fits. gamma should be tuned using validation or cross validation in order to protect against overfitting."
  },
  {
    "objectID": "lecture-notes/regression-live.html#activity",
    "href": "lecture-notes/regression-live.html#activity",
    "title": "Least-Squares Linear Regression",
    "section": "Activity",
    "text": "Activity\nThe following code produces a random feature matrix \\(\\mathbf{X}\\) and weight vector \\(\\mathbf{w}\\):\n\nX = np.random.rand(10, 3)\nX = pad(X)\nw = np.random.rand(X.shape[1])\n\ny = X@w + np.random.randn(X.shape[0])\n\n\nImplement a predict function that computes y_hat from X and w (hint: don’t overthink it).\nImplement a score function that accepts X, y, and w as arguments and computes the coefficient of determination of the predictions. The coefficient of determination is \\[\nc = 1 - \\frac{\\sum_{i = 1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i = 1}^n (\\bar{y}_i - y_i)^2}\\;,\n\\] where \\(\\bar{y} = \\frac{1}{n} \\sum_{i = 1}^n\\).\n\nYou can modify these functions and use them in the blog post on linear regression."
  },
  {
    "objectID": "lecture-notes/clustering.html",
    "href": "lecture-notes/clustering.html",
    "title": "Clustering Data",
    "section": "",
    "text": "$$\n$$\nClustering is a fundamental form of unsupervised learning in which the aim is to group data points into “similar” or “related” groups. In this way, it is superficially similar to classification. Suppose we have some data \\(\\mathbf{X}\\) that, when we plot it, looks like this:\nfrom sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\nX, y = make_blobs(n_samples=100, n_features=2, \n                                centers=2, random_state=1)\n\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nThis data looks a lot like the kind of data that we used for classification. This time, however, we ignore \\(\\mathbf{y}\\) (imagine our data didn’t come with any labels). We can still look at the plot and see that it apparently contains two groups or “clusters” of data. The clustering task is identify clusters that “fit” the data well, according to some criterion of “fit.” The k-means algorithm is one algorithm that attempts to perform this task. On this particular data, k-means does pretty well:\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters = 2)\nkm.fit(X)                    # NOTE: fit does not use y!! \n\nclusters = km.predict(X)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1], c = clusters, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nNote that what “pretty well” means here is subjective; because we don’t have any labels \\(\\mathbf{y}\\), we can’t say that the clustering found by the model is “good” according to any objective criterion. Often we just have to eyeball the groups, or attempt to interpret them based on some prior knowledge we might have about the data.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/clustering.html#k-means-clustering",
    "href": "lecture-notes/clustering.html#k-means-clustering",
    "title": "Clustering Data",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nIn the k-means algorithm, we divide the data into clusters by finding a cluster centroid for each. The centroid is the mean feature value in each cluster, and gives a summary of the location of the cluster. Suppose we have \\(\\ell\\) clusters, and let \\(\\mathbf{m}_j \\in \\mathbb{R}^p\\), \\(j = 1,\\ldots,\\ell\\) be the proposed cluster centroids, which we can collect into a matrix \\(\\mathbf{M}\\in \\mathbb{R}^{\\ell \\times p}\\). To associate each data point \\(\\mathbf{x}_i\\) to a cluster centroid \\(\\mathbf{m}_j\\), we also posit a vector \\(\\mathbf{z}\\in [\\ell]^n\\) of cluster labels, one for each point. If \\(z_i = j\\), this says that “point \\(i\\) belongs to cluster \\(j\\).”The label vector \\(\\mathbf{z}\\) might look superficially similar to the target vector \\(\\mathbf{y}\\) in classification problems. The difference here is that we are free to set \\(\\mathbf{z}\\) as part of the learning process.\nOur aim is to find the cluster centroids \\(\\mathbf{m}_j\\), \\(j = 1,\\ldots, \\ell\\) and labels \\(\\mathbf{z}\\) that separate the data “well.” What does “well” mean? As usual, in order to define this, we need to define a loss function. We’re outside our usual supervised framework, so we need our loss function to come from a different place. The standard loss function for k-means comes from a simple intuition: a good clustering is one in which the points in each group are close to their group centroid. This leads us to the problem\n \\[\n\\hat{\\mathbf{M}}, \\hat{\\mathbf{z}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{M},\\;\\mathbf{z}} \\sum_{i = 1}^n \\lVert \\mathbf{x}_i - \\mathbf{m}_{z_i} \\rVert^2\\;.\n\\tag{1}\\]In this optimization, we we assume that \\(\\mathbf{M}\\in \\mathbb{R}^{\\ell \\times p}\\). The problem of how to choose a good value of \\(\\ell\\), the number of clusters, is a more challenging one that we won’t discuss in this course.\nThis is a nice mathematical formulation with a deep, dark secret: it’s not convex! This means that we can’t expect to find the “best” \\(\\hat{\\mathbf{M}}\\) and \\(\\hat{\\mathbf{z}}\\); we can only find “pretty good ones,” where “pretty good” means “locally optimal.” There’s a beautifully simple algorithm that we can use to perform this task:\n\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\n\ndef k_means_step(X, M, z):\n    # compute the distances between all points and all centroids\n    D = pairwise_distances(X, M)\n    # each point's new group is its closest centroid\n    z = np.argmin(D, axis = 1)\n    # each centroid's new value is the mean of all the points in its group\n    for j in range(M.shape[0]):\n        M[j,:] = X[z == j].mean(axis = 0)\n    return M, z\n\ndef k_means(X, ell):\n\n    # random initialization\n    n, p = X.shape\n    M = np.random.rand(ell, p)\n    z_prev = np.random.randint(0, ell, n)\n    done = False\n\n    # do k_means_step until z stops changing\n    while not done: \n        M, z = k_means_step(X, M, z_prev)\n        if np.all(z_prev == z):\n            done = True\n        z_prev = z\n    \n    # return the centroid matrix and cluster labels\n    return M, z\n\nLet’s run this algorithm and plot the results.\n\nM, z = k_means(X, 2)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1], c = z, alpha = 0.4, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nax.scatter(M[:,0], M[:,1], s = 50, color = \"black\")\n\n&lt;matplotlib.collections.PathCollection at 0x7fdf49b60220&gt;\nOriginal data is shown in blue and yellow according to the cluster learned via the k-means algorithm. Centroids of each cluster are shown in black.\n\n\n\n\n\nThe implementation of the algorithm above doesn’t handle an important issue: if one of the centroids is far away from all of the clusters and therefore gets no points assigned to it. This leads to errors because it’s impossible to compute a centroid \\(\\mathbf{m}\\) for those clusters. A complete implementation would need to handle this issue, but we won’t worry about it here.\nLooks ok! We have segmented the data into two clusters and found reasonable looking centroids, even though we didn’t begin with any true labels.\nThe following result is the fundamental theorem for k-means:\n\n\n\n\n\n\n\nTheorem 1 (“K-means works”) Each iteration of k-means-step does not increase the objective function in Equation 1, provided that every cluster centroid is associated to at least one point. Furthermore, in this case, the k-means algorithm converges after a finite number of steps."
  },
  {
    "objectID": "lecture-notes/clustering.html#laplacian-spectral-clustering",
    "href": "lecture-notes/clustering.html#laplacian-spectral-clustering",
    "title": "Clustering Data",
    "section": "Laplacian Spectral Clustering",
    "text": "Laplacian Spectral Clustering\nK-means is a useful algorithm with an important limitation: it works best when the data is approximately linearly separable! So, we wouldn’t expect k-means to do very well at all if we wanted to separate the two rings in a data set like this:\n\nnp.random.seed(1234)\n\nn = 500\nX, y = make_circles(n_samples=n, shuffle=True, noise=0.07, random_state=None, factor = 0.5)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nLet’s check:\n\nM, z = k_means(X, 2)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1], c = z, alpha = 0.4, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nax.scatter(M[:,0], M[:,1], s = 50, color = \"black\")\n\n&lt;matplotlib.collections.PathCollection at 0x7fdf4a02afd0&gt;\n\n\n\n\n\nYou can see what k-means was going for here, but the result doesn’t distinguish the two features that most of us would pick out by eye.\nWe’ll now develop Laplacian spectral clustering, a clustering method that uses matrix eigenvectors to cluster the data. Spectral clustering is a very beautiful and effective technique for nonlinear clustering of data. It has two primary limitations: it is most effective for binary clustering, and it can be computationally expensive for larger data sets."
  },
  {
    "objectID": "lecture-notes/clustering.html#nearest-neighbors-graph",
    "href": "lecture-notes/clustering.html#nearest-neighbors-graph",
    "title": "Clustering Data",
    "section": "Nearest-Neighbors Graph",
    "text": "Nearest-Neighbors Graph\nLaplacian clustering begins by computing the \\(k\\)-nearest-neighbors graph. In the \\(k\\)-nearest-neighbors graph, we draw a connecting edge between each point and the \\(k\\) points that are nearest to it in distance.\n\nfrom sklearn.neighbors import NearestNeighbors\n\nk = 10\nnbrs = NearestNeighbors(n_neighbors=k).fit(X)\nA = nbrs.kneighbors_graph().toarray()\n\n# symmetrize the matrix\nA = A + A.T\nA[A &gt; 1] = 1\n\nHere, the ith row of A contains a 1 in each column j such that j is one of the five nearest neighbors of i. The following function will draw this graph for us:\n\nimport networkx as nx\n\ndef plot_graph(X, A, z = None, ax = None, show_edge_cuts = True):\n    G = nx.from_numpy_array(A)\n    if z is None:\n        nx.draw(G, pos = X, alpha = .4, node_color = \"grey\", node_size = 20, ax = ax)\n    else: \n        if show_edge_cuts:\n            colors = [\"red\" if z[i] != z[j] else \"grey\" for i, j in G.edges()]\n            widths = [2 if z[i] != z[j] else 1 for i, j in G.edges()]\n        else:\n            colors = \"black\"\n            widths = 1\n        \n        nx.draw(G, pos = X, alpha = .4, node_color = z, node_size = 20, edge_color = colors, width = widths, ax = ax, cmap=plt.cm.cividis)\n\n    plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nfig, ax = plt.subplots(figsize = (4, 4))\nplot_graph(X, A)\n\n\n\n\nWe can now frame our task as one of finding the “pieces” of this graph. Defining a “piece” of a graph mathematically takes a bit of work. To think about this problem mathematically, let’s first define a label vector \\(\\mathbf{z}\\in [0,1]^n\\). As usual, our machine learning problem is actually a mathematical minimization problem: we want to define an objective function \\(f\\) such that \\(f(\\mathbf{z})\\) is small when the labels of \\(\\mathbf{z}\\) are “good.”"
  },
  {
    "objectID": "lecture-notes/clustering.html#cut-based-clustering",
    "href": "lecture-notes/clustering.html#cut-based-clustering",
    "title": "Clustering Data",
    "section": "Cut-Based Clustering",
    "text": "Cut-Based Clustering\nA reasonable first pass at this problem is to define a clustering to be good when it doesn’t “cut” too many edges. An edge is “cut” if it has two nodes in different clusters. For example, here are two possible clusterings, with the edges cut by each clustering shown in red.\n\nfig, axarr = plt.subplots(1, 2, figsize = (8, 4))\ny_bad = np.random.randint(0, 2, n)\n\nplot_graph(X, A, z = y, ax = axarr[0])\nplot_graph(X, A, z = y_bad, ax = axarr[1])\n\n\n\n\nThe righthand plot has many more red edges, which connect nodes in two different proposed clusters. In contrast, the lefthand plot has many fewer.\nHere are the cut values of these two clusterings:\n\nfrom sklearn.neighbors import NearestNeighbors\n\ndef cut(A, z):\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ngood labels cut = 14.0\nbad labels cut = 2950.0\n\n\nSo, it might seem as though a good approach would be to minimize the cut value of a label vector. Unfortunately, cut minimization has a fatal flaw: the best cut is always the one that assigns all nodes to the same label! This clustering always achieves a cut score of 0. So, we need to try something different."
  },
  {
    "objectID": "lecture-notes/clustering.html#normalized-cut",
    "href": "lecture-notes/clustering.html#normalized-cut",
    "title": "Clustering Data",
    "section": "Normalized Cut",
    "text": "Normalized Cut\nTo define a better objective, we need a little notation. The volume of cluster \\(j\\) in matrix \\(\\mathbf{A}\\) with clustering vector \\(\\mathbf{z}\\) is defined as\n\\[\n\\mathrm{vol}_{j}\\left(\\mathbf{A},\\mathbf{z}\\right) = \\sum_{i = 1}^n \\sum_{i' = 1}^n \\mathbb{1}\\left[ z_i = j \\right] a_{ii'}\n\\]\nHeuristically, \\(\\mathrm{vol}_{j}\\left(\\mathbf{A},\\mathbf{z}\\right)\\) is the number of edges that have one node in cluster \\(j\\). The normalized cut objective function is\n\\[\nf(\\mathbf{z}, \\mathbf{A}) = \\mathrm{cut}\\left(\\mathbf{A},\\mathbf{z}\\right)\\left(\\frac{1}{\\mathrm{vol}_{0}\\left(\\mathbf{A},\\mathbf{z}\\right)} + \\frac{1}{\\mathrm{vol}_{1}\\left(\\mathbf{A},\\mathbf{z}\\right)}\\right)\n\\]\nThe normcut of our preferable clustering is still better than the random one:\n\ndef vol(j, A, z):\n    return A[z == j,:].sum()\n\ndef normcut(A, z):\n    return cut(A, z) * (1/vol(0, A, z) + 1/vol(1, A, z))\n\nprint(f\"good labels normcut = {normcut(A, z = y)}\") \nprint(f\"bad labels normcut = {normcut(A, z = y_bad)}\") \n\ngood labels normcut = 0.009632954990428188\nbad labels normcut = 2.036335377012906\n\n\n\n\n\n\n\n\nHow do the \\(\\frac{1}{\\mathrm{vol}_{j}\\left(\\mathbf{A},\\mathbf{z}\\right)}\\) terms stop the normcut from favoring the clustering in which one cluster contains no nodes?"
  },
  {
    "objectID": "lecture-notes/clustering.html#spectral-approximation",
    "href": "lecture-notes/clustering.html#spectral-approximation",
    "title": "Clustering Data",
    "section": "Spectral Approximation",
    "text": "Spectral Approximation\nGreat! How do we choose \\(\\mathbf{z}\\) to minimize the normcut?\n\n\n\n\n\n\n\nTheorem 2 The problem of finding a binary vector \\(\\mathbf{z}\\in \\{0,1\\}^n\\) that minimizes the normcut objective is NP-hard.\n\n\n\n\nWhoops! In fact, we can’t minimize the normcut exactly, so we need to do so approximately.\nHere is the trick. Suppose that we have a binary clustering vector \\(\\mathbf{z}\\). We’re going to associate \\(\\mathbf{z}\\) with a modified clustering vector \\(\\tilde{\\mathbf{z}}\\) with entries\n\\[\n\\tilde{z}_i = \\begin{cases}\n    &\\frac{1}{\\mathrm{vol}_{0}\\left(\\mathbf{A},\\mathbf{z}\\right)} \\quad z_i = 0 \\\\\n    - &\\frac{1}{\\mathrm{vol}_{1}\\left(\\mathbf{A},\\mathbf{z}\\right)} \\quad z_i = 1\\;.\n\\end{cases}\n\\]\nA direct mathematical calculation then shows that we can write the normcut objective as \\(f(\\mathbf{z}) = \\tilde{f}({\\tilde{\\mathbf{z}}})\\), where\n\\[\n\\tilde{f}({\\tilde{\\mathbf{z}}}) = \\frac{\\tilde{\\mathbf{z}}^T(\\mathbf{D}- \\mathbf{A})\\tilde{\\mathbf{z}}}{\\tilde{\\mathbf{z}}^T\\mathbf{D}\\tilde{\\mathbf{z}}}\\;,\n\\tag{2}\\]\nwhere\n\\[\n\\mathbf{D}= \\left[\\begin{matrix} \\sum_{i = 1}^n a_{i1} & & & \\\\\n    & \\sum_{i = 1}^n a_{i2} & & \\\\\n    &  & \\ddots & \\\\\n    & & & \\sum_{i = 1}^n a_{in}\n\\end{matrix}\\right]\\;.\n\\]\nAdditionally, we have \\[\n\\tilde{\\mathbf{z}}^T\\mathbf{D}\\mathbf{1}= 0\\;,\n\\tag{3}\\] where \\(\\mathbf{1}\\) is the vector containing all 1s. Heuristically, Equation 3 says that cluster 0 and cluster 1 contain roughly the same number of edges within them.\nThis cheat is common enough that it has a name: we call it the spectral relaxation.\nAt this stage we do a cheat: we forget about the requirement that \\(\\tilde{\\mathbf{z}}\\) have the form we stated above. Instead, we simply solve the optimization problem\n\\[\n\\begin{aligned}\n\\tilde{\\mathbf{z}} =& \\mathop{\\mathrm{arg\\,min}}_{\\tilde{\\mathbf{z}}} \\frac{\\tilde{\\mathbf{z}}^T(\\mathbf{D}- \\mathbf{A})\\tilde{\\mathbf{z}}}{\\tilde{\\mathbf{z}}^T\\mathbf{D}\\tilde{\\mathbf{z}}} \\\\\n& \\text{such that }\\tilde{\\mathbf{z}}^T\\mathbf{D}\\mathbf{1}= 0\n\\end{aligned}\n\\]\nA beautiful theorem from linear algebra states that there is an explicit solution to this problem: \\(\\mathbf{z}\\) should be the eigenvector with the second-smallest eigenvalue of the matrix \\(\\mathbf{L}= \\mathbf{D}^{-1}[\\mathbf{D}- \\mathbf{A}]\\). This matrix \\(\\mathbf{L}\\) is called the normalized Laplacian, and it is the reason that this clustering algorithm is called Laplacian spectral clustering.\n\nfrom hidden.spectral import second_laplacian_eigenvector\nfig, ax = plt.subplots(figsize = (4, 4))\nz_ = second_laplacian_eigenvector(A)\nplot_graph(X, A, z = z_, show_edge_cuts = False, ax = ax)\n\n\n\n\nDarker shades correspond to nodes on which the eigenvector is negative, while lighter shades correspond to nodes on which the eigenvector is positive. We can get a final set of cluster labels just by assigning nodes to the cluster depending on the sign of the estimated \\(\\tilde{\\mathbf{z}}\\):\n\nz = z_ &gt; 0\nfig, ax = plt.subplots(figsize = (4, 4))\nplot_graph(X, A, z, show_edge_cuts = True, ax = ax)\n\n\n\n\nOur method of distinguishing the two rings using Laplacian spectral clustering seems to have worked! Let’s try with another data set:\n\nfrom sklearn.datasets import make_moons\n\nX, z = make_moons(n_samples=100, random_state=1, noise = .1)\nfig, ax = plt.subplots(figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nNow we’ll use a complete implementation of Laplacian spectral clustering, which you can complete in an optional blog post.\n\nfrom hidden.spectral import spectral_cluster\n\nfig, ax = plt.subplots(figsize = (4, 4))\nz = spectral_cluster(X, n_neighbors = 6)\na = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nLooks pretty good! Note, however, that we chose a pretty specific value of n_neighbors, the number of neighbors to use when forming the nearest-neighbors graph. This is a hyperparameter that needs to be tuned in some way. Unfortunately, cross-validation isn’t really an option here, as we don’t have a loss function or training data to use for validation purposes.\nThe performance of Laplacian spectral clustering can depend pretty strongly on this parameter. In this data set, small values can lead to oddly fragmented clusters, while larger values lead to results that don’t look too different from what we might expect from k-means:\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spectral_cluster(X, n_neighbors = i)\n    a = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\nAs usual, there are ways to address these limitations, but these are beyond our scope for today."
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html",
    "href": "lecture-notes/classification-in-practice-live.html",
    "title": "Introduction to Classification in Practice",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html#but",
    "href": "lecture-notes/classification-in-practice-live.html#but",
    "title": "Introduction to Classification in Practice",
    "section": "But…",
    "text": "But…\nThere are actually a lot of practicalities to consider here as well! Where does our data come from? How do we prepare it for analysis? If we are going to use a feature map \\(\\phi\\) for things like polynomial features, how do we choose the right feature map? If our model has hyperparameters for things like regularization, how do we choose the right hyperparameters? All of these are questions that we need to handle in the practice of machine learning.\nOur purpose in this lecture is to actually work through some of the common steps of the standard machine learning workflow."
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html#prediction-question",
    "href": "lecture-notes/classification-in-practice-live.html#prediction-question",
    "title": "Introduction to Classification in Practice",
    "section": "Prediction Question",
    "text": "Prediction Question\nThe standard prediction question for this data set is:\n\nCan we predict whether or not a given passenger survived the crash of the Titanic, given information about them and their position on the ship?"
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html#data-inspection",
    "href": "lecture-notes/classification-in-practice-live.html#data-inspection",
    "title": "Introduction to Classification in Practice",
    "section": "Data Inspection",
    "text": "Data Inspection\nBefore modeling, it’s usually beneficial to learn about your data. It’s not always possible to do this without modeling, for example if your data is very high-dimensional. Because this data set has a relatively small number of features, we can learn a lot about it just through summaries. Let’s ask a few questions:\n\nWhat percentage of passengers in the training set survived?\n\nApproximately 40% of passengers in the training set survived. It’s important to keep this in mind because it sets the base rate for our problem. The base rate is the accuracy rate of a trivial model that doesn’t use the features. In this case, the trivial model is the model that always predicts that a passenger died. This base model would be right about 60% of the time.\n\nHow wealthy were the passengers on the Titanic?\n\nWe can’t know for certain, but we can learn about how much was paid for each passenger class:\n\nThe average price of 88 pounds for a first-class ticket corresponds to nearly $15,000 USD today.\nThe second-class ticket corresponds to roughly $3,500\nThe third class ticket corresponds to roughly $2,500.\n\nWe can safely assume that the first-class passengers were indeed substantially more wealthy on average than the others.\n\nDid wealth disparities make a difference for who was most likely to survive?\n\nWe can segment out survival rates by passenger class to learn more about this:\nIndeed, the higher passenger classes had significantly higher survival rates.\nThis difference is even starker if we also segment out the data by the sex of the passenger:\nThis table reflects the famous maritime tradition of prioritizing women and children first into the lifeboats, resulting in vastly higher survival rates among women in these data. Note the role of class: a 1st-class woman was twice as likely to survive as a third class woman, and a 1st-class man was nearly three times as likely to survive as a 3rd class man. Based on these observations, we might expect that passenger sex and Pclass might be useful features for us to incorporate into algorithms."
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html#data-preparation",
    "href": "lecture-notes/classification-in-practice-live.html#data-preparation",
    "title": "Introduction to Classification in Practice",
    "section": "Data Preparation",
    "text": "Data Preparation\nSo far, we’ve been working with 2d numpy arrays (matrices) of features and 1d numpy arrays (vectors) of target variables. We can treat pandas data frames of numbers like matrices, and we can treat pandas columns of numbers like vectors. For example, our y_train is already in a format that we can use:\nOn the other hand, our data frame has one column that we can’t use: the Sex column contains strings representing categories, rather than numbers. ML algorithms only understand numbers, and so we need to encode the Sex of the passengers as a number. We use so-called “one-hot encoding” for this, in which each category is represented by a binary column, with a 1 indicating that the passenger fell into that category. The Pandas function get_dummies() is an extremely convenient way to achieve this:\nThis looks better! We can now treat X_train as a matrix of features and use it as an input for any of our machine learning algorithms."
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html#modeling",
    "href": "lecture-notes/classification-in-practice-live.html#modeling",
    "title": "Introduction to Classification in Practice",
    "section": "Modeling",
    "text": "Modeling\nNow we’re ready to do some modeling! You may know how to implement logistic regression (and maybe you’ve already done it!), but for today we’ll use the scikit-learn implementation. We can already go ahead and fit our model. In sklearn, the score of a classification model is just the accuracy rate.\nSo, our model achieves about 80% accuracy on the training data, which is much better than the 60% we could have achieved by random guessing.\nLet’s take a look at the optimal parameter vector \\(\\mathbf{w}\\). This is stored in LR in the coef_ instance variable:\n\npd.DataFrame({\n  \"column\" : X_train.columns, \n  \"coefficient\" : LR.coef_.ravel()\n  })\n\nThe way to read these coefficients is that when the number in the corresponding column gets larger, the odds of survival decrease. For example, the negative coefficient of Pclass means that someone with a larger value of Pclass (e.g. 3) has a lower chance of survival in the model than someone with a lower value (e.g. 1). Note that very strongly negative coefficient of Sex_male, which expresses the much lower survival rate of men.\nAt this point we could just go ahead and and evaluate our model’s predictive capabilities by downloading the test set and checking our predictive accuracy. However, we should ask ourselves:\n\nIs this the best we can do?\n\nWe have all kinds of different choices that we can make that may help us improve our models. For example:\n\nFrom our first model it looks like Fare may not be an especially strong predictor because of its small coefficient. Maybe our model would generalize better if we just didn’t include it?\nShould we try incorporating some feature transformations, like polynomial features?\nShould we try regularizing our logistic regression?\n\nWe can’t exhaustively explore all possibilities, but let’s try to address one of these. Should we try incorporating polynomial features, and if so, of what degree?\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\nLet’s write a simple function that will construct a model with polynomial features for us:\n\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = \"none\", max_iter = 1e3))])\n\nWe can use it like this:\nIs that better or worse than the simple logistic model without polynomial features? Unfortunately we don’t really know; the reason is again that accuracy on the training isn’t usually a reliable indicator of predictive performance. In order to make an assessment, we can instead simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to plr.fit() happening under the hood.\nEach of these scores represents the model’s performance when used to predict one of the 5 folds of data after having been fit on the other 4. We often just average them to get an overall metric:\nNow we can try using cross-validation to get a sense for what degree of polynomial feature we should use. Degree 0 is actually the baseline model, and degree 1 corresponds to simple logistic regression without a polynomial feature map.\n\nfor deg in range(4):\n  plr = poly_LR(deg = deg)\n  cv_scores = cross_val_score(plr, X_train, y_train, cv=5)\n  mean_score = cv_scores.mean()\n  print(f\"Polynomial degree = {deg}, score = {mean_score.round(3)}\")\n\nIt looks like it doesn’t make a huge difference, but degree-2 polynomial features might be our best bet according to cross-validation. Let’s try go ahead and fit a single copy of this model on the entire training data:\nLet’s finally see how we do on the test set. We need to download the test set and process it in the same way that we did the training set.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/test.csv\"\n\ndf_test, X_test, y_test = read_titanic_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"], drop_first=\"if_binary\")\n\nNow we’re finally ready to compute the score. Drumroll please!\nWe achieve roughly 85% accuracy on the test data!\n\n\nIn case you’re wondering, our original logistic regression without polynomial features does almost as well on the test data:"
  },
  {
    "objectID": "lecture-notes/classification-in-practice-live.html#breaking-down-accuracy",
    "href": "lecture-notes/classification-in-practice-live.html#breaking-down-accuracy",
    "title": "Introduction to Classification in Practice",
    "section": "Breaking Down Accuracy",
    "text": "Breaking Down Accuracy\nWhen evaluating the performance of our algorithms, it’s not usually enough to just compute an overall score. The confusion matrix of a classifier on the test data is a convenient way to understand the kind of mistakes that your model most frequently makes. To construct a confusion matrix, we can use the confusion_matrix function from sklearn.metrics.\nThis matrix compares the real values of the label on each data point to their predicted values. There are two possibilities for the labels and we are comparing to their predicted values, so we have four possibilities.\n\nTrue positive (TP): \\(y_i = 1\\) and \\(\\hat{y}_i = 1\\). There are 51 true positives for this predictor on the test set.\nTrue negative (TN): \\(y_i = 0\\) and \\(\\hat{y}_i = 0\\). There are 100 true negatives for this predictor on the test set.\nFalse positive (FP): \\(y_i = 0\\) and \\(\\hat{y}_i = 1\\). There are 13 false positives for this predictor on the test set.\nFalse negative (FN): \\(y_i = 1\\) and \\(\\hat{y}_i = 0\\). There are 14 false negatives for this predictor on the test set.\n\nIt’s possible to normalize the confusion matrix in order to compute some quantities of frequent interest, like the true positive rate, the false positive rate, the true negative rate, and the false negative rate.\nThe true positive rate is the proportion of the time that the classifier correctly categorized a positive instance, out of all positive instances.\n\\[\n\\text{TPR} = \\frac{\\text{\\#TP}}{\\text{\\#TP} + \\text{\\#FN}}\n\\]\nThe false positive rate is the fraction of the time that the classifier incorrectly predicted a positive instance, out of all negative instances. \\[\n\\text{FPR} = \\frac{\\text{\\#FP}}{\\text{\\#FP} + \\text{\\#TN}}\n\\]\nThe true negative rate and false negative right are defined similarly. Normalizing the confusion matrix allows us to read off these rates:\nWe observe that not only does our model make mistakes, it makes different kinds of mistakes. Not only that – it makes different kinds of mistakes on different groups! For example, let’s compare the model’s confusion matrices on test data for female and male passengers:\n\nix = X_test[\"Sex_male\"] == 0\nprint(\"Female passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nix = X_test[\"Sex_male\"] == 1\nprint(\"Male passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nA few observations: on the test set…\n\n…when a female passenger survives, the model always correctly predicts this.\n…when a female passenger perishes, however, the model is actually still more likely to incorrectly predict that she did survive.\n…when a male passenger survives, the model almost always (87% of the time) instead incorrectly predicts that he perished.\n…when a male passenger perishes, the model always correctly predicts this.\n\nWe’ll go into much more detail on these rates in an upcoming lecture.\n\n\n\n\n\n\nDiscussion\n\n\n\nYou have gotten into the lucrative business of selling luxury cruise life insurance for passengers on ships like the Titanic. Here’s how your insurance works: if a passenger who has bought your insurance perishes, then you will make a payout of $100,000 to their named beneficiary.\nPer usual practices in insurance pricing, you plan to charge different prices for different passengers. You plan to set the price using the machine learning model we just trained on the Titanic data set. You will give passenger information to the model, and then base the price on the model’s prediction\n\nIf a passenger is predicted to survive, the price is $500.\nIf a passenger is predicted to perish, the price is $5,000.\n\nPlease discuss the following questions:\nAs we saw above, the model is more likely to incorrectly predict male passengers to perish, and more likely to incorrectly predict female passengers to survive. As a result, the insurance prices for men are significantly higher. Does your insurance scheme have an anti-male bias? Please consider both of the following points of view: - No, there is no bias because the model is simply doing its best to replicate the patterns found in the data. It’s not the model’s fault that men tended not to survive the crash! - Yes, there is bias because the model is creating higher prices for men on the basis of information that includes their sex.\nSuppose now that we train and evaluate a version of the model that doesn’t include passenger sex at all:\n\n# versions of the training and test data with sex column removed\nX_train_ = X_train.drop(\"Sex_male\", axis = 1)\nX_test_   = X_test.drop(\"Sex_male\", axis = 1)\n\n# fit the model\nplr.fit(X_train_, y_train)\n\n# extract a prediction\ny_pred = plr.predict(X_test_)\n\n# print confusion matrices\nix = X_test[\"Sex_male\"] == 0\nprint(\"Female passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nix = X_test[\"Sex_male\"] == 1\nprint(\"Male passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nThis looks more even, but now we have a different discrepancy: male passengers are more likely than female passengers to be incorrectly predicted to survive, so now the male insurance prices are lower than the female ones. Discuss the following two propositions:\n\nThis version of the model cannot have gender bias because gender was not a feature on which the model was trained.\nThis version of the model still has gender bias because it leads to higher insurance prices for men."
  },
  {
    "objectID": "lecture-notes/convex-linear-models-demo.html",
    "href": "lecture-notes/convex-linear-models-demo.html",
    "title": "",
    "section": "",
    "text": "from matplotlib import pyplot as plt\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom sklearn.datasets import make_blobs\nplt.rcParams[\"figure.figsize\"] = (4, 3)\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}\n\n\n\nz = np.linspace(0, 5, 101)\nplt.plot(z, -np.log(1/(1 + np.exp(-z)))) \nlabs = plt.gca().set(xlabel = r\"$\\hat{y}$\", ylabel = r\"$-\\log \\sigma(\\hat{y})$\")\n\n\n\n\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nNote that this data is not linearly separable. The perceptron algorithm wouldn’t even have converged for this data set, but logistic regression will do great.\n\n\n\n\n\n# add a constant feature to the feature matrix\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nNow we’ll define some functions to compute the empirical risk:\n\n# implement: (WE ARE CODING TOGETHER HERE!!)\n# - predict\n# - sigmoid\n# - logistic_loss\n# - empirical_risk\n\ndef predict(X, w):\n    return X@w\n\ndef logistic_loss(y_hat, y):\n    return -y*np.log(sigmoid(y_hat)) - (1-y)*np.log(1 - sigmoid(y_hat))\n\ndef sigmoid(y_hat): \n    return 1 / (1 + np.exp(-y_hat))\n\ndef empirical_risk(X, y, w, loss):\n    y_hat = predict(X, w)\n    return loss(y_hat, y).mean()\n\nFinally, we can write the function that will solve the empirical risk minimization problem for us. We’re going to use the scipy.optimize.minimize function, which is a built-in function for solving minimization problems. Soon, we’ll study how to solve minimization problems from scratch.\nThe scipy.optimize.minimize function requires us to pass it a single function that accepts a vector of parameters, plus an initial guess for the parameters.\n\ndef find_pars(X, y):\n    \n    p = X.shape[1]\n    w0 = np.random.rand(p) # random initial guess\n    \n    # perform the minimization\n    result = minimize(lambda w: empirical_risk(X, y, w, logistic_loss), \n                      x0 = w0) \n    \n    # return the parameters\n    return result.x\n\nOk, let’s try it and take a look at the parameters we obtained. Because the final column of X_ is the constant column of 1s, the final entry of w is interpretable as the intercept term b.\n\nw = find_pars(X_, y)\nw\n\narray([ 2.13023501,  1.71745945, -0.15021883])\n\n\nAnd, finally, we can plot the linear classifier that we learned.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\n\n\n\nSince the logistic loss is convex, we are guaranteed that this solution is the unique best solution (as measured by the logistic loss). There is no other possible set of parameters that would lead to a better result (again, as measured by the logistic loss).\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/gradient-descent.html",
    "href": "lecture-notes/gradient-descent.html",
    "title": "Optimization with Gradient Descent",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/gradient-descent.html#gradients",
    "href": "lecture-notes/gradient-descent.html#gradients",
    "title": "Optimization with Gradient Descent",
    "section": "Gradients",
    "text": "Gradients\nWe’re not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.\n\n\n\n\n\n\n\nDefinition 1 (Gradient of a Multivariate Function) Let \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be a multivariate differentiable function. The gradient of \\(f\\) evaluated at point \\(\\mathbf{z}\\in \\mathbb{R}^p\\) is written \\(\\nabla f(\\mathbf{z})\\), and has value\n\\[\n\\nabla f(\\mathbf{z}) \\triangleq\n\\left(\\begin{matrix}\n    \\frac{\\partial f(\\mathbf{z})}{\\partial z_1} \\\\\n    \\frac{\\partial f(\\mathbf{z})}{\\partial z_2} \\\\\n    \\cdots \\\\\n    \\frac{\\partial f(\\mathbf{z})}{\\partial z_p} \\\\\n\\end{matrix}\\right) \\in \\mathbb{R}^p\\;.\n\\]\nHere, \\(\\frac{\\partial f(\\mathbf{z})}{\\partial z_1}\\) is the partial derivative of \\(f\\) with respect to \\(z_1\\), evaluated at \\(\\mathbf{z}\\). To compute it:\n\nTake the derivative of \\(f\\) *with respect to variable \\(z_1\\), holding all other variables constant, and then evaluate the result at \\(\\mathbf{z}\\).\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nLet \\(p = 3\\). Let \\(f(\\mathbf{z}) = z_2\\sin z_1 + z_1e^{2z_3}\\). The partial derivatives we need are\n\\[\n\\begin{align}\n\\frac{\\partial f(\\mathbf{z})}{\\partial z_1} &= z_2 \\cos z_1 + e^{2z_3}\\\\\n\\frac{\\partial f(\\mathbf{z})}{\\partial z_2} &= \\sin z_1\\\\\n\\frac{\\partial f(\\mathbf{z})}{\\partial z_3} &= 2z_1 e^{2z_3}\\;.\n\\end{align}\n\\]\nSo, the gradient of \\(f\\) evaluated at a point \\(\\mathbf{z}\\) is\n\\[\n\\nabla f(\\mathbf{z}) =\n\\left(\\begin{matrix}\n    \\frac{\\partial f(\\mathbf{z})}{\\partial z_1} \\\\\n    \\frac{\\partial f(\\mathbf{z})}{\\partial z_2} \\\\\n    \\frac{\\partial f(\\mathbf{z})}{\\partial z_3} \\\\\n\\end{matrix}\\right) =\n\\left(\\begin{matrix}\n    z_2 \\cos z_1 + e^{2z_3}\\\\\n    \\sin z_1\\\\\n    2z_1 e^{2z_3}\n\\end{matrix}\\right)\n\\]\n\n\nSo, a gradient \\(\\nabla f(\\mathbf{z})\\) is a vector of the same dimension as \\(\\mathbf{z}\\). What happens if we combine them? This is where we get to the really important aspects of gradients for practical purposes:\n\n\n\n\n\n\n\nTheorem 1 (Local Minima Have \\(\\nabla f(\\mathbf{z}_0) = \\mathbf{0}\\)) Let \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be differentiable. If \\(\\mathbf{z}_0\\) is a local minimum of \\(f\\), then \\(\\nabla f(\\mathbf{z}_0) = \\mathbf{0}\\).\n\n\nTheorem 2 (\\(-\\nabla f\\) Is A Descent Direction) Let \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) be differentiable. Then, for any point \\(\\mathbf{z}\\in \\mathbb{R}^p\\), if \\(\\nabla f(\\mathbf{z}) \\neq \\mathbf{0}\\), then there exists a scalar \\(\\alpha\\) such that, if \\(\\mathbf{z}' = \\mathbf{z}- \\alpha \\nabla f(\\mathbf{z})\\), then \\(f(\\mathbf{z}') \\leq f(\\mathbf{z})\\).\n\n\n\n\nTheorem 1 and Theorem 2 are important because they give us the mechanics of how to solve the empirical risk minimization problem (Equation 1). Here’s our first version:\n\n\n\n\n\n\nAlgorithm: (Batch) Gradient Descent\n\n\n\nInputs: Function \\(f\\), initial starting point \\(\\mathbf{z}^{(0)}\\), learning rate \\(\\alpha\\).\nUntil convergence, in each iteration \\(t\\),\n\nCompute \\(\\mathbf{z}^{(t+1)} \\gets \\mathbf{z}^{(t)} - \\alpha \\nabla f(\\mathbf{z}^{(t)})\\).\n\nReturn the final value of \\(\\mathbf{z}^{(t)}\\).\n\n\nConvergence for gradient descent can be decided in a few different ways. One approach is to declare convergence when \\(\\nabla f(\\mathbf{z}^{(t)})\\) is close to 0. Another way is to declare convergence when the improvement in the function \\(f\\) is small enough in magnitude.\n\n\nSample code:\nif np.allclose(grad, np.zeros(len(grad))):\n    print(\"converged\")\n\n# or\n\nif f(w_new) - f(w_prev) &lt; 1e-6:\n    print(\"converged\")\nThe following theorem says that gradient descent works if the learning rate is small enough:\n\n\n\n\n\n\n\nTheorem 3 Suppose that \\(f\\) is strictly convex, is differentiable, and has a global minimizer \\(\\mathbf{z}^*\\). Then, there exists some \\(\\alpha &gt; 0\\) such that gradient descent applied to \\(f\\) produces a sequence of points \\(\\mathbf{z}^{(0)}, \\mathbf{z}^{(1)},\\ldots, \\mathbf{z}^{(t)}\\) that converges to \\(\\mathbf{z}^*\\)."
  },
  {
    "objectID": "lecture-notes/gradient-descent.html#gradient-descent-for-empirical-risk-minimization",
    "href": "lecture-notes/gradient-descent.html#gradient-descent-for-empirical-risk-minimization",
    "title": "Optimization with Gradient Descent",
    "section": "Gradient Descent For Empirical Risk Minimization",
    "text": "Gradient Descent For Empirical Risk Minimization\nSuppose that we have a per-observation loss function \\(\\ell\\) that is strictly convex and differentiable. Suppose that we are still dealing with a linear predictor of the form in Equation 2. Then, we know that the empirical risk objective function \\(L\\) is also strictly convex and differentiable. It follows from Theorem 3 that, if there is a minimizer \\(\\mathbf{w}^*\\) for the empirical risk, then we can find it using gradient descent. To do this, we need to be able to calculate the gradient of the loss function \\(L\\). Here’s how this looks. Keep in mind that we are differentiating with respect to \\(\\mathbf{w}\\).\n\\[\n\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\nabla \\left(\\frac{1}{n} \\sum_{i = 1}^n \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i)\\right) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n \\nabla \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i) \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}} \\nabla f_{\\mathbf{w}}(\\mathbf{x}_i) \\tag{multivariate chain rule} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}}  \\mathbf{x}_i \\tag{gradient of a linear function} \\\\\n              &= \\frac{1}{n} \\sum_{i = 1}^n  \\frac{d\\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)}{d\\hat{y}} \\mathbf{x}_i \\tag{$\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$} \\\\\n\\end{align}\n\\]\nThe good news here is that for linear models, we don’t actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form \\(\\frac{d\\ell(\\hat{y}_i, y_i)}{d\\hat{y}}\\) and then plug in \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\).\nLet’s do an example with the logistic loss:\n\\[\\ell(\\hat{y}, y) = -y \\log \\sigma(\\hat{y}) - (1-y)\\log (1-\\sigma(\\hat{y}))\\;.\\]\nA useful fact to know about the logistic sigmoid function \\(\\sigma\\) is that \\(\\frac{d\\sigma(\\hat{y}) }{d\\hat{y}} = \\sigma(\\hat{y}) (1 - \\sigma(\\hat{y}))\\). So, using that and the chain rule, the derivative we need is\n\\[\n\\begin{align}\n\\frac{d\\ell(\\hat{y}, y)}{d\\hat{y}} &= -y \\frac{1}{\\sigma(\\hat{y})}\\frac{d\\sigma(\\hat{y}) }{d\\hat{y}} - (1-y)\\frac{1}{1-\\sigma(\\hat{y})}\\left(- \\frac{d\\sigma(\\hat{y}) }{d\\hat{y}}\\right) \\\\\n&= -y \\frac{1}{\\sigma(\\hat{y})}\\sigma(\\hat{y}) (1 - \\sigma(\\hat{y})) - (1-y)\\frac{1}{1-\\sigma(\\hat{y})}\\left(- \\sigma(\\hat{y}) (1 - \\sigma(\\hat{y}))\\right) \\\\\n&= -y (1 - \\sigma(\\hat{y})) + (1-y)\\sigma(\\hat{y}) \\\\\n&= \\sigma(\\hat{y}) - y\\;.\n\\end{align}\n\\] Finally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression:\n\\[\n\\begin{align}\n\\nabla L(\\mathbf{w}) &= \\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\hat{y}_i) - y_i)\\mathbf{x}_i \\\\\n              &=\\frac{1}{n} \\sum_{i = 1}^n (\\sigma(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle) - y_i)\\mathbf{x}_i\\;.\n\\end{align}\n\\]\nSo, we can do logistic regression by choosing a learning rate and iterating the update \\(\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - \\alpha \\nabla L(\\mathbf{w}^{(t)})\\) until convergence.\nLet’s see this in action. Here’s a data set:\n\n\nCode\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nNote that this data is not linearly separable. The perceptron algorithm wouldn’t even have converged for this data set, but logistic regression will do great.\n\n\n\n\nThe code below is very similar to the code from last time, but I’m going to first transform the feature matrix \\(\\mathbf{X}\\) so that it contains a column of constant features. This is going to make our mathematical life a lot easier.\n\nimport numpy as np \nfrom scipy.optimize import minimize\nnp.seterr(all='ignore') \n\n# add a constant feature to the feature matrix\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef predict(X, w):\n    return X@w\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef logistic_loss(y_hat, y): \n    return -y*np.log(sigmoid(y_hat)) - (1-y)*np.log(1-sigmoid(y_hat))\n\ndef empirical_risk(X, y, loss, w):\n    y_hat = predict(X, w)\n    return loss(y_hat, y).mean()\n\nI’ll start by picking a random parameter vector, visualizing the corresponding line, and computing the loss:\n\nnp.random.seed(123)\n\n# pick a random weight vector and calculate the loss\nw = .5 - np.random.rand(p_features)\nloss = empirical_risk(X_, y, logistic_loss, w)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\ntitle = plt.gca().set_title(f\"Loss = {loss}\")\n\n\n\n\nIt can be hard to put these kinds of numbers in context, but this is a pretty poor value of the loss. You could probably guess that considering how bad the classifier line looks.\nNow let’s go ahead and use gradient descent to compute a better value of the parameter vector \\(\\tilde{\\mathbf{w}}\\). I’ve shown you the main loop of the algorithm, but not the calculation of the gradient. It’ll be up to you to implement the gradient (as well as variations of gradient descent) in an upcoming blog post.\n\nfrom hidden.logistic import gradient\n\nalpha = .001 # learning rate\n\ndone = False       # initialize for while loop\nprev_loss = np.inf # handy way to start off the loss\n\nhistory = []\n\n# main loop\nwhile not done: \n    w -= alpha*gradient(w, X_, y)                      # gradient step\n    new_loss = empirical_risk(X_, y, logistic_loss, w) # compute loss\n    \n    history.append(new_loss)\n    # check if loss hasn't changed and terminate if so\n    if np.isclose(new_loss, prev_loss):          \n        done = True\n    else:\n        prev_loss = new_loss\n\nNow we can visualize the resulting classifier and check the value of the loss that it achieves.\n\n\nCode\nloss = empirical_risk(X_, y, logistic_loss, w)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:,0], X[:,1], c = y)\naxarr[0].set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"Loss = {loss}\")\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\naxarr[1].plot(history)\naxarr[1].set(xlabel = \"Iteration number\", ylabel = \"Empirical Risk\")\nplt.tight_layout()\n\n\n\n\n\nThat looks better! Note that the data is not linearly separable, but our algorithm still converged to a reasonable solution."
  },
  {
    "objectID": "lecture-notes/gradient-descent.html#activity",
    "href": "lecture-notes/gradient-descent.html#activity",
    "title": "Optimization with Gradient Descent",
    "section": "Activity",
    "text": "Activity\nConsider the function \\(f(w_0, w_1) = \\sin(w_0w_1)\\). You can define this function like this:\n\nimport numpy as np\ndef f(w):\n    return np.sin(w[0]*w[1])\n\nMathematically, the gradient of this function is\n\\[\\nabla f(w_0, w_1) = (w_1\\cos w_0w_1, w_0 \\cos w_0w_1)^T.\\]\n\nImplement a simple loop that uses gradient descent to find a minimum of this function.\n\nYou’ll have to choose the learning rate \\(\\alpha\\).\nThe np.cos() function will be useful for programming the gradient.\nIt’s not the fastest approach, but if you’re not show how to program the gradient you can always first implement it as a list of two floats, and then use np.array(my_list) to convert it into a numpy array.\nYou’ll also need to pick a random starting guess.\n\nFind two initial guesses for the parameter vector \\(\\mathbf{w}\\) such that you get two different final minimizers (this is possible because \\(f\\) is not convex)."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html",
    "href": "lecture-notes/introducing-dimensionality-reduction.html",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#dimensionality-reduction",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#dimensionality-reduction",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nOn the other hand, dimensionality reduction is the task of identifying similar or related features (columns of \\(\\mathbf{X}\\)). This often allows us to identify patterns in the data that we wouldn’t be able to spot without algorithmic help. Dimensionality reduction is our topic for this lecture, and we’ll discuss clustering in the next one."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#approximation-the-frobenius-norm",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#approximation-the-frobenius-norm",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Approximation: the Frobenius Norm",
    "text": "Approximation: the Frobenius Norm\nIn PCA, \\(\\mathbf{X}\\approx \\mathbf{U}\\mathbf{W}\\) means that \\(\\lVert \\mathbf{X}- \\mathbf{U}\\mathbf{W} \\rVert_F\\) is small, where \\(\\lVert \\cdot \\rVert_F\\) is the Frobenius norm. The Frobenius norm of a matrix is just the square root of the sum of its squared entries: \\[\n\\lVert \\mathbf{A} \\rVert_F = \\sqrt{\\sum_{i = 1}^n \\sum_{j = 1}^n a_{ij}^2}\\;.\n\\]"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#choice-of-k",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#choice-of-k",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Choice of \\(k\\)",
    "text": "Choice of \\(k\\)\nFor PCA, we leave the choice of \\(k\\) to the user. There are heuristic ways for choosing \\(k\\), but we won’t discuss them in this course."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#constraints-on-mathbfu-and-mathbfw",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#constraints-on-mathbfu-and-mathbfw",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Constraints on \\(\\mathbf{U}\\) and \\(\\mathbf{W}\\)",
    "text": "Constraints on \\(\\mathbf{U}\\) and \\(\\mathbf{W}\\)\nIn PCA, we place a certain special structure on the factorization matrices \\(\\mathbf{U}\\) and \\(\\mathbf{W}\\) that help us to interpret them:\n\n\\(\\mathbf{W}\\in \\mathbb{R}^{k \\times p}\\)\n\\(\\mathbf{U}= \\mathbf{X}\\mathbf{W}^T\\). Note that this ensures that \\(\\mathbf{U}\\in \\mathbb{R}^{n\\times k}\\).\n\\(\\mathbf{W}\\mathbf{W}^T = \\mathbf{I}\\).\n\nThese assumptions can be derived statistically from the problem of finding the subspace (spanned by \\(\\mathbf{W}\\)) that maximizes the variance of the data when projected onto that subspace."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#the-pca-model",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#the-pca-model",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "The PCA Model",
    "text": "The PCA Model\nWith these choices, the PCA optimization problem becomes:\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{W}} =& \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{W}\\in \\mathbb{R}^{n\\times k}} \\lVert \\mathbf{X}- \\mathbf{X}\\mathbf{W}^T\\mathbf{W} \\rVert_{F} \\\\\n          &\\text{Such that }  \\mathbf{W}\\mathbf{W}^T = \\mathbf{I}\n\\end{aligned}\n\\]\nNow, we could aim to solve this problem with gradient descent (in the entries of \\(\\mathbf{W}\\)) or some similar approach. This, however, is more complicated than needed. As it turns out, some nice theory from linear algebra gives us a formula for finding \\(\\mathbf{W}\\) in terms of the eigenvalues of the matrix \\(\\mathbf{X}^T\\mathbf{X}\\). In particular: Take a moment to convince yourself that \\(\\hat{\\mathbf{W}}\\) has dimensions \\(k \\times p\\) as required.\n\\[\n\\hat{\\mathbf{W}} = \\left[\\begin{matrix}\n              - & \\mathbf{v}_1 & - \\\\\n              - & \\mathbf{v}_2 & - \\\\\n              \\vdots & \\vdots & \\vdots \\\\\n              - & \\mathbf{v}_k & - \\\\\n\\end{matrix}\\right]\n\\]\nwhere \\(\\mathbf{v}_i\\) is the \\(i\\)th eigenvector of \\(\\mathbf{X}^T\\mathbf{X}\\)."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#implementing-pca",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#implementing-pca",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Implementing PCA",
    "text": "Implementing PCA\nLet’s go ahead and implement PCA. We’ll do this using a term-document matrix that I derived from Lewis Caroll’s famous book Alice’s Adventures in Wonderland. Each row of the data corresponds to a chapter of the book, and each column corresponds to a word that may appear in that chapter:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport nltk\nfrom nltk.corpus import gutenberg\n# nltk.download('gutenberg') # need to run once\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ns = gutenberg.raw(\"carroll-alice.txt\")\nchapters = s.split(\"CHAPTER\")[1:]\ndf = pd.DataFrame({\n    \"chapter\" : range(1, len(chapters) + 1),\n    \"text\" : chapters\n})\ndf\n\nvec = CountVectorizer(max_df = 0.5, min_df = 0, stop_words = \"english\")\n\ncounts = vec.fit_transform(df['text'])\ncounts = counts.toarray()\nM = pd.DataFrame(counts, columns = vec.get_feature_names_out())\n\n\nNow we’re ready to implement PCA. Because there is an explicit solution in terms of the eigenvalues of a matrix, our implementation can be very short. This is a very slow implementation of PCA; faster methods involve numerical methods for computing singular values.\n\ndef pca(M, k):\n    X = np.array(M)\n    ev = np.linalg.eigh(X.T@X) # special eigensolver for symmetric matrices\n    W = ev[1][:,-k:].T         # matrix of top k eigenvectors\n    return X@W.T, W            # U and W\n\nHere’s PCA in action:\n\nk = 4\nU, W = pca(M, k)"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#interpreting-pca",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#interpreting-pca",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Interpreting PCA",
    "text": "Interpreting PCA\nOk, so why did we do this? Both the matrices \\(\\mathbf{U}\\) and \\(\\mathbf{W}\\) can be used to give us information about the structure of the text. In the context of text analysis, we usually interpret \\(\\mathbf{U}\\) and \\(\\mathbf{W}\\) as expressing information about some latent topics. The \\(j\\) th column of \\(\\mathbf{W}\\) gives us a weight for each term in the text; this tells us how present each term is in the topic. The following function can be used to inspect the top words in each topic:\n\ndef top_words_pca(M, W, component, num_words):\n    orders = np.argsort(np.abs(W), axis = 1)\n    important_words = np.array(M.columns)[orders]\n    return important_words[component][-num_words:]\n\nfor i in range(k):\n  print(f\"topic {i}: {top_words_pca(M, W, i, 10)}\")\n\ntopic 0: ['cats' 'jury' 'court' 'baby' 'mad' 'footman' 'caterpillar' 'mouse' 'cat'\n 'king']\ntopic 1: ['gryphon' 'soldiers' 'tea' 'hare' 'march' 'cat' 'dormouse' 'king'\n 'hatter' 'queen']\ntopic 2: ['tea' 'queen' 'hare' 'march' 'gryphon' 'king' 'mock' 'turtle' 'dormouse'\n 'hatter']\ntopic 3: ['march' 'course' 'soup' 'dormouse' 'hatter' 'king' 'queen' 'gryphon'\n 'mock' 'turtle']\n\n\nOn the other hand, the matrix \\(\\mathbf{U}= \\mathbf{X}\\mathbf{W}^T\\) tells us about how present each of the \\(k\\) topics are in each of the documents (in this case, chapters of the book). We can visualize the topic weight in each chapter over time:\n\nfig, ax = plt.subplots(1)\n\nfor i in range(k):\n    ax.plot(np.arange(1, M.shape[0]+1), U[:,i], label = top_words_pca(M, W, i, 8))\n\nax.set(xlabel = \"chapter\", ylabel = \"topic weight\")\n\nax.legend(bbox_to_anchor=(1, -.15))\n\n&lt;matplotlib.legend.Legend at 0x7fbeeafa1f10&gt;"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction.html#limitations-of-pca",
    "href": "lecture-notes/introducing-dimensionality-reduction.html#limitations-of-pca",
    "title": "Dimensionality Reduction and Topic Modeling",
    "section": "Limitations of PCA",
    "text": "Limitations of PCA\nPCA is a fantastically powerful algorithm in a wide variety of settings, but it does have some important limitations. In this case, it can be difficult for us to interpret the results of PCA because some of the topics can be “negatively involved” in a chapter. That is, the words in that topic are “especially absent” from the chapter. This is confusing at best. So, we have a mismatch between what this specific matrix factorization method does and our interpretability needs. This is a very common situation, and is one of the most frequent motivators of new methods. For more on PCA, including some settings in which it is more effective, see this section of the Python Data Science Handbook by Jake VanderPlas."
  },
  {
    "objectID": "lecture-notes/features-regularization-live.html",
    "href": "lecture-notes/features-regularization-live.html",
    "title": "Quick Recap",
    "section": "",
    "text": "$$\n$$\nLast time, we considered the problem of empirical risk minimization with a convex loss function. We assumed that we had data, a pair \\((\\mathbf{X}, \\mathbf{y})\\) where\nUsing this data, we defined the empirical risk minimization problem, which had the general form \\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w})\\;,\n\\tag{1}\\] where \\[\nL(\\mathbf{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(f_{\\mathbf{w}}(\\mathbf{x}_i), y_i)\\;.\n\\]\nHere, \\(f_{\\mathbf{w}}:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) is our predictor function, which takes in a feature vector \\(\\mathbf{x}_i\\) and spits out a prediction \\(\\hat{y}_i\\). We are still assuming that \\(f_{\\mathbf{w}}\\) is linear and therefore has the form\nIn our last lecture, we studied how to compute the gradient of \\(L(\\mathbf{w})\\) in minimize the convex loss and find a good value \\(\\hat{\\mathbf{w}}\\) for the parameter vector. In this lecture we’re going to assume that we can cheerfully solve the empirical risk minimization for convex linear models. This time, we’re going to see how we can use the framework of convex linear models to try to get around one of the main limitations we’ve seen in class so far: our models only work with linear decision boundaries. Most of the data we care about has nonlinear decision boundaries. Here’s a dramatic example. For this example, I’m using the implementation of logistic regression from scikit-learn. I’m also using the plot_decision_regions function from the mlxtend package, which is a handy plotting utility for visualizing the behavior of our models.\nfrom sklearn.datasets import make_moons, make_circles\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all=\"ignore\")\n\nX, y = make_circles(200, shuffle = True, noise = 0.1, factor = 0.5)\n\nLR = LogisticRegression()\nLR.fit(X, y)\nplot_decision_regions(X, y, clf = LR)\nscore = plt.gca().set_title(f\"Accuracy = {LR.score(X, y)}\")\nYikes! Our accuracy isn’t much better than 50%.\nVisually this should be pretty easy data to classify. But the linear decision boundary isn’t the way.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/features-regularization-live.html#feature-maps",
    "href": "lecture-notes/features-regularization-live.html#feature-maps",
    "title": "Quick Recap",
    "section": "Feature Maps",
    "text": "Feature Maps\nSuppose that we were able to extract from each point its distance from the origin. In 2d, we could take a point \\(\\mathbf{x}\\) and simply compute\n\\[\nr^2 = x_1^2 + x_2^2\\;.\n\\]\nWe could then make the classification based on the value of \\(r^2\\). In this data set, it looks like the classification rule that predicts \\(1\\) if \\(r^2 &lt; 1\\) and \\(0\\) otherwise would be a pretty good one. The important insight here is that this is also a linear model, with linear predictor function\n\\[\n\\hat{y} = \\langle \\mathbf{r}, \\mathbf{w} \\rangle\\;,\n\\]\nand predicted labels \\(\\mathbb{1}[\\hat{y} &lt; 0]\\).\nwhere \\(\\mathbf{r}= (r^2, 1)\\) and \\(\\mathbf{w}= (1, -1)\\). This means that we can use empirical risk minimization for this problem if we just transform the features \\(\\mathbf{X}\\) first! We need to compute a matrix \\(\\mathbf{R}\\) whose \\(i\\)th row is \\(\\mathbf{r}_i = (r^2_i, 1) = (x_{i1}^2 + x_{i2}^2, 1)\\), and then use this matrix in place of \\(\\mathbf{X}\\) for our classification task.\nThe transformation \\((x_1, x_2) \\mapsto (x_1^2 + x_2^2, 1)\\) is an example of a feature map.\n\n\n\n\n\n\n\nDefinition 1 A feature map \\(\\phi\\) is a function \\(\\phi:D \\rightarrow \\mathbb{R}^p\\), where \\(D\\) is the set of possible data values. If \\(d\\in D\\) is a data point, we call \\(\\phi(d) = \\mathbf{x}\\in \\mathbb{R}^p\\) the feature vector corresponding to \\(d\\). For a given feature map \\(\\phi\\), we define the map \\(\\Phi:D^n \\rightarrow \\mathbb{R}^{n\\times p}\\) as\n\\[\n\\Phi(\\mathbf{d}) = \\left(\\begin{matrix}\n     - & \\phi(d_1) & - \\\\\n     - & \\phi(d_2) & - \\\\\n     \\vdots & \\vdots & \\vdots \\\\\n     - & \\phi(d_n) & - \\\\\n\\end{matrix}\\right)\n\\]\nWe’ll often write\n\\[\n\\mathbf{X}= \\Phi(\\mathbf{d})\n\\]\nto say that \\(\\mathbf{X}\\) is the feature matrix for a data set \\(\\mathbf{d}\\).\n\n\n\n\nWe can think of feature maps in two ways:\nFeature maps can represent measurement processes. For example, maybe I am trying to classify penguins by species, based on physiological measurements. The real data is the penguin, and the measurements are how I represent that penguin with numbers. In this case, I might write my feature map as \\[\\phi(🐧) = (\\mathrm{height}, \\mathrm{weight}, \\text{bill length})\\] Here, \\(D\\) is a set of many penguins \\(D = \\{🐧_1, 🐧_2, 🐧_3, 🐧_4, 🐧_5, 🐧_6, 🐧_7\\}\\), and \\(d\\in D\\) is a specific penguin. The process of transforming an object into a vector via a feature map is often called vectorization as well, especially in the context of representing digital data as vectors. We often talk about vectorizing text and images for example; this can be done using feature maps.\nFeature maps can also represent data processing, which is more like our example above. There, we’re taking some data that’s already a vector and turning it into a DIFFERENT vector that we think will be helpful for our learning task."
  },
  {
    "objectID": "lecture-notes/features-regularization-live.html#feature-maps-and-linear-separability",
    "href": "lecture-notes/features-regularization-live.html#feature-maps-and-linear-separability",
    "title": "Quick Recap",
    "section": "Feature Maps and Linear Separability",
    "text": "Feature Maps and Linear Separability\nWe often think of feature maps as taking us from a space in which the data is not linearly separable to a space in which it is. For example, consider the feature map\n\\[\n(x_1, x_2) \\maps_to (x_1^2, x_2^2)\\;.\n\\]\nThis map is sufficient to express the radius information, since we can represent the radius as\n\\[\nr^2 = \\langle (1, 1), (x_1^2, x_2^2) \\rangle\\;.\n\\]\nLet’s see how this looks. We’ll again show the failed linear separator, and we’ll also show a successful separator in a transformed feature space:\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\nX_ = X**2\nLR2 = LogisticRegression()\nLR2.fit(X_, y)\nplot_decision_regions(X_, y, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_, y)}\")"
  },
  {
    "objectID": "lecture-notes/features-regularization-live.html#feature-maps-in-practice",
    "href": "lecture-notes/features-regularization-live.html#feature-maps-in-practice",
    "title": "Quick Recap",
    "section": "Feature Maps in Practice",
    "text": "Feature Maps in Practice\nGoing back to our example of trying to classify the two nested circles, we could just compute the radius. In practice, however, we don’t really know which features are going to be most useful, and so we just compute a set of features. In our case, the square of the radius is an example of a polynomial of degree 2: \\[\nr^2 = x_1^2 + x_2^2\\;.\n\\] So, instead of just assuming that the radius is definitely the right thing to compute, we more frequently just compute all the monomials of degree 2 or lower. If \\(\\mathbf{x}= (x_1, x_2)\\), then this is\n\\[\n\\phi(\\mathbf{x}_i) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)\\;.\n\\]\nWe then use a linear model to solve the empirical risk minimization problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{w} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i)\\;.\n\\]\nThe important point to keep track of is that the new feature matrix \\(\\mathbf{X}' = \\Phi(\\mathbf{X})\\) has more columns than \\(\\mathbf{X}\\). In this case, for example, \\(\\mathbf{X}\\) had just 2 columns but \\(\\Phi(\\mathbf{X})\\) has 6. This means that \\(\\hat{\\mathbf{w}}\\) has 6 components, instead of 2!\nLet’s now run logistic regression with degree-2 polynomial features on this data set. The most convenient way to make this happen in the scikit-learn framework is with at Pipeline. The Pipeline first applies the feature map and then calls the model during both fitting and evaluation. We’ll wrap the pipeline in a simple function for easy reuse.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(degree, **kwargs):\n    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n                    (\"LR\", LogisticRegression(**kwargs))])\n    return plr\n\ndef viz_plr(plr, X, y):  \n    plot_decision_regions(X, y, clf = plr)\n    score = plt.gca().set_title(f\"Accuracy = {plr.score(X, y)}\")  \n\n\nplr = poly_LR(degree = 2)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nLet’s check the coefficients of the model:\n\n#\n\nNotice that two coefficients are much larger than the others, and approximately equal. These are the coefficients for the features \\(x_1^2\\) and \\(x_2^2\\). The fact that these are approximately equal means that our model is very close to using the square radius \\(r^2 = x_1^2 + x_2^2\\) for this data, just like we’d expect. The benefit is that we didn’t have to hard-code that in; the model just detected on its own the right pattern to find.\nPart of the reason this might be beneficial is that for some data sets, we might not really know what specific features we should try. For example, here’s another one where a linear classifier doesn’t do so great (degree 1 corresponds to no transformation of the features).\n\nnp.random.seed(123)\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\nplr = poly_LR(degree = 5)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\n\nplr.named_steps[\"LR\"].coef_\n\narray([[-2.55579727e-05,  1.35298953e-01, -2.50514385e+00,\n        -2.60266667e+00, -8.30713930e-01, -6.30431140e-01,\n        -1.22029940e-01, -1.25686564e+00,  2.00495256e-01,\n        -9.53134307e-01, -8.86217141e-01, -5.66003771e-01,\n        -1.31942269e-01, -2.17573278e-01, -4.71614719e-01,\n         1.47754692e+00, -7.21316467e-01,  1.89326224e-01,\n        -3.17134507e-01,  1.37450850e-02, -5.02202198e-01]])\n\n\nIt’s not as obvious that we should use the radius or any other specific feature for our feature map. Fortunately we don’t need to think too much about it – we can just increase the degree and let the model figure things out:\n\n# \n# \n#\n\nMuch nicer!"
  },
  {
    "objectID": "lecture-notes/features-regularization-live.html#generalization-feature-selection-regularization",
    "href": "lecture-notes/features-regularization-live.html#generalization-feature-selection-regularization",
    "title": "Quick Recap",
    "section": "Generalization, Feature Selection, Regularization",
    "text": "Generalization, Feature Selection, Regularization\nSo, why don’t we just use as many features as it takes to get perfect accuracy on the training data? Here’s an example where we get perfect accuracy on the training data:\n\nplr = poly_LR(degree = 15, penalty = \"none\", max_iter = 1e6)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nI’ve had to change some parameters to the LogisticRegression in order to ensure that it fully ran the optimization procedure for this many polynomials.\nThe problem here is that, although this classifier might achieve perfect training accuracy, it doesn’t really look like it’s captured “the right” pattern. This means that if we ask it to classify similar new data, it’s unlikely to do as well:\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nviz_plr(plr, X, y)\n\n\n\n\nWhoops! We have overfit: our model was so flexible that it was able to learn both some real patterns that we wanted it to learn and some noise that we didn’t. As a result, when it made a prediction on new data, the model’s predictions were imperfect, reflecting the noise it learned in the training process.\nIn machine learning practice, we don’t actually want our models to get perfect scores on the training data – we want them to generalize to new instances of unseen data. Overfitting is one way in which a model can fail to generalize.\nLet’s do an experiment in which we see what happens to the model’s generalization ability when we increase the number of polynomial features:\n\nimport pandas as pd\nnp.random.seed()\n\ndegs = range(0, 11)\n\ndf = pd.DataFrame({\"deg\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .4)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .4)\n\n    for deg in degs:\n        plr = poly_LR(degree = deg, penalty = \"none\", max_iter = 1e3)\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"deg\" : [deg],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n        \nmeans = df.groupby(\"deg\").mean().reset_index()\n\nplt.plot(means[\"deg\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"deg\"], means[\"test\"], label = \"validation\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Degree of polynomial feature\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\n\n\n\nWe observe that there is an optimal number of features for which the model is most able to generalize: around 3 or so. More features than that is actually harmful to the model’s predictive performance.\nSo, one way to promote generalization is to try to find “the right” or “the right number” of features and use them for prediction. This problem is often called feature selection.\nAnother common approach to avoid overfitting is called regularization. In regularization, we actually modify the empirical risk objective function that is to be minimized. Instead of trying to minimize Equation 1, we instead consider the modified objective function \\[\nL'(\\mathbf{w}) = L(\\mathbf{w}) + \\lambda R(\\mathbf{w})\\;,\n\\] where \\(\\lambda\\) is a regularization strength and \\(R(\\mathbf{w})\\) is a regularization function that aims to influence the entries of \\(\\mathbf{w}\\) in some way. Common choices of regularization function include the Euclidean norm \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2^2\\) and the \\(\\ell_1\\) norm \\(R(\\mathbf{w}) = \\sum_{j = 1}^p \\lvert x_j \\rvert\\). To see regularization in action, let’s go back to our logistic regression model with a large number of polynomial features. We can see the presence of overfitting in the excessive “wiggliness” of the decision boundary.\n\nX, y = make_moons(200, shuffle = True, noise = 0.3)\n\nplr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", max_iter = 1e4, C = 10)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nFortunately for us, we can actually use regularization directly from inside the scikit-learn implementation of LogisticRegression. We specify the penalty (the \\(\\ell_1\\) regularization), the strength of the penalty (in the scikit-learn implementation, you specify \\(C = \\frac{1}{\\lambda}\\) so that larger \\(C\\) means less regularization) and the optimization solver (not all solvers work with all penalties).\n\n#\n#\n#\n\nThis looks more likely to generalize! We can also increase the regularization:\n\n#\n#\n#\n\nor decrease it:\n\n#\n#\n#\n\nLike last time, we can conduct a search (often called a grid-search) to find the best value of the regularization strength for a given problem. We’ll hold fixed the number of features, and instead vary the regularization strength:\n\nnp.random.seed()\n\nC = 10.0**np.arange(-4, 5)\n\ndf = pd.DataFrame({\"C\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .3)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .3)\n\n    for c in C:\n        plr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = c)\n\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"C\" : [c],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n     \nmeans = df.groupby(\"C\").mean().reset_index()\n\nplt.plot(means[\"C\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"C\"], means[\"test\"], label = \"validation\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"C\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\nUsing 15 features, it looks like a regularization strength of approximately \\(C = 10\\) is a good choice for this problem."
  },
  {
    "objectID": "lecture-notes/perceptron-demo.html",
    "href": "lecture-notes/perceptron-demo.html",
    "title": "Prep Data",
    "section": "",
    "text": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom hidden.perceptron import perceptron_update, draw_line\n\nnp.random.seed(123)\n\nplt.rcParams[\"figure.figsize\"] = (4, 4)\n\nX1 = np.random.normal(0, 1, 100)\nX2 = np.random.normal(0, 1, 100)\n\nX3 = np.random.normal(0, 1, 100)*.5+3\nX4 = np.random.normal(0, 1, 100)*.5+3\n\nfig, ax = plt.subplots(1, 1)\n\ndef plot_scatter(X1, X2, X3, X4, ax, legend = True):\n    \n    s = ax.scatter(X1, X2, color = \"#ED90A4\", alpha = 0.5, label = r\"$y_i = -1$\")\n    s = ax.scatter(X3, X4, color = \"#00C1B2\", alpha = 0.5, label = r\"$y_i = 1$\")\n    l = ax.set(xlabel = r\"$x_{i1}$\")\n    l = ax.set(ylabel = \"$x_{i2}$\")\n    if legend:\n        l = ax.legend()\n    \nplot_scatter(X1, X2, X3, X4, ax)\n\nX = np.append(np.column_stack((X1, X2)), np.column_stack((X3, X4)), axis = 0) # feature matrix\ny = 2*(np.arange(0, 200) &gt;= 100) - 1 # target vector\n\n\n\n\n\nX[:5,:], y[:5]\n\n(array([[-1.0856306 ,  0.64205469],\n        [ 0.99734545, -1.97788793],\n        [ 0.2829785 ,  0.71226464],\n        [-1.50629471,  2.59830393],\n        [-0.57860025, -0.02462598]]),\n array([-1, -1, -1, -1, -1]))\n\n\n\nPerceptron Algorithm\n\nw = np.random.rand(3)\nw\n\narray([8.18876137e-05, 9.80597342e-01, 8.82712985e-01])\n\n\n\n\nw = np.random.rand(3)\nloss = 1 # 1 - accuracy\n\nwhile loss != 0:\n    w, i, loss = perceptron_update(X, y, w)\n    print(f\"Current weight = {w}, current loss = {loss}\" )\n\nCurrent weight = [ 5.1229715  -7.09197354 -2.31015032], current loss = 0.69\nCurrent weight = [ 8.15736566 -4.12317281 -1.31015032], current loss = 0.23\nCurrent weight = [ 5.37497837 -6.40602099 -4.31015032], current loss = 0.65\nCurrent weight = [ 7.91732483 -3.03071603 -3.31015032], current loss = 0.19\nCurrent weight = [ 3.43039242 -7.16248848 -6.31015032], current loss = 0.6\nCurrent weight = [ 6.48804178 -3.13425481 -5.31015032], current loss = 0.175\nCurrent weight = [ 3.03742515 -3.6658318  -8.31015032], current loss = 0.525\nCurrent weight = [ -2.19371152   0.56983483 -11.31015032], current loss = 0.5\nCurrent weight = [  0.70911712   3.84045048 -10.31015032], current loss = 0.025\nCurrent weight = [ 2.74725925  5.9535648  -9.31015032], current loss = 0.035\nCurrent weight = [ -0.26490244   0.07533077 -12.31015032], current loss = 0.5\nCurrent weight = [  2.94518736   3.17879453 -11.31015032], current loss = 0.0\n\n\n\n\nPerceptron Algorithm (with visualization)\n\nnp.random.seed(123456)\nw = np.random.rand(3)\n\nX = np.append(np.column_stack((X1, X2)), np.column_stack((X3, X4)), axis = 0) # feature matrix\ny = 2*(np.arange(0, 200) &gt;= 100) - 1 # target vector\n\nplt.rcParams[\"figure.figsize\"] = (10, 5)\nfig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    plot_scatter(X1, X2, X3, X4, ax, legend = False)\n    draw_line(w, -10, 10, ax, color = \"black\", linestyle = \"dashed\")\n    w, i, loss = perceptron_update(X, y, w)    \n    ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\")\n    draw_line(w, -10, 10, ax, color = \"black\")\n    ax.set_title(f\"loss = {loss}\")\n    \nplt.tight_layout()\n\n\n\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/vectorization-live.html",
    "href": "lecture-notes/vectorization-live.html",
    "title": "Introduction",
    "section": "",
    "text": "So far in this course, we’ve considered the general supervised learning scenario, in which we are given a feature matrix \\(\\mX \\in \\R^{n\\times p}\\) and a target vector \\(\\vy \\in \\R^n\\). We then solve the empirical risk minimization problem in order to choose model parameters that minimize a loss function on the training data. The exact structure of this loss function depends on things like whether we are doing classification or regression, what our computational resources are, and other considerations.\nBut feature matrices \\(\\mX\\) and target vectors \\(\\vy\\) don’t just exist in the world: they are collected and measured. We can think of data collection and measurement as posing three fundamental questions:\nBroadly, we can think of the complete machine learning workflow as having phases corresponding to problem definition, data collection + measurement, modeling, and evaluation. Here’s roughly how this looks:\nflowchart TB\n\n    subgraph problem[problem definition]\n        need[identify need]--&gt;design_collection[design data collection]\n    end\n    subgraph measurement[data collection + measurement]\n        training[training data] \n        testing[testing data]\n    end\n    subgraph modeling\n        explore[explore data] --&gt; engineer[engineer features]\n        engineer --&gt; design[design model]\n    end\n    subgraph assessment\n        test --&gt; audit\n        audit --&gt; deploy\n        deploy--&gt;evaluate\n    end\n    design_collection--&gt;measurement\n    training --vectorization--&gt; modeling\n    design --&gt; assessment\n    testing --vectorization--&gt; assessment\n    need--&gt;assessment\nSo far, we’ve spent most of our time in the “modeling” module, especially the last two steps. We’ve also studied some of the ways to test and audit algorithms. Today we’re going to discuss vectorization. We can think of vectorization as what happens between the collection of raw data and the use of that data as input for models.\nThe reason that vectorization is necessary is that machine learning models only understand numbers. So, if our data isn’t numbers, we need to convert it into numbers in order to use it for modeling.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/vectorization-live.html#what-data-needs-vectorization",
    "href": "lecture-notes/vectorization-live.html#what-data-needs-vectorization",
    "title": "Introduction",
    "section": "What Data Needs Vectorization?",
    "text": "What Data Needs Vectorization?\nMost of it!\n\nIf your data comes to you as a table or matrix containing only numbers, in which each row corresponds to exactly one observation, then you may not need to vectorize.\nIf your data comes to you in any other form, then you need to vectorize.\n\nSome data that usually require vectorization:\n\nImages\nText\nAudio files\nMost genomic data\nEtc. etc.\n\nThere are tons of ways of vectorizing different kinds of data, and we’re not going to cover all of them. Instead, we’re going to go a little more in depth on text vectorization. We’ll discuss image vectorization much more when we get to convolutional neural networks. For your projects, depending on the data you want to work with, you may need to research vectorization schemes appropriate to your data."
  },
  {
    "objectID": "lecture-notes/vectorization-live.html#sketchy-labels",
    "href": "lecture-notes/vectorization-live.html#sketchy-labels",
    "title": "Introduction",
    "section": "Sketchy Labels",
    "text": "Sketchy Labels\nThese tweets were labeled manually by the original collector of the data. As with any setting in which humans need to make subjective decisions, there is considerable possibility for debate. For example, here is one tweet that was labeld “extremely positive”:\n\nprint(df_train.iloc[[40338]][\"OriginalTweet\"].iloc[0])\n\nWE NEED COVID-19 TESTING FOR EVERYONE TODAY!\nI have never been afraid to leave my house for a trip to the grocery store in my life. Now I am. I don't want to bring home a virus to my loved ones. It's not me, it's them.\n#StayHomeSaveLives\n\n\nChallenges that can cause sketchy labels include:\n\nSpeed of labeling (it takes a LONG time to make high-quality labels)\nLanguage familiarity\nAmbiguity in the target language\nLots more!\n\nAlmost always, when working with real-world data sets, we need to keep in mind that not only is our model approximate and our data incomplete, but the data may also be contaminated with errors that we aren’t really able to control. See @northcutt2021labelerrors for much more on label errors in common machine learning benchmarks."
  },
  {
    "objectID": "lecture-notes/vectorization-live.html#target-vectorization",
    "href": "lecture-notes/vectorization-live.html#target-vectorization",
    "title": "Introduction",
    "section": "Target Vectorization",
    "text": "Target Vectorization\nOur aim is to predict the Sentiment in terms of the text of the OriginalTweet. However, neither the text OriginalTweet nor the target Sentiment are numbers. So, we need to vectorize.\nThe possible values of the Sentiment column are\n\nimport numpy as np\nnp.unique(df_train[\"Sentiment\"])\n\narray(['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral',\n       'Positive'], dtype=object)\n\n\nVectorizing the target Sentiment is simple (although there are multiple ways). We’ll construct a new target vector which is 1 if the sentiment is Positive or Extremely Positive and 0 otherwise:\n\ntarget = 1*df_train[\"Sentiment\"].str.contains(\"Positive\")\ntarget.head()\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: Sentiment, dtype: int64\n\n\nVectorizing the predictor OriginalTweet is much more complicated, and here we face a number of choices.\n\nTerm Frequency (TF) Vectorization\nIn natural language processing (NLP), a data set of text is often called a corpus, and each observation is often called a document. Here, each document is a tweet.\nOne standard vectorization technique is to construct a term-document matrix. In a term-document matrix, each row corresponds to a document and each column corresponds to a “term” (usually a word) that is present in the document. The entry \\(x_{ij}\\) of this matrix is the number of terms that term \\(j\\) appears in document \\(i\\), which we’ll call \\(\\mathrm{tf}_{ij}\\). To construct a term-document matrix, we can use the CountVectorizer from sklearn.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_df = 0.2, min_df = 0.001, stop_words = \"english\")\n\nHere, max_df and min_df specify a range of frequencies to include. If a term is present in almost all documents (like “the” or “of”), then this term may not be a good indication of sentiment. On the other hand, if a term appears in only one or two documents, we probably don’t have enough data to figure out whether it matters. Finally, the choice of stop_words tells our vectorizer to ignore common English words that are unlikely to carry much emotional meaning, like “and” or “if”.\n\nf = cv.fit(df_train[\"OriginalTweet\"])\ncounts = cv.transform(df_train[\"OriginalTweet\"])\ntdm = pd.DataFrame(counts.toarray(), columns = cv.get_feature_names_out())\n\n\ntdm\n\n\n\n\n\n\n\n\n00\n000\n10\n100\n11\n12\n13\n14\n15\n16\n...\nyear\nyears\nyes\nyesterday\nyork\nyoung\nyoutube\nyouâ\nyâ\nzero\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41152\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41154\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41155\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41156\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n41157 rows × 2322 columns\n\n\n\nHere’s our term-document matrix. Note that most of the entries are 0 because tweets are so short!\nThe function below summarizes our entire data prep pipeline, which we’ll need for when we get to the test set.\n\ndef prep_tweets(df, vectorizer, train = True):\n    if train: \n        vectorizer.fit(df_train[\"OriginalTweet\"])\n    X = vectorizer.transform(df[\"OriginalTweet\"]) # term-document matrix\n    y = 1*df[\"Sentiment\"].str.contains(\"Positive\")\n\n    return X, y\n\n\nX_train_cv, y_train = prep_tweets(df_train, cv, train = True)\n\n\n\nFirst Model\nLet’s check on the base rate:\nSo, always guessing that a tweet is not positive would be correct 56% of the time. Let’s see if we can beat this using logistic regression.\n\nfrom sklearn.linear_model import LogisticRegression\nLR_cv = LogisticRegression()\nLR_cv.fit(X_train_cv, y_train)\nLR_cv.score(X_train_cv, y_train)\n\n/Users/philchodrow/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.8755011298199578\n\n\nThis model achieves 87% accuracy on the training data.\n\n\nInverse Document Frequency Weighting\nSimple term-document matrices are good for some tasks, but in other cases it is useful to downweight terms according to their frequency in the overall training corpus. This allows our models to place greater emphasis on rarer terms, which might be more expressive of strong emotions.\nIn term-frequency-inverse-document-frequency (TF-IDF) weighting, the entry for term \\(j\\) in document \\(i\\) is Exact details of TF-IDF weightings differ; this is the one implemented by default in sklearn.\n\\[\n\\tilde{\\mathrm{x}}_{ij} = \\overbrace{\\mathrm{tf}_{ij}}^{\\text{Term frequency}}\\times \\underbrace{\\mathrm{idf}_i}_{\\text{inverse document frequency}}\\;.\n\\]\nHere, the term frequency \\(\\mathrm{tf}_{ij}\\) is again the number of times that term \\(i\\) appears in document \\(j\\), while the inverse document frequency \\(\\mathrm{idf}_i\\) is computed with the formula\n\\[\n\\mathrm{idf}_i = \\log \\frac{1+n}{1+\\mathrm{df}_i} + 1\\;\n\\] with \\(\\mathrm{df}_i\\) being the total number of documents in which term \\(i\\) appears. Finally, each row of \\(\\tilde{\\mathrm{x}}_{ij}\\) is normalized to have unit length:\n\\[\nx_{ij} = \\frac{x_{ij}}{\\sqrt{\\sum_{j}x_{ij}^2}}\n\\]\nThese \\(x_{ij}\\) are then collected to form the feature matrix \\(\\mX\\). Let’s try constructing a model using TF-IDF vectorization:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidfv = TfidfVectorizer(max_df = 0.2, min_df = 0.001, stop_words = 'english')\nX_train_tfidf, y_train = prep_tweets(df_train, tfidfv, train = True)\n\n\nLR_tfidf = LogisticRegression()\nLR_tfidf.fit(X_train_tfidf, y_train)\nLR_tfidf.score(X_train_tfidf, y_train)\n\n0.8623077483781617\n\n\nOur TF-IDF model got a lower training score. At this stage, one good approach would be to choose which vectorization to use (as well as the vectorization parameters) using cross-validation. For now, we’ll just go ahead and grab the test set:\n\ndf_test = grab_tweets(data_set = \"test\")\nX_test_cv, y_test = prep_tweets(df_test, vectorizer = cv, train = False)\nX_test_tfidf, y_test = prep_tweets(df_test, vectorizer = tfidfv, train = False)\n\nAnd evaluate!\n\nprint(\"Term-Document Frequency\")\nprint(LR_cv.score(X_test_cv, y_test))\nprint(\"TF-IDF\")\nprint(LR_tfidf.score(X_test_tfidf, y_test))\n\nTerm-Document Frequency\n0.8412322274881516\nTF-IDF\n0.8367561874670879\n\n\nIn this case, TF-IDF did a little worse than term-document frequency vectorization on the test set."
  },
  {
    "objectID": "lecture-notes/vectorization-live.html#model-inspection",
    "href": "lecture-notes/vectorization-live.html#model-inspection",
    "title": "Introduction",
    "section": "Model Inspection",
    "text": "Model Inspection\nLet’s take a moment to learn more about how our term-document frequency-based model looks at the data. One good way to do this is by looking at the confusion matrices:\nThe false negative rate is higher than the true positive rate, suggesting that our model tends to tilt negative. Let’s take a look at some tweets that our model labeled as negative even though the label was positive:\nAt this point we might have some further questions for the producer of this data set about how he did the labeling: don’t some of these tweets look like they “really” should be negative?"
  },
  {
    "objectID": "lecture-notes/vectorization-live.html#word-based-sentiment-analysis",
    "href": "lecture-notes/vectorization-live.html#word-based-sentiment-analysis",
    "title": "Introduction",
    "section": "Word-Based Sentiment Analysis",
    "text": "Word-Based Sentiment Analysis\nA nice feature of linear models like logistic regression is that we can actually check the coefficient for each word in the model. This coefficient can give us important information about which words the model believes are most positive or most negative. One easy way to get at this information is to construct a data frame with the coefficients and the words:\n\ncoef_df = pd.DataFrame({\"coef\" : LR_cv.coef_[0], \"word\" : cv.get_feature_names_out()})\n\ncoef_df\n\n\n\n\n\n\n\n\ncoef\nword\n\n\n\n\n0\n-0.063696\n00\n\n\n1\n-0.015816\n000\n\n\n2\n-0.035407\n10\n\n\n3\n0.002059\n100\n\n\n4\n-0.214862\n11\n\n\n...\n...\n...\n\n\n2317\n0.444522\nyoung\n\n\n2318\n-0.446678\nyoutube\n\n\n2319\n-0.343226\nyouâ\n\n\n2320\n0.274800\nyâ\n\n\n2321\n-0.115663\nzero\n\n\n\n\n2322 rows × 2 columns\n\n\n\nNow we can obtain positive and negative words by sorting. Here are some of the good ones:\n\ncoef_df.sort_values(\"coef\", ascending = False).head(10)\n\n\n\n\n\n\n\n\ncoef\nword\n\n\n\n\n223\n3.832914\nbest\n\n\n2280\n3.470211\nwon\n\n\n849\n3.327824\nfriend\n\n\n945\n3.304033\nhand\n\n\n241\n3.265563\nbonus\n\n\n916\n3.261603\ngreat\n\n\n1541\n3.153919\npositive\n\n\n701\n3.145394\nenjoy\n\n\n565\n3.088850\ndedicated\n\n\n2303\n3.066329\nwow\n\n\n\n\n\n\n\nOn the other hand, here are some of the negative ones:\nA common use for these coefficients is to assign sentiment scores to sentences. Here’s a function that does this. It works by first stripping the punctuation and capitalization from a string, and then looking up each of its individual words in a dictionary.\n\nfrom string import punctuation \n\nd = {coef_df[\"word\"].loc[i] : coef_df[\"coef\"].loc[i] for i in coef_df.index}\n\ndef sentiment_of_string(s):\n    no_punc = s\n    for punc in punctuation:\n        no_punc = no_punc.replace(punc, \"\")\n    \n    words = no_punc.lower().split()\n    return np.mean([d[word] for word in words if word in d ])\n\nThis approach is the basis of The Hedonometer, a large-scale Twitter sentiment analysis tool from our friends at the University of Vermont."
  },
  {
    "objectID": "lecture-notes/vectorization-live.html#activity-1",
    "href": "lecture-notes/vectorization-live.html#activity-1",
    "title": "Introduction",
    "section": "Activity",
    "text": "Activity\nThere is a very important kind of information that is not captured by term-document matrices, even with inverse-document-frequency weighting. Consider the following two sentences:\n\n“I like pears, not apples.”\n“I like apples, not pears.”\n\nWould these sentences have different representations in a term-document matrix?"
  },
  {
    "objectID": "lecture-notes/more-on-classification.html",
    "href": "lecture-notes/more-on-classification.html",
    "title": "More Classifiers and More Labels",
    "section": "",
    "text": "$$\n$$\nIn this set of notes, we’ll introduce a few new classifiers at a high level, including classifiers that go beyond the framework of convex linear models.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/more-on-classification.html#multiple-class-labels",
    "href": "lecture-notes/more-on-classification.html#multiple-class-labels",
    "title": "More Classifiers and More Labels",
    "section": "Multiple Class Labels",
    "text": "Multiple Class Labels\nSo far, we’ve treated binary classification, especially in the setting where the labels \\(y \\in \\{0,1\\}\\). We’d like to do multiclass classification, where, for example, \\(y \\in \\{0, 1, 2\\}\\). This is the setting, for example, that you encounter in the blog post on penguin classification. The transition from binary classification to multiple class labels is not too complex, if we allow ourselves to think of the target label \\(y\\) as encoding a target vector \\(\\tilde{\\mathbf{y}}\\) with zeros in all entries except the \\(y\\)th entry. Let \\(k\\) be the number of possible classes. Then, if \\(k = 3\\) and \\(y = 1\\), then \\(\\tilde{\\mathbf{y}} = (0, 1, 0)\\).  For this to work, we have to make a few other modifications as well:This is often called one-hot encoding.\n\nPrediction Vectors\nOur prediction model \\(f(\\mathbf{x})\\) can’t just spit out a real number any more – it needs to spit out something that we can compare with \\(\\tilde{\\mathbf{y}}\\). So, things like \\(f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\) don’t work anymore! We usually assume that \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}^k\\), that is, \\(\\hat{\\mathbf{y}} = f(\\mathbf{x})\\) is a vector of the same length as \\(\\tilde{\\mathbf{y}}\\). As one important example of this, we might assume that \\[\nf(\\mathbf{x}) = \\mathbf{W}\\mathbf{x}\\;,\n\\] where now \\(\\mathbf{W}\\in \\mathbb{R}^{k \\times p}\\) is a matrix of weights. This is a direct generalization of our previous setting: if \\(f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\), then we can think of \\(\\mathbf{w}\\) as being a \\(p\\times 1\\) matrix.\n\n\nLoss Function\nWe also need to modify our loss function so that we can compute things like \\[\n\\ell(\\hat{\\mathbf{y}}, \\tilde{\\mathbf{y}})\n\\] when both \\(\\hat{\\mathbf{y}}\\) and \\(\\tilde{\\mathbf{y}}\\) are vectors. One common way we do this is via the categorical cross-entropy. First, define the softmax function \\(\\boldsymbol{\\sigma}:\\mathbb{R}^k\\rightarrow \\mathbb{R}^k\\) by the formula\n\\[\n\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})_h = \\frac{e^{\\hat{y}_h}}{\\sum_{h' = 1}^k e^{\\hat{y}_{h'}}}\\;.\n\\]\nThe vector \\(\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})\\) is a probability vector: all its entries are nonnegative and sum to 1. For convenience, write \\(\\hat{\\mathbf{p}} = \\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})\\). Then, then categorical cross-entropy is\n\\[\n\\ell(\\hat{\\mathbf{y}}, \\tilde{\\mathbf{y}}) = -\\sum_{h = 1}^k \\tilde{y}_h \\log \\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})_h\\;.  \n\\tag{1}\\]\nThe categorical cross-entropy is a generalization of the logistic loss."
  },
  {
    "objectID": "lecture-notes/more-on-classification.html#multiclass-empirical-risk",
    "href": "lecture-notes/more-on-classification.html#multiclass-empirical-risk",
    "title": "More Classifiers and More Labels",
    "section": "Multiclass Empirical Risk",
    "text": "Multiclass Empirical Risk\nWe can now write the general empirical risk (not assuming linearity or convexity) as\n\\[\n\\sum_{i = 1}^n \\ell(f(\\mathbf{x}_i), \\tilde{\\mathbf{y}}_i)\\;.\n\\]\nAs usual, we’d like to find a prediction rule \\(f\\) that makes the empirical risk small, although we need to be aware of possible issues related to overfitting."
  },
  {
    "objectID": "lecture-notes/more-on-classification.html#multinomial-logistic-regression",
    "href": "lecture-notes/more-on-classification.html#multinomial-logistic-regression",
    "title": "More Classifiers and More Labels",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nIn multinomial logistic regression, \\(f(\\mathbf{x}_i) = \\mathbf{W}\\mathbf{x}_i\\) and the loss function is the categorical cross-entropy from Equation 1. An important feature of multinomial logistic regression is that it has linear decision boundaries.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\nspecies = [s.split()[0] for s in le.classes_]\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n\ndef training_decision_regions(model, cols, **kwargs):\n    m = model(**kwargs)\n    m.fit(np.array(X_train[cols]), y_train)\n    plot_decision_regions(np.array(X_train[cols]), y_train, clf = m)\n    ax = plt.gca()\n    ax.set(xlabel = cols[0], \n                  ylabel = cols[1], \n                  title = f\"Training accuracy = {m.score(np.array(X_train[cols]), y_train).round(2)}\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, \n              species, \n              framealpha=0.3, \n              scatterpoints=1)\n\n\ntraining_decision_regions(LogisticRegression, cols)\n\n\n\n\nIf we fit an individual logistic regression model, we’ll be able to see how its predictions work:\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nNow that we’ve fit the model, we can inspect the weight matrix:\n\nw_hat = LR.coef_\nw_hat\n\narray([[-1.02133384,  2.03824239],\n       [ 0.21649256,  0.51465801],\n       [ 0.80484128, -2.5529004 ]])\n\n\nThis weight matrix multiplies the feature matrix to get the prediction matrix \\(\\hat{\\mathbf{Y}}\\).\n\ny_hat = np.array(X_train[cols])@w_hat.T\n\nThe built-in method LR.predict_proba will compute the predictions after having passed them through the softmax function. The advantage of this is that we can interpret each entry as the probability of class membership:\n\nP = LR.predict_proba(X_train[cols])\n\nP[0,:] # guess is category 2, Gentoo\n\narray([4.32383911e-06, 5.37250378e-03, 9.94623172e-01])\n\n\nHere’s a heatmap of the first 20 individuals and their predicted labels. Brighter yellow means greater predicted probability of belonging to the specified class.\n\np = plt.imshow(P[0:20,:].T)\n\n\n\n\nAlmost all of the individuals are clearly predicted in just one of the classes, while the model is less confident about the membership of the penguin with index 10."
  },
  {
    "objectID": "lecture-notes/more-on-classification.html#support-vector-machine",
    "href": "lecture-notes/more-on-classification.html#support-vector-machine",
    "title": "More Classifiers and More Labels",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\nThe support vector machine classification problem for binary classification is a convex linear model in which we use the so-called hinge loss. In the notation from our previous lectures, it can be written like this:\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\left[\\sum_{i = 1}^n \\max \\{1 - y_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, 0\\} + \\frac{1}{2C}\\sum_{\\ell = 1}^p w_\\ell^2\\right]\\;.\n\\]\nMathematically, the support vector machine is an exceptionally beautiful algorithm, primarily because it admits a “kernel trick.” The kernel trick allows us to use infinite-dimensional nonlinear features for prediction, which can significantly enhance the expressive power of our models.  To my knowledge, unfortunately, the support vector machine doesn’t handle multiclass classification very well. What scikit-learn does is split the problem into a sequence of binary problems (“blue or not blue”) to obtain the final result. Here’s an example:For more on the kernel trick, see Hardt and Recht, p. 58-62.\n\nfrom sklearn.svm import SVC\ntraining_decision_regions(SVC, cols, kernel = \"rbf\", gamma = .1)\n\n\n\n\nHere, the rbf kernel can be changed according to user preferences. gamma controls how wiggly the decision boundary is allowed to be:\n\nfrom sklearn.svm import SVC\ntraining_decision_regions(SVC, cols, kernel = \"rbf\", gamma = 10)\n\n\n\n\nCross-validation or other tools should be used in order to determine a value of \\(\\gamma\\) that has good expressive power while avoiding overfitting."
  },
  {
    "objectID": "lecture-notes/more-on-classification.html#multilayer-perceptron",
    "href": "lecture-notes/more-on-classification.html#multilayer-perceptron",
    "title": "More Classifiers and More Labels",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\nLogistic regression and support vector machine are both still in the convex linear model framework. Let’s now move beyond this framework for the first time. We’ll consider\n\nA new nonconvex linear model.\nA nonconvex nonlinear model.\n\nWe’ve already seen a nonconvex linear model: perceptron! To create a more useful one, let’s consider the following idea: we’re going to just stack logistic regressions on top of each other, like this: \\[\n\\mathbf{Z}= \\boldsymbol{\\sigma}(\\mathbf{X}\\mathbf{W})\n\\]\nThat is, the matrix \\(\\mathbf{Z}\\) is the result of computing the matrix product \\(\\mathbf{X}\\mathbf{W}\\) and then applying the softmax function row-wise. If \\(\\mathbf{W}\\) is \\(p\\times \\ell\\), then \\(\\mathbf{X}\\mathbf{W}\\) is an \\(n\\times \\ell\\) matrix, as is \\(\\mathbf{Z}\\). This is essentially multinomial logistic regression. Now, here’s the thing: what if we just used \\(\\mathbf{Z}\\) as the input to another logistic regression? That is, we compute \\[\n\\hat{\\mathbf{Y}} = \\boldsymbol{\\sigma}(\\mathbf{Z}\\mathbf{W}')\\;,\n\\]\nwhere \\(\\mathbf{W}'\\) is a new matrix of weights and \\(\\hat{\\mathbf{Y}}\\) is our matrix of predictions that we will assess using the categorical cross-entropy or another such function. Then, the empirical risk minimization problem is \\[\n\\hat{\\mathbf{W}}, \\hat{\\mathbf{W}}' = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{W}, \\mathbf{W}'} \\sum_{i = 1}^n \\ell(\\boldsymbol{\\sigma}(\\boldsymbol{\\sigma}(\\mathbf{X}\\mathbf{W})\\mathbf{W}')_i, \\tilde{\\mathbf{y}}_i) \\;.\n\\]\nThis problem is no longer convex, but we can still try to optimize it with gradient descent.\nWe often call the computation of \\(\\mathbf{Z}\\) a hidden layer because it is neither the feature matrix \\(\\mathbf{X}\\) nor the target \\(\\tilde{\\mathbf{y}}\\). So, we have created a model with a single hidden layer. The idea of stacking together simple linear transformations with simple nonlinearities is the fundamental idea of modern deep learning.\nscikit-learn implements models like this under the heading of “multilayer perceptron” (the name is mostly historical). We can create a multilayer perceptron like this:\n\nfrom sklearn.neural_network import MLPClassifier\n\ntraining_decision_regions(MLPClassifier, cols, activation = \"logistic\", hidden_layer_sizes = (100, 100, 100))\n\n\n\n\nWe observe apparently linear decision boundaries in the data set this time, although in principle the model could also have generated nonlinear boundaries."
  },
  {
    "objectID": "lecture-notes/more-on-classification.html#decision-tree-classifiers",
    "href": "lecture-notes/more-on-classification.html#decision-tree-classifiers",
    "title": "More Classifiers and More Labels",
    "section": "Decision Tree Classifiers",
    "text": "Decision Tree Classifiers\nDecision tree classifiers still do empirical risk minimization, but they are both nonlinear and nonconvex. The best way to see what a decision tree classifier does is to train one and visualize it:\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nDTC = DecisionTreeClassifier(max_depth = 3)\nDTC.fit(X_train[cols], y_train)\n\np = plot_tree(DTC, feature_names = cols, filled = True, class_names = species)\n\n\n\n\nWe observe that the decision tree works by making a sequence of decisions that sort the data into progressively finer buckets. You can implement a decision tree as nothing more than a sequence of nested if-else statements, although the algorithms to actually train them can be trickier. The decision regions for decision trees look “boxy,” composed of vertical and horizontal segments:\n\ntraining_decision_regions(DecisionTreeClassifier, cols, max_depth = 3)\n\n\n\n\nDecision trees are very flexible models, but it is easy for them to overfit if the depth is too high:\n\ntraining_decision_regions(DecisionTreeClassifier, cols, max_depth = 10)\n\n\n\n\nFor this reason, it is common to choose the depth through cross validation:\n\nfrom sklearn.model_selection import cross_val_score\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1)\n\nfor d in range(2, 10):\n    T = DecisionTreeClassifier(max_depth = d)\n    m = cross_val_score(T, X_train[cols], y_train, cv = 10).mean()\n    ax.scatter(d, m, color = \"black\")\n    # ax.scatter(d, T.score(X_test, y_test), color = \"firebrick\")\n\nlabs = ax.set(xlabel = \"Complexity (depth)\", ylabel = \"Performance (score)\")\n\n\n\n\nIt looks like a depth of roughly 6 might be about right for this data set:\n\ntraining_decision_regions(DecisionTreeClassifier, cols, max_depth = 6)\n\n\n\n\n\nRandom Forest\nA random forest is essentially a collection of many decision trees that have been trained on random subsets of the data. Random forest classifiers have some very good properties that help them be fairly resistent to overfitting – they usually work pretty well “out of the box.”\n\nfrom sklearn.ensemble import RandomForestClassifier\ntraining_decision_regions(RandomForestClassifier, cols)"
  },
  {
    "objectID": "lecture-notes/intro-deep-live.html",
    "href": "lecture-notes/intro-deep-live.html",
    "title": "Deep Penguin Classification",
    "section": "",
    "text": "Code\nimport torch\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\ntarget_ix_dict = {\n  \"Adelie Penguin (Pygoscelis adeliae)\" : 0,\n  \"Chinstrap penguin (Pygoscelis antarctica)\" : 1,\n  \"Gentoo penguin (Pygoscelis papua)\" : 2\n}\n\nclass PenguinsDataset(Dataset):\n  def __init__(self, train = True):\n    if train: \n      url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\n    else:\n      url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\n\n    df = pd.read_csv(url)\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = pd.get_dummies(df, columns = [\n      \"Island\", \"Stage\", \"Clutch Completion\", \"Sex\"\n    ])\n    df = df.dropna()\n\n    self.df = df\n    self.transform = lambda x: torch.tensor(x, dtype = torch.float32)\n    self.target_ix = lambda x: target_ix_dict[x]\n    self.target_transform = lambda x: torch.zeros(\n    3, dtype=torch.float).scatter_(dim=0, index=torch.tensor(x), value=1)\n\n  def __len__(self):\n    return self.df.shape[0]\n\n  def __getitem__(self, idx):\n    features = self.df.drop([\"Species\"], axis = 1).iloc[idx,:]\n    label    = self.df.iloc[idx,:][\"Species\"]\n    features = self.transform(features)\n    \n    label    = self.target_ix(label)\n    label    = self.target_transform(label)\n\n    features = features.to(device)\n    label = label.to(device)\n\n    return features, label\n\n\n\n\nCode\nif torch.backends.mps.is_available():\n  device = \"cpu\"\nelif torch.cuda.is_available():\n  device = \"cuda\"\nelse:\n  device = \"cpu\"\n\n\nHere are our data sets. PenguinsDataset is a custom class that I implemented above, while DataLoader is a utility from torch that automatically handles things like batching and randomization for gradient descent.\n\ntrain = PenguinsDataset()\ntrain_dataloader = DataLoader(train, batch_size=10, shuffle=True)\n\nval = PenguinsDataset(False)\nval_dataloader = DataLoader(val, batch_size=10, shuffle=True)\n\nNow we’re ready to define our model. To see how we do things in torch, let’s start by implementing logistic regression. To start, we need to create a predictor model that does the prediction step of logistic regression. If you’ll recall, this is nothing more than matrix multiplication. The nn.Linear “layer” supplied by torch implements this:\n\nfrom torch import nn\n\nclass penguinLogistic(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n    self.linear = nn.Sequential(\n      nn.Linear(14, 3) # (number of features, number of class labels)\n    )\n  def forward(self, x):\n    return self.linear(x)\n\nWe can optimize this model using the cross-entropy loss and the Adam optimizer. As you’ll recall, logistic regression is nothing more than matrix multiplication plus the cross-entropy loss, so this is a logistic regression model!\n\nmodel = penguinLogistic()\nloss_fn = nn.CrossEntropyLoss()\n\nlearning_rate = 1e-4\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nNow we need to actually perform the optimization. The torch package gives us a lot of control over exactly how this happens, and we’ll go over the details in a future lecture.\n\n\nCode\ndef training_loop(model, train_dataloader, val_dataloader, learning_rate, epochs):\n\n  train_size = len(train_dataloader.dataset)\n  val_size = len(val_dataloader.dataset)\n\n  train_loss_history = []\n  train_acc_history  = []\n  val_loss_history   = []\n  val_acc_history    = []\n\n  for t in range(epochs):\n    train_loss = 0.0\n    train_acc  = 0.0\n    val_loss   = 0.0\n    val_acc    = 0.0\n\n    for batch, (X, y) in enumerate(train_dataloader):\n      \n      optimizer.zero_grad()\n      pred = model(X)\n      fit = loss_fn(pred, y)\n      train_loss += fit.item() / train_size\n      \n      train_acc += (pred.argmax(dim = 1) == y.argmax(dim = 1)).sum() / train_size\n      \n      # Backpropagation\n      \n      fit.backward()\n      optimizer.step()\n\n    train_loss_history += [train_loss]\n    train_acc_history  += [train_acc]\n\n    for batch, (X, y) in enumerate(val_dataloader):\n      with torch.no_grad():\n        pred = model(X)\n        fit = loss_fn(pred, y)\n        val_loss += fit.item() / val_size\n        val_acc += (pred.argmax(dim = 1) == y.argmax(dim = 1)).sum() / val_size\n\n    val_loss_history += [val_loss]\n    val_acc_history  += [val_acc]\n\n    if t % 50 == 0:\n      print(f\"epoch {t}: val_loss = {val_loss}, val_accuracy = {val_acc}\")\n    \n  return train_loss_history, train_acc_history, val_loss_history, val_acc_history\n\n\nfrom matplotlib import pyplot as plt\n\ndef plot_histories(tlh, tah, vlh, vah):\n\n  fig, axarr = plt.subplots(1, 2, figsize = (6, 3))\n\n  axarr[0].plot(tlh, label = \"train\")\n  axarr[0].plot(vlh, label = \"validation\")\n  axarr[0].legend()\n  axarr[0].set(xlabel = \"epoch\", title = \"loss\")\n  axarr[0].semilogy()\n\n  axarr[1].plot(tah, label = \"train\")\n  axarr[1].plot(vah, label = \"validation\")\n  axarr[1].legend()\n  axarr[1].set(xlabel = \"epoch\", title = \"accuracy\")\n\n\nLet’s go ahead and train!\n\ntlh, tah, vlh, vah = training_loop(model, train_dataloader, val_dataloader, 1e-4, 500)\nplot_histories(tlh, tah, vlh, vah)\n\nepoch 0: val_loss = 5.624737823710722, val_accuracy = 0.3382353186607361\nepoch 50: val_loss = 0.2299227907377131, val_accuracy = 0.38235294818878174\nepoch 100: val_loss = 0.1359223749707727, val_accuracy = 0.5147058963775635\nepoch 150: val_loss = 0.09175908959963744, val_accuracy = 0.6029412150382996\nepoch 200: val_loss = 0.06135511223007652, val_accuracy = 0.7500000596046448\nepoch 250: val_loss = 0.050761135185466086, val_accuracy = 0.75\nepoch 300: val_loss = 0.04315454644315383, val_accuracy = 0.8382353186607361\nepoch 350: val_loss = 0.03966436578946955, val_accuracy = 0.8529411554336548\nepoch 400: val_loss = 0.029550061725518283, val_accuracy = 0.9411765336990356\nepoch 450: val_loss = 0.024872612865532145, val_accuracy = 0.9411765336990356\n\n\n\n\n\nWe can see that our model is doing much better than we would expect from random guessing on this problem, although it may not be competitive with many of the models that you implemented in your analysis of the penguins data set. Further training or tweaks to parameters like batch sizes and learning rates could potentially help improve performance here.\nLet’s try adding a hidden layer, as in ?@eq-single-layer. To do this, we need to add a nonlinearity \\(\\alpha\\) and more Linear layers. The important points are:\n\nThe first dimension of the first linear layer needs to match the number of features of the input.\nThe final dimension of the last linear layer needs to match the number of possible labels.\nThe final dimension of each linear layer needs to match the first dimension of the next layer.\n\nThese rules follow directly from the need to make all the matrix multiplications turn out right.\nSo, let’s try a model with a single layer of 100 hidden units:\n\nfrom torch import nn\nclass penguinNN(nn.Module):\n  def __init__(self):\n    super().__init__()\n    \n    self.linear_relu_stack = nn.Sequential(\n          nn.Linear(14, 100), # U_1\n          nn.ReLU(),          # common choice of alpha these days\n          nn.Linear(100, 3)   # W\n      )\n\n  def forward(self, x):\n    return self.linear_relu_stack(x)\n\nmodel = penguinNN().to(device)\n\nWe can optimize it using the same approach as before:\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\ntlh, tah, vlh, vah = training_loop(model, train_dataloader, val_dataloader, 1e-4, 500)\nplot_histories(tlh, tah, vlh, vah)\n\nepoch 0: val_loss = 10.76734447479248, val_accuracy = 0.4852941036224365\nepoch 50: val_loss = 0.0789748820311883, val_accuracy = 0.6470587849617004\nepoch 100: val_loss = 0.0914852855836644, val_accuracy = 0.6176470518112183\nepoch 150: val_loss = 0.042512568480828225, val_accuracy = 0.8823529481887817\nepoch 200: val_loss = 0.041880780283142544, val_accuracy = 0.8970588445663452\nepoch 250: val_loss = 0.04079361150369924, val_accuracy = 0.8529411554336548\nepoch 300: val_loss = 0.06874325433198143, val_accuracy = 0.6323530077934265\nepoch 350: val_loss = 0.03214393775252735, val_accuracy = 0.8529411554336548\nepoch 400: val_loss = 0.02306258481215028, val_accuracy = 0.8970588445663452\nepoch 450: val_loss = 0.013708957306602423, val_accuracy = 0.9705883264541626\n\n\n\n\n\nThis model has also done well on this task, although still not closer to perfect accuracy. It’s also interesting to note that the model’s loss and accuracy both vary quite wildly during the training process. This is not uncommon, but may also point to a possible benefit of using a smaller step-size.\nIt might appear that we haven’t really gained much from the massive costs of mathematical structure and computational architecture – we could have done better with just tools from sklearn, or even our own hand-built implementations! We’ll soon see the infrastructure working to our advantage when we get to problems involving large data sets with complex structure, like text and images.\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html",
    "href": "lecture-notes/intro-allocative-bias.html",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "",
    "text": "$$\n$$\nToday we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by Angwin et al. (2016), originally written for ProPublica. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nYou’ve already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html#data-preparation",
    "href": "lecture-notes/intro-allocative-bias.html#data-preparation",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\nLet’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.\n\nimport pandas as pd\nimport seaborn as sns\ncompas_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/compas-scores-two-years.csv\"\ncompas = pd.read_csv(compas_url)\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\n# boolean vectors (technically, pd.Series)\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\n\ncompas = compas[is_white | is_black]\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas.head()\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html#preliminary-explorations",
    "href": "lecture-notes/intro-allocative-bias.html#preliminary-explorations",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Preliminary Explorations",
    "text": "Preliminary Explorations\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as related to quantities like \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\), which is a large number when the algorithm has high confidence in predicting a 1 label. Here, a decile score of 10 indicates high confidence in predicting a 1 (= recidivating) label.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\nsns.barplot(data = counts, x = \"decile_score\", y = \"n\", hue = \"race\")\n\n&lt;AxesSubplot: xlabel='decile_score', ylabel='n'&gt;\n\n\n\n\n\nFinally, let’s take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in this data, approximately 47% of all defendants went on to be charged of another crime within the next two years. We can also compute the recidivism rate by race:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514340\nCaucasian           0.393643\nName: two_year_recid, dtype: float64"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html#the-propublica-findings",
    "href": "lecture-notes/intro-allocative-bias.html#the-propublica-findings",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "The ProPublica Findings",
    "text": "The ProPublica Findings\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of Angwin et al. (2016), we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = (compas[\"decile_score\"] &gt; 4)\n\nNow we have a binary prediction, and we can compute things like confusion matrices:\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(compas[\"two_year_recid\"], \n                 compas[\"predicted_high_risk\"])\n\narray([[2129, 1154],\n       [ 993, 1874]])\n\n\nWe can normalize this confusion matrix to get things like the false positive and false negative rates:\n\nconfusion_matrix(compas[\"two_year_recid\"], \n                 compas[\"predicted_high_risk\"],\n                 normalize = \"true\")\n\narray([[0.64849223, 0.35150777],\n       [0.34635507, 0.65364493]])\n\n\nWe see that the algorithm (predicting recidivism if decile_score is 5 or above) is right about 65% of the time. A bit more specifically, both the true positive (TP) and true negative (TN) rates are approximately 65%. Both the false positive (FP) and false negative (FN) rates are approximately 35%.\nWe can also check the overall accuracy:\n\n(compas[\"two_year_recid\"] == compas[\"predicted_high_risk\"]).mean()\n\n0.6508943089430894\n\n\nThe accuracy is relatively consistent even when we break things down by race:\n\nblack_ix = compas[\"race\"] == \"African-American\"\nwhite_ix = compas[\"race\"] == \"Caucasian\"\n\ncorrect_pred = compas[\"two_year_recid\"] == compas[\"predicted_high_risk\"]\n\n# accuracy on Black defendants\naccuracy_black = correct_pred[black_ix].mean()\n\n# accuracy on white defendants\naccuracy_white = correct_pred[white_ix].mean()\n\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race. Here’s the confusion matrix for Black defendants:\n\nconfusion_matrix(compas[\"two_year_recid\"][black_ix], \n                 compas[\"predicted_high_risk\"][black_ix],\n                 normalize = \"true\")\n\narray([[0.55153203, 0.44846797],\n       [0.27985271, 0.72014729]])\n\n\nAnd here it is for white defendants:\n\nconfusion_matrix(compas[\"two_year_recid\"][white_ix], \n                 compas[\"predicted_high_risk\"][white_ix],\n                 normalize = \"true\")\n\narray([[0.76545699, 0.23454301],\n       [0.47722567, 0.52277433]])\n\n\nThe ProPublica study focused on the false positive rate (FPR), which is in the top right corner of the confusion matrices. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to representational bias, which we’ll discuss more later in the semester.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. We might claim this as a form of allocative bias: bias in how resources or opportunities (in this case, freedom) are allocated between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nIn the language of Corbett-Davies et al. (2017), an algorithm that has equal FPRs across two groups satisfies predictive equality with respect to those two groups. So, the COMPAS algorithm fails to possess predictive equality. The idea of error rate balance in Chouldechova (2017) and balance for the positive/negative class in Kleinberg, Mullainathan, and Raghavan (2016) are similar to predictive equality.\nIn summary, the ProPublica argument was:\n\nSince the FPR differs across racial groups in ways that reinforce the oppression of Black people, the COMPAS algorithm possesses racial bias."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html#calibration",
    "href": "lecture-notes/intro-allocative-bias.html#calibration",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Calibration",
    "text": "Calibration\nIs that the end of the story? Emphatically not! Angwin et al. (2016) kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. For example, Corbett-Davies et al. (2017) consider a different idea of fairness. While predictive equality requires that the FPRs for white and Black defendants be equal, calibration expresses a different intuition:\n\nA white defendant and a Black defendant who each receive the same score should both have the same risk of recidivating.\n\n Another way to say this is that a score of 7 means the same thing, no matter the race of the defendant.Compare: an “A” in CS 201 means the same thing for your future success in CS, no matter your gender.\nWe can compute the recidivism rates for each race at each decile score using some Pandas .groupby magic:\n\nmeans = compas.groupby([\"race\", \"decile_score\"])[\"two_year_recid\"].mean().reset_index(name = \"mean\")\n\nsns.lineplot(data = means, x = \"decile_score\", y = \"mean\", hue = \"race\")\n\n&lt;AxesSubplot: xlabel='decile_score', ylabel='mean'&gt;\n\n\n\n\n\nThe actual recidivism rate at each risk score is roughly the same between Black and white defendants, especially for decile scores past 5 or so."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html#calibration-for-binary-classifiers",
    "href": "lecture-notes/intro-allocative-bias.html#calibration-for-binary-classifiers",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Calibration for Binary Classifiers",
    "text": "Calibration for Binary Classifiers\nSo far in this course, we have primarily studied binary classifiers that produce a single 0-1 predicted label, rather than a score like a decile. For these classifiers, calibration means that the fraction of predicted recidivists who actually recidivated is the same across groups. If we follow the Angwin et al. (2016) approach and say that the algorithm predicts someone as high risk if their decile score is 4 or above, we would obtain the following results:\n\ncompas[\"pred_high_risk\"] = compas[\"decile_score\"] &gt;= 4\n\nmeans = compas.groupby([\"race\", \"pred_high_risk\"])[\"two_year_recid\"].mean().reset_index(name = \"mean\")\n\np = sns.barplot(data = means, x = \"pred_high_risk\", y = \"mean\", hue = \"race\")\n\n\n\n\nThere are arguments to be had here, but from the perspective of calibration at the decile score threshold of 4, the algorithm might appear to be biased in the other direction: of those who were predicted high risk, slightly more Black than white defendants were arrested within the next two years. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be reasonably well calibrated."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias.html#overcoming-bias",
    "href": "lecture-notes/intro-allocative-bias.html#overcoming-bias",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Overcoming Bias?",
    "text": "Overcoming Bias?\nOk, so COMPAS is reasonably calibrated, but does not satisfy predictive equality. Couldn’t we just find a way to fix it so that it could be both calibrated and predictively equitable? A little fine-tuning here and there maybe? Sadly, no: this is not just difficult, but actually mathematically impossible, as shown by Chouldechova (2017).\nKleinberg, Mullainathan, and Raghavan (2016) give some other definitions of fairness in algorithmic decision-making, again concluding that several concepts of fairness mathematically exclude other ones."
  },
  {
    "objectID": "lecture-notes/auto-diff.html",
    "href": "lecture-notes/auto-diff.html",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "",
    "text": "$$\n$$\nThe code presented in this lecture is derived from Boaz Barak’s blog post “Yet Another Backpropagation Tutorial” on his blog Windows on Theory. This code was in turn inspired by the micrograd package developed by Andrej Karpathy.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/auto-diff.html#the-computational-graph",
    "href": "lecture-notes/auto-diff.html#the-computational-graph",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "The Computational Graph",
    "text": "The Computational Graph\nA computational graph is a directed acyclic graph that describes the sequence of computations performed by a function. For example, consider the following function, which computes the loss in 1D linear regression on a single observation:\n\\[\n\\mathcal{L}(w, b) =  (wx + b - y)^2\\;.\n\\]\nWe might be accustomed to looking at functions like these and taking them in “all at once.” We can, however, break down the steps into individual operations. Let’s suppose that all we know how to do is add, subtract, and multiply pairs of numbers. We could write the complete sequence of calculations like this:\nh_1 = w*x\nh_2 = h_1+b\nh_3 = h_2 - y\nh_4 = h_3*h_3\nA nicer way to present this sequence of calculations is through a computational graph:\n\n\n\n\nflowchart LR\n    subgraph inputs\n        w\n        b\n    end\n\n    subgraph constants\n        x\n        y\n    end\n\n    w & x --&gt; *\n    b & * --&gt; +\n    + & y --&gt; -\n    - --&gt; m[*]\n    - --&gt; m[*]\n\n\n\n\n\nArranging all our computations in an orderly computational graph turns out to be the key to automatic differentiation, which is the topic of today’s lecture."
  },
  {
    "objectID": "lecture-notes/auto-diff.html#automatic-differentiation",
    "href": "lecture-notes/auto-diff.html#automatic-differentiation",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\nOur standard gradient descent update is\n\\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha \\nabla \\mathcal{L}(\\mathbf{w}^{(t)})\\;,\n\\]\nwhere \\(\\mathcal{L}\\) is the empirical risk. Most modern models don’t use exactly this update, but instead use other methods involving the gradient or stochastic estimates of it. Even though we’re looking at more complicated algorithms, though, we still need to calculate gradients! This lecture is all about how to do that efficiently.\n\nDifferentiation By Hand\nTo start, let’s compute the gradient of our function \\(\\mathcal{L}(w, b)\\) with respect to the parameters \\(w\\) and \\(b\\). Doing this requires us to apply the chain rule. For single variables, the chain rule says that, if \\(f(x) = g(h(x))\\), then\n\\[\n\\frac{df(x_0)}{dx} = \\frac{dg(h(x_0))}{dy} \\frac{dh(x_0)}{dx}\n\\]\nMechanically, the chain rule says that we start be differentiating the outermost operation (in this case, squaring) and then multiplying by the derivative of what’s inside. So, applying the chain rule here to our linear regression loss function yields \\[\n\\begin{aligned}\n\\frac{\\partial \\mathcal{L}(w_0, b_0)}{\\partial w} &= 2(w_0 x + b) w_0 \\\\\n\\frac{\\partial \\mathcal{L}(w_0, b_0)}{\\partial b} &= 2(w_0 x + b)\n\\end{aligned}\n\\]\nThis process of working out-to-in when differentiating a function corresponds to working right-to-left or backwards in the computational graph. At each step, the computational graph combines two numbers to produce a third number. We could formally write down all the steps in the computational graph like this:\n\\[\n\\begin{aligned}\nz_1 &= w*x \\\\\nz_2 &= z_1 + b \\\\\nz_3 &= z_2 - y \\\\\n\\mathcal{L}&= z_3*z_3\n\\end{aligned}\n\\]\nWe then work bottom-to-top to compute all the derivatives.\n\\[\n\\begin{aligned}\n    \\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{L}} &= 1 \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial z_3} &= \\frac{\\partial \\mathcal{L}}{\\partial \\mathcal{L}}\\frac{\\partial \\mathcal{L}}{\\partial z_3}      =  2z_3 \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial z_2} &= \\frac{\\partial \\mathcal{L}}{\\partial z_3} \\frac{\\partial z_3}{\\partial z_2}      = 2z_3\\times 1 \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial z_1} &= \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\frac{\\partial z_2}{\\partial z_1}      = 2z_3\\times 1\\times 1 \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial w}   &= \\frac{\\partial \\mathcal{L}}{\\partial z_1} \\frac{\\partial z_1}{\\partial w}        = 2z_3\\times 1\\times 1\\times x \\\\\n    \\frac{\\partial \\mathcal{L}}{\\partial b}   &= \\frac{\\partial \\mathcal{L}}{\\partial z_1} \\frac{\\partial z_1}{\\partial b}        = 2z_3\\times 1\\times 1\\times 1\\;.\n\\end{aligned}\n\\]\nWe then need to replace each of \\(z_1\\), \\(z_2\\), and \\(z_3\\) with their values (in terms of \\(w\\), \\(b\\), \\(x\\), and \\(y\\)).\nThis process shows us some hints for how we can automate the process of taking a derivative:\n\nFirst, organize the computation into a computational graph.\nConduct a forward pass in which we evaluate the computation at the inputs (in this case, \\(w\\) and \\(b\\)). This tells the value of the loss function and the value of each node in terms of the inputs.\nConduct a backward pass in which we form the derivative of each node in terms of the derivatives of its children in the computational graph."
  },
  {
    "objectID": "lecture-notes/auto-diff.html#backpropagation",
    "href": "lecture-notes/auto-diff.html#backpropagation",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "Backpropagation",
    "text": "Backpropagation\nThe backpropagation (backprop) algorithm expresses this heuristic idea as an efficient algorithm. Backprop is one of the most important methods of automatic differentiation.\n\n\n\n\n\n\nBackpropagation (Backprop)\n\n\n\n\nBegin with the final expression on the far right of the computational graph, which we’ll call \\(z\\).\nCompute the value of this expression, storing the value of all intermediate expressions, and the manner in which these intermediate expressions are combined throughout the graph.\nSet \\(\\frac{\\partial z}{\\partial z} = 1\\).\nThen, proceeding recursively, backwards along the computational graph, set \\(\\frac{\\partial z}{\\partial z_i} = \\sum_{j \\in \\mathrm{children}(z_i)} \\frac{\\partial z}{\\partial z_j} \\frac{\\partial z_j}{\\partial z_i}\\).\n\n\n\nThis process can be fully automated, as we’ll see below.\nIt is sometimes said that backprop is just the chain rule of (multivariable) calculus. This is not entirely correct. The chain rule is indeed the mathematical proof that backprop is correct, but backprop provides an extremely efficient, scalable way of organizing the computation of derivatives that is not implied by the chain rule."
  },
  {
    "objectID": "lecture-notes/auto-diff.html#implementing-backprop",
    "href": "lecture-notes/auto-diff.html#implementing-backprop",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "Implementing Backprop",
    "text": "Implementing Backprop\nHere we’ll code up just enough of an automatic differentiation via backprop engine to implement 1D linear regression with stochastic gradient descent. The centerpiece of the implementation is the Value class. You can think of the Value class as representing a node in the computation graph. Each node has:\n\ndata (the number currently stored at that node).\ngrad, the derivative of the final value in the computational graph with respect to the node’s data.\nA _backward method which will be used to compute the gradient of the node. What this method actually is will depend on how the node is used in future computations.\nA set of _prev (nodes that are “previous” to the current node in the sense of being upstream in the computational graph).\n\n\nclass Value:\n    \"\"\" stores a single scalar value v \n         and placeholder for derivative d output/ d v\"\"\"\n \n    def __init__(self, data, _children=()):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n\nThe most important method of this class, which handles backwards navigation along the computational graph, is the backward method:\n\ndef backward(self, visited = None): \n1    if visited is None:\n        visited= set([self])\n        self.grad = 1\n2    self._backward()\n3    for child in self._prev:\n        if not child in visited:\n            visited.add(child)\n            child.backward(visited)\n\nValue.backward = backward # assign this function as a method of Value\n\n\n1\n\nThis case corresponds the top-level node in the computational graph. In our setting this will usually be the value of the loss function.\n\n2\n\nThis is the step in which we call the node’s internal _backward() method. This method “does the calculus” and depends on how the node was computed.\n\n3\n\nWe then need to go through every node that is used in the computation of this node and also calculate their gradients.\n\n\n\n\nThe backward method handles the recursive logic of automatic differentiation. However, it doesn’t do any of the actual math. We need to implement this math within each of the arithmetic operations that we are going to implement for the Value class. Here’s how we implement addition:\n\ndef __add__(self, other):\n1    other = other if isinstance(other, Value) else Value(other)\n2    out = Value(self.data + other.data, (self, other))\n\n3    def _backward():\n        self.grad += out.grad\n        other.grad += out.grad\n    out._backward = _backward\n\n    return out\n\nValue.__add__ = __add__ # assign this function as a method of Value\n\n\n1\n\nThis adds other to the computational graph, converting it to a Value if needed.\n\n2\n\nWe need to do the addition itself, log the fact that self and other were used as inputs into this operation, and return a Value that contains both the data reflecting the addition and the information about the inputs.\n\n3\n\nDefine a _backward method to update the gradients for self and other, using upstream gradient information from out, and attach it to out. This method will then be used when backward is called. The exact structure of _backward requires that we do some calculus.\n\n\n\n\nLet’s do another math operation. This operation is very similar, but with a slightly more complex _backward method that reflects the chain rule from calculus.\n\ndef __mul__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data * other.data, (self, other))\n\n    def _backward():\n        self.grad += other.data * out.grad\n        other.grad += self.data * out.grad\n    out._backward = _backward\n\n    return out\n\nValue.__mul__ = __mul__\n\nHaving defined addition and multiplication, we can also pretty quickly define subtraction:\n\ndef __neg__(self): return self * -1\ndef __sub__(self, other):  return self + (-other)\n\nValue.__neg__ = __neg__\nValue.__sub__ = __sub__\n\nLet’s see an example of this in action. We can define the function \\(f(x) = (x + 2)^2 + x^3\\). Let’s do this and compute \\(f(3)\\):\n\na = Value(3)\ndef f(x): return (x+2)*(x+2) + x*x*x\ny = f(a)\nprint(f\"computed value = {y.data}\\n\")\n\ncomputed value = 52\n\n\n\nNow here’s the thing: because we can represent \\(f\\) in terms of multiplications and additions, we can also calculate \\(f'(3)\\), just by running the backward method. Since\n\\[\nf'(x) = 2(x+2) + 3x^2\\;,\n\\]\nwe are expecting that \\(f'(3) = 2(3+2) + 3\\cdot 3^2 = 37\\). Let’s check:\n\ny.backward()\nprint(f\"derivative = {a.grad}\")\n\nderivative = 37\n\n\nLooks good!"
  },
  {
    "objectID": "lecture-notes/auto-diff.html#autodiff-linear-regression",
    "href": "lecture-notes/auto-diff.html#autodiff-linear-regression",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "Autodiff Linear Regression",
    "text": "Autodiff Linear Regression\nWe’ve implemented enough automatic differentiation that we can differentiation any function that can be constructed out of a combination of additions, subtractions, and multiplications. This is enough to do linear regression with stochastic gradient descent! We’ll focus on the 1-dimensional version, in which we want to minimize \\[\n\\mathcal{L}(w, b) = \\frac{1}{n}\\sum_{i = 1}^n (wx_i + b - y_i)^2\\;.\n\\]\nIn stochastic gradient descent, we don’t actually need to evaluate all of these terms at once: instead, we can just evaluate (and differentiate) \\[\n\\mathcal{L}_i(w, b) =  (wx_i + b - y_i)^2\\;.\n\\]\nHere’s the computational graph describing the loss function:\n\n\n\n\nflowchart LR\n    subgraph inputs\n    w\n    b\n    end\n\n    w & x_i --&gt; *\n    b & * --&gt; +\n    + & y_i --&gt; -\n    - --&gt; m[*]\n    - --&gt; m[*]\n\n\n\n\n\nIn order to implement this with automatic differentiation, let’s first implement the predictor model \\(f(x) = wx + b\\) as a class, using\n\nclass Linear:\n  def __init__(self):\n1    self.w,self.b = Value(np.random.rand()),Value(np.random.rand())\n\n2  def __call__(self,x): return self.forward(x)\n\n3  def forward(self, x): return self.w*x+self.b\n\n4  def zero_grad(self): self.w.grad, self.b.grad = 0,0\n\n\n1\n\nInitialize a random slope \\(w\\) and intercept \\(b\\).\n\n2\n\nWhat should happen when the model accepts input.\n\n3\n\nThis backend to __call__ isn’t necessary; I just implemented it this way because PyTorch wants us to implement a method called forward for our models.\n\n4\n\nZero out the gradients (need to do after each round of gradient descent).\n\n\n\n\nNow that we’ve implemented this model, we’re already to train it. First let’s create some random data:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nw, b = np.random.rand(), np.random.rand()\n\nn = 100\nX = np.random.rand(n)\ny = w*X + b + 0.05*np.random.randn(n)\n\nplt.scatter(X, y)\nplt.gca().set(xlabel = \"Predictor\", ylabel = \"Target\")\n\n[Text(0.5, 0, 'Predictor'), Text(0, 0.5, 'Target')]\n\n\n\n\n\nAnd now let’s do stochastic gradient descent:\n\nalpha = 0.01\nepochs = 5\n\nmodel = Linear()\n\norder = np.arange(n) # order in which we'll visit the data\n\nfor t in range(epochs):\n    np.random.shuffle(order)\n    for i in order: \n1        model.zero_grad()\n2        loss = (model(X[i])-y[i])*(model(X[i])-y[i])\n3        loss.backward()\n4        model.w, model.b = (\n            model.w - alpha*model.w.grad, \n            model.b - alpha*model.b.grad\n        )               \n\n\n1\n\nZero out all previous gradient information in the model\n\n2\n\nCompute the loss on a single data pair \\((x_i, y_i)\\) (we’re writing \\(z^2\\) as \\(z \\cdot z\\) because we haven’t yet implemented powers, only multiplication).\n\n3\n\nCompute the gradients of all the model parameters using automatic differentiation.\n\n4\n\nUpdate the model parameters using gradient descent.\n\n\n\n\nWe’re now able to visualize our model parameters:\n\nplt.scatter(X, y, alpha = 0.5, label = \"data\")\nplt.gca().set(xlabel = \"Predictor\", ylabel = \"Target\", )\n\nx_lin = np.linspace(0, 1, 101)\ny_hat = model.w.data*x_lin + model.b.data\n\nplt.plot(x_lin, y_hat, color = \"black\", label = \"fitted model\")\nl = plt.legend()\n\n\n\n\nLooks ok!"
  },
  {
    "objectID": "lecture-notes/auto-diff.html#from-here-to-torch",
    "href": "lecture-notes/auto-diff.html#from-here-to-torch",
    "title": "Automatic Differentiation and Backpropagation",
    "section": "From Here to Torch",
    "text": "From Here to Torch\nOnce we understand the basic idea of automatic differentiation, it’s not so hard to see what goes in to making a high-performance framework like Torch:\n\nImplement lots and lots of math operations like __add__ and __mul__, with their corresponding _backwards methods.\nDo these implementations for arrays rather than just numbers.\nDo those implementations in speedy, low-level code that can run on specialized hardware.\nAdd various utility functions to make it easy to construct more complicated mathematical functions of the inputs."
  },
  {
    "objectID": "lecture-notes/intro-classification.html",
    "href": "lecture-notes/intro-classification.html",
    "title": "Introduction to Classification and Auditing",
    "section": "",
    "text": "In this lecture, we’ll make our first acquaintance to the classification task. Classification is a form of supervised machine learning. Here’s the big-picture version of the supervised ML task.\nWe are given:\n\nA set of observations of predictor variables. We’ll call the \\(i\\)th such observation \\(\\mathbf{x}_i\\). We write it this way because \\(\\mathbf{x}_i\\) is usually a vector of multiple variables, often called features or covariates. We often collect these observations into a matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), where \\(n\\) is the number of observations and \\(p\\) is the total number of features.\nA set of observations of a single target variable. We’ll call the \\(i\\)th such observation \\(y_i\\). We write it this way because (at least in this course) \\(y_i\\) will always be a scalar number, rather than a vector. We can collect these observations into a (column) vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\).\nWe can refer to a single observation as a pair \\((\\mathbf{x}_i, y_i)\\).\n\nBig picture, the supervised machine learning task is to use \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to find a function \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) with the property that\n\\[\nf(\\mathbf{x}) \\approx y\n\\]\nWhat does it mean for \\[f(\\mathbf{x}) \\approx y\\]? This requires mathematical fleshing-out that we’ll do very soon.\nfor new observations \\((\\mathbf{x}, y)\\). We can think of the function \\(f\\) as an expression of the (unknown) relationship between the features \\(\\mathbf{x}\\) and the target \\(y\\). If we can find an approximation of that pattern, then we’ll have the ability to make predictions.\nWe often use \\(\\hat{y} = f(\\mathbf{x})\\) to denote the predicted value for \\(y\\) based on \\(\\mathbf{x}\\). So, we want to choose \\(f\\) so that \\(\\hat{y} \\approx y\\).\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/intro-classification.html#supervised-learning",
    "href": "lecture-notes/intro-classification.html#supervised-learning",
    "title": "Introduction to Classification and Auditing",
    "section": "",
    "text": "In this lecture, we’ll make our first acquaintance to the classification task. Classification is a form of supervised machine learning. Here’s the big-picture version of the supervised ML task.\nWe are given:\n\nA set of observations of predictor variables. We’ll call the \\(i\\)th such observation \\(\\mathbf{x}_i\\). We write it this way because \\(\\mathbf{x}_i\\) is usually a vector of multiple variables, often called features or covariates. We often collect these observations into a matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), where \\(n\\) is the number of observations and \\(p\\) is the total number of features.\nA set of observations of a single target variable. We’ll call the \\(i\\)th such observation \\(y_i\\). We write it this way because (at least in this course) \\(y_i\\) will always be a scalar number, rather than a vector. We can collect these observations into a (column) vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\).\nWe can refer to a single observation as a pair \\((\\mathbf{x}_i, y_i)\\).\n\nBig picture, the supervised machine learning task is to use \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\) to find a function \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}\\) with the property that\n\\[\nf(\\mathbf{x}) \\approx y\n\\]\nWhat does it mean for \\[f(\\mathbf{x}) \\approx y\\]? This requires mathematical fleshing-out that we’ll do very soon.\nfor new observations \\((\\mathbf{x}, y)\\). We can think of the function \\(f\\) as an expression of the (unknown) relationship between the features \\(\\mathbf{x}\\) and the target \\(y\\). If we can find an approximation of that pattern, then we’ll have the ability to make predictions.\nWe often use \\(\\hat{y} = f(\\mathbf{x})\\) to denote the predicted value for \\(y\\) based on \\(\\mathbf{x}\\). So, we want to choose \\(f\\) so that \\(\\hat{y} \\approx y\\)."
  },
  {
    "objectID": "lecture-notes/intro-classification.html#classification",
    "href": "lecture-notes/intro-classification.html#classification",
    "title": "Introduction to Classification and Auditing",
    "section": "Classification",
    "text": "Classification\nWe use the vector \\(\\mathbf{y}\\) to hold our observations of the target variable. We have assumed that each observation of the target variable is a real number (i.e. an element of \\(\\mathbb{R}\\)). This looks reasonable for when the thing we want to predictive is a real number (like a stock price or a probability to like a post), but what about when we want to predict a categorical label? In this case, we can simply encode labels using integers: \\(0\\) for one category, \\(1\\) for the next category, \\(2\\) for the one after that, and so on."
  },
  {
    "objectID": "lecture-notes/intro-classification.html#the-compas-recidivism-prediction-algorithm",
    "href": "lecture-notes/intro-classification.html#the-compas-recidivism-prediction-algorithm",
    "title": "Introduction to Classification and Auditing",
    "section": "The COMPAS Recidivism Prediction Algorithm",
    "text": "The COMPAS Recidivism Prediction Algorithm\nCriminal recidivism occurs when a person is convicted of a crime, completes the legal terms of their punishment, and is then convicted of another crime after release. In the American penal system, predictions of recidivism play a role in determining whether or not a defendant will be released on bail before trial or granted parole after serving a portion of a prison sentence. In other words, the belief of the court about whether a person is likely to commit a future crime can have concrete consequences for that person’s current and future freedom. Of course, it’s difficult for a human to predict whether a defendant is likely to commit a future crime. Furthermore, humans are subject to bias. Wouldn’t it be nice if we could use a machine learning algorithm to make this prediction for us?\nIn 2016, the journalism website ProPublica published an investigative story on COMPAS, a machine learning algorithm used to predict recidivism in Broward County, Florida. They obtained data for criminal defendants in Broward County in the years 2013 and 2014. These data include the COMPAS predictions, as well as demographic information (like age, gender, and race) and legal information (e.g. the crime with which the defendant was charged). The data also include an indicator of whether or not the defendant went on to be arrested of a crime within the two years following their initial trial.\nThe COMPAS algorithm actually uses information about the defendant beyond what is shown in this table; here is an example of the survey used for COMPAS to form its prediction.\n\n\n\n\n\n\nActivity\n\n\n\nHere are three concepts:\n\nDemographic data and legal information related to a defendant.\nWhether or not the defendant proceeds to be arrested for a crime within the two years following their initial trial.\nThe COMPAS algorithm.\n\nMatch these three concepts to the three mathematical symbols in the relationship\n\\[f(\\mathbf{x}) \\approx y\\].\n\n\nLet’s look at an excerpt of the data that ProPublica obtained. I have chosen only a subset of the columns and I have filtered out some of the rows as well. The hidden code saves the data in a pandas.DataFrame called df, and then views it.\nClick the little arrow to the right to view the code I used to display this table.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_csv(\"https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/compas-scores-two-years.csv\")\n\n# filtering as in the original analysis by ProPublica\n# https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb\n\ndf = df[df.days_b_screening_arrest &lt;= 30]\ndf = df[df.days_b_screening_arrest &gt;= -30]\ndf = df[df.is_recid != -1]\ndf = df[df.c_charge_degree != \"O\"]\ndf = df[df.score_text != \"NA\"]\ndf = df[(df.race == \"African-American\") | (df.race == \"Caucasian\")]\n\ncol_list = df.columns\n\ndf[\"compas_prediction\"] = 1*(df.score_text != \"Low\")\ndf = df.reset_index()\ncols = [\"race\", \"age\", \"compas_prediction\", \"two_year_recid\"]\ndf = df[cols]\n\ndf\n\n\n\n\n\n\n\n\n\nrace\nage\ncompas_prediction\ntwo_year_recid\n\n\n\n\n0\nAfrican-American\n34\n0\n1\n\n\n1\nAfrican-American\n24\n0\n1\n\n\n2\nCaucasian\n41\n1\n1\n\n\n3\nCaucasian\n39\n0\n0\n\n\n4\nCaucasian\n27\n0\n0\n\n\n...\n...\n...\n...\n...\n\n\n5273\nAfrican-American\n30\n0\n1\n\n\n5274\nAfrican-American\n20\n1\n0\n\n\n5275\nAfrican-American\n23\n1\n0\n\n\n5276\nAfrican-American\n23\n0\n0\n\n\n5277\nAfrican-American\n33\n0\n0\n\n\n\n\n5278 rows × 4 columns\n\n\n\nThe column compas_prediction is the COMPAS algorithm’s prediction of whether the individual will be arrested again.\n\n0 means “no:” according to COMPAS, the individual does not have an elevated risk to be arrested for a crime within the next two years.\n1 means “yes:” according to COMPAS, the individual does have an elevated risk to be arrested for a crime within the next two years.\n\nwhere 0 means The column two_year_recid records the actual outcome: 0 means “no,” the individual was not arrested within the next two years, while 1 means “yes,” the individual was arrested within the next two years.\nThere are a number of other columns that I have omitted, including the defendant name, the severity of the criminal charge, whether or not the charge is for a violent crime, presence of a prior record, sex, and other information.\n\n\n\n\n\n\nDiscussion\n\n\n\nTake some time to look at the excerpted data set and my description of it. What questions do you have when you look at the data? Try to find at least two questions about:\n\nHow the data was collected/gathered/presented by me.\nWhat patterns might be present in the data? What concerns might you have that you would want to check?"
  },
  {
    "objectID": "lecture-notes/intro-classification.html#evaluating-classification-algorithms",
    "href": "lecture-notes/intro-classification.html#evaluating-classification-algorithms",
    "title": "Introduction to Classification and Auditing",
    "section": "Evaluating Classification Algorithms",
    "text": "Evaluating Classification Algorithms\nWas COMPAS successful at making its predictions? There are lots of ways to assess this.\n\nOverall Accuracy\nOne way is the overall accuracy of the predictions: how often was it the case that the predictions were correct? The code below computes the proportion of the time that the COMPAS prediction matched reality:\n\ndf[\"accurate\"] = df[\"compas_prediction\"] == df[\"two_year_recid\"]\ndf[\"accurate\"].mean()\n\n0.6582038651004168\n\n\nIs this a good result? We can compare it to the performance of a hypothetical algorithm that simply always predicted that the individual would not reoffend.\n\n\nCode\n(1-df[\"two_year_recid\"]).mean()\n\n\n0.5295566502463054\n\n\nThis is an example of comparing against a base rate. There’s no formal definition of a base rate, but you can think of it as the performance of the best approach to the problem that doesn’t involve anything fancy. Here, the base rate is 53% and the accuracy of COMPAS is 66%, indicating that the COMPAS algorithm is significantly outperforming the base rate."
  },
  {
    "objectID": "lecture-notes/intro-classification.html#classification-rates",
    "href": "lecture-notes/intro-classification.html#classification-rates",
    "title": "Introduction to Classification and Auditing",
    "section": "Classification Rates",
    "text": "Classification Rates\nWhile accuracy is a useful metric for classification problems, it’s useful to break it down in more detailed ways. In the case of binary classification, there are four cases:\nRecall that \\(\\hat{y}\\) is just another name for \\(f(\\mathbf{x})\\), the predicted value of \\(y\\) based on \\(\\mathbf{x}\\).\n\nIf \\(y = 1\\) and \\(\\hat{y} = 1\\), we have a true positive.\nIf \\(y = 0\\) and \\(\\hat{y} = 1\\), we have a false positive.\nIf \\(y = 1\\) and \\(\\hat{y} = 0\\), we have a false negative.\nIf \\(y = 0\\) and \\(\\hat{y} = 0\\), we have a true negative.\n\nThe false positive rate is the fraction of all negative events for which the prediction is positive:\nHere \\(\\mathbb{1}\\) is the indicator function that is 1 if its arguments all evaluate to true and 0 otherwise.\n\\[\\mathrm{FPR}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\sum_{i=1}^n \\mathbb{1}(\\hat{y}_i = 1, y_i = 0)}{\\sum_{i = 1}^n \\mathbb{1}(y_i = 0)}\\]\nWe can calculate the FPR like this: numpy boolean arrays and pandas boolean columns can be multiplied to do entrywise Boolean and.\n\n\nCode\ndef FPR(y, y_hat):\n    return sum((y_hat == 1)*(y == 0))/sum(y == 0) \n\n\nOne can also define the False Negative Rate, True Positive Rate, and True Negative Rate. Let’s also do the False Negative Rate:\n\n\nCode\ndef FNR(y, y_hat):\n    return sum((y_hat == 0)*(y == 1))/sum(y == 1) \n\n\n\n\nCode\ny = df[\"two_year_recid\"]\ny_hat = df[\"compas_prediction\"]\nFPR(y, y_hat), FNR(y, y_hat)\n\n\n(0.3302325581395349, 0.35481272654047524)\n\n\nIn other words:\n\nOf people who were not arrested within two years, the COMPAS algorithm wrongly predicted that 33% of them would be arrested within two years (but correctly predicted that 67% of them would not be).\nOf people who were arrested within two years, the COMPAS algorithm wrongly predicted that 35% of them would not be arrested within two years (but correctly predicted that 65% of them would be).\n\n\n\nCode\nfor label, fun in {\"False positive rates\": FPR, \"False negative rates\" : FNR}.items():\n    print(label)\n    print(df.groupby(\"race\").apply(lambda df: fun(df[\"two_year_recid\"], df[\"compas_prediction\"])))\n    print(\"\")\n\n\nFalse positive rates\nrace\nAfrican-American    0.423382\nCaucasian           0.220141\ndtype: float64\n\nFalse negative rates\nrace\nAfrican-American    0.284768\nCaucasian           0.496350\ndtype: float64"
  },
  {
    "objectID": "lecture-notes/auto-diff-live.html",
    "href": "lecture-notes/auto-diff-live.html",
    "title": "Autodiff Linear Regression",
    "section": "",
    "text": "The most important method of this class, which handles backwards navigation along the computational graph, is the backward method:\nThe backward method handles the recursive logic of automatic differentiation. However, it doesn’t do any of the actual math. We need to implement this math within each of the arithmetic operations that we are going to implement for the Value class. Here’s how we implement addition:\nLet’s do another math operation. This operation is very similar, but with a slightly more complex _backward method that reflects the chain rule from calculus.\ndef __mul__(self, other):\n    other = other if isinstance(other, Value) else Value(other)\n    out = Value(self.data * other.data, (self, other))\n\n    def _backward():\n        self.grad += other.data * out.grad\n        other.grad += self.data * out.grad\n    out._backward = _backward\n\n    return out\n\nValue.__mul__ = __mul__\nHaving defined addition and multiplication, we can also pretty quickly define subtraction:\ndef __neg__(self): return self * -1\ndef __sub__(self, other):  return self + (-other)\n\nValue.__neg__ = __neg__\nValue.__sub__ = __sub__\nLet’s see an example of this in action. We can define the function \\(f(x) = (x + 2)^2 + x^3\\). Let’s do this and compute \\(f(3)\\):\nNow here’s the thing: because we can represent \\(f\\) in terms of multiplications and additions, we can also calculate \\(f'(3)\\), just by running the backward method. Since\n\\[\nf'(x) = 2(x+2) + 3x^2\\;,\n\\]\nwe are expecting that \\(f'(3) = 2(3+2) + 3\\cdot 3^2 = 37\\). Let’s check:\nLooks good!\nWe’ve implemented enough automatic differentiation that we can differentiation any function that can be constructed out of a combination of additions, subtractions, and multiplications. This is enough to do linear regression with stochastic gradient descent! We’ll focus on the 1-dimensional version, in which we want to minimize \\[\n\\mathcal{L}(w, b) = \\frac{1}{n}\\sum_{i = 1}^n (wx_i + b - y_i)^2\\;.\n\\]\nIn stochastic gradient descent, we don’t actually need to evaluate all of these terms at once: instead, we can just evaluate (and differentiate) \\[\n\\mathcal{L}_i(w, b) =  (wx_i + b - y_i)^2\\;.\n\\]\nHere’s the computational graph describing the loss function:\nflowchart LR\n    subgraph inputs\n    w\n    b\n    end\n\n    w & x_i --&gt; *\n    b & * --&gt; +\n    + & y_i --&gt; -\n    - --&gt; m[*]\n    - --&gt; m[*]\nIn order to implement this with automatic differentiation, let’s first implement the predictor model \\(f(x) = wx + b\\) as a class, using\nNow that we’ve implemented this model, we’re already to train it. First let’s create some random data:\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nw, b = np.random.rand(), np.random.rand()\n\nn = 100\nX = np.random.rand(n)\ny = w*X + b + 0.05*np.random.randn(n)\n\nplt.scatter(X, y)\nplt.gca().set(xlabel = \"Predictor\", ylabel = \"Target\")\nAnd now let’s do stochastic gradient descent:\nalpha = 0.01\nepochs = 5\n\nmodel = Linear()\n\norder = np.arange(n) # order in which we'll visit the data\n\nfor t in range(epochs):\n    np.random.shuffle(order)\n    # stochastic gradient descent implementation\nWe’re now able to visualize our model parameters:\nplt.scatter(X, y, alpha = 0.5, label = \"data\")\nplt.gca().set(xlabel = \"Predictor\", ylabel = \"Target\", )\n\nx_lin = np.linspace(0, 1, 101)\ny_hat = model.w.data*x_lin + model.b.data\n\nplt.plot(x_lin, y_hat, color = \"black\", label = \"fitted model\")\nl = plt.legend()\nLooks ok!\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/auto-diff-live.html#from-here-to-torch",
    "href": "lecture-notes/auto-diff-live.html#from-here-to-torch",
    "title": "Autodiff Linear Regression",
    "section": "From Here to Torch",
    "text": "From Here to Torch\nOnce we understand the basic idea of automatic differentiation, it’s not so hard to see what goes in to making a high-performance framework like Torch:\n\nImplement lots and lots of math operations like __add__ and __mul__, with their corresponding _backwards methods.\nDo these implementations for arrays rather than just numbers.\nDo those implementations in speedy, low-level code that can run on specialized hardware.\nAdd various utility functions to make it easy to construct more complicated mathematical functions of the inputs."
  },
  {
    "objectID": "lecture-notes/regression.html",
    "href": "lecture-notes/regression.html",
    "title": "Least-Squares Linear Regression",
    "section": "",
    "text": "$$\n$$\nSo far in this course, we’ve focused exclusively on classification tasks: how to predict a categorical label for each data point. The other important task we need to consider is regression, in which we predict a real number for each data point based on its features. Here’s the stereotypical example:\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nw0 = -0.5\nw1 =  0.7\n\nn = 100\nx = np.random.rand(n, 1)\ny = w1*x + w0 + 0.1*np.random.randn(n, 1)\n\nplt.scatter(x, y)\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nLooking at this data, we can see an apparent linear trend that we would like to use in order to make prediction on new data points.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/regression.html#mathematical-formulation",
    "href": "lecture-notes/regression.html#mathematical-formulation",
    "title": "Least-Squares Linear Regression",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nWe’re going to focus on least-squares linear regression. The nice thing about least-squares linear regression is that it falls perfectly into our framework of convex linear models. In least-squares linear regression, we still make predictions of the form \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\), since these are exactly linear predictions! The loss function is \\(\\ell(\\hat{y}, y) = (\\hat{y} - y)^2\\), the squared error, which is convex. The empirical risk minimization problem is\n\\[\n\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) \\\\\n          &= \\sum_{i = 1}^n \\ell(\\hat{y}_i, y_i) \\\\\n          &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\sum_{i = 1}^n \\left(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - y_i \\right)^2\\;.\n\\end{aligned}\n\\] It’s useful to write this in a more compact way using matrix-vector notation: the loss function \\(L(\\mathbf{w})\\) can be written\n \\[\nL(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2\\;.\n\\]Reminder: \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times p}\\), \\(\\mathbf{w}\\in \\mathbb{R}^{p}\\), \\(\\mathbf{X}\\mathbf{w}\\in \\mathbb{R}^n\\), which is the same dimension as \\(\\mathbf{y}\\).\nSo, we want to solve the problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2\\;.\n\\tag{1}\\]"
  },
  {
    "objectID": "lecture-notes/regression.html#solution-methods",
    "href": "lecture-notes/regression.html#solution-methods",
    "title": "Least-Squares Linear Regression",
    "section": "Solution Methods",
    "text": "Solution Methods\nThere are a lot of ways to solve Equation 1. Let’s start by taking the gradient with respect to \\(\\hat{\\mathbf{w}}\\). Using the multivariate chain rule, this is\n\\[\n\\nabla L(\\mathbf{w}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\tag{2}\\]\nOne way to approach the linear regression problem is with gradient descent: repeat the iteration\n\\[\n\\mathbf{w}^{(t+1)} \\gets \\mathbf{w}^{(t)} - 2\\alpha \\mathbf{X}^T(\\mathbf{X}\\mathbf{w}^{(t)} - \\mathbf{y})\n\\]\nto convergence. As it turns out, there’s also an explicit formula involving a matrix inversion that we can obtain by using the condition \\(\\nabla L(\\mathbf{w}) = \\mathbf{0}\\) which must hold at the minimum. Plugging in our expression for \\(L(\\mathbf{w})\\), we get\n\\[\n\\mathbf{0}= \\mathbf{X}^T(\\mathbf{X}\\hat{\\mathbf{w}} - \\mathbf{y})\\;.\n\\]\nTo start solving for \\(\\hat{\\mathbf{w}}\\), we can move \\(\\mathbf{X}^T\\mathbf{y}\\) to the other side:\n\\[\n\\mathbf{X}^T\\mathbf{X}\\hat{\\mathbf{w}} = \\mathbf{X}^T\\mathbf{y}\\;.\n\\]\n Now, provided that the matrix \\(\\mathbf{X}^T\\mathbf{X}\\) is of full rank, we can multiply both sides by \\((\\mathbf{X}^T\\mathbf{X})^{-1}\\) to obtain \\[\n\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\;,\n\\tag{3}\\] which is an explicit formula for \\(\\hat{\\mathbf{w}}\\).This requires that there are at least \\(p\\) linearly independent rows of \\(\\mathbf{X}\\). In particular, \\(\\mathbf{X}\\) must have at least as many rows as it has columns.\nLet’s see if we can use this to compute predictions for our fake data above. In order for this formula to work, we need to ensure that \\(\\mathbf{X}\\) is padded with a vector of ones.\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nX = pad(x)\n\nNow we can use the formula:\n\nw_hat = np.linalg.inv(X.T@X)@X.T@y\n\nLet’s test this out on our fake data:\n\nplt.scatter(x, y)\n\nx_fake = np.linspace(0, 1, 101)[:,np.newaxis]\nX_fake = pad(x_fake)\nplt.plot(x_fake, X_fake@w_hat, color = \"black\")\nlabels = plt.gca().set(xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\n\n\n\n\nNot bad!\n\n\n\n\n\n\nActivity 1: Computational Complexity of Exact Least-Squares Regression\n\n\n\nMultiplying a \\(k\\times \\ell\\) matrix with an \\(\\ell \\times k\\) matrix using the standard algorithm has time complexity \\(k \\ell^2\\). Inverting a \\(k\\times k\\) matrix (when the inverse exists) has time complexity \\(k^3\\). Left-multiplying a \\(\\k \\times \\ell\\) matrix and an \\(\\ell\\)-vector has time complexity \\(k\\ell\\). The total time complexity of performing these operations is just the sum of the complexities of each individual one.\n\nWith these facts in mind, describe the time complexity of computing \\(\\hat{\\mathbf{w}}\\) using Equation 3 in terms of the number of data points \\(n\\) and the number of features \\(p\\). What would the computational bottleneck when the number of data points \\(n\\) is very large? What about what the number of features \\(p\\) is very large?\n\n\nReminder: \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times p}\\) and \\(\\mathbf{y}\\in \\mathbb{R}^n\\).As a reminder, we are talking about the formula \\(\\hat{\\mathbf{w}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\;,\\).\n\n\n\n\n\n\nActivity 2: Computational Complexity of Gradient Descent\n\n\n\nAs you’ll implement in your blog post, gradient descent can actually be implemented via the iteration\n\\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha (\\mathbf{P}\\mathbf{w}^{(t)} - \\mathbf{q})\\;,\n\\]\nwhere \\(\\mathbf{P}\\) is a \\(p \\times p\\) matrix and \\(\\mathbf{q}\\) is a \\(p\\)-vector.  What is the time complexity of a single iteration of gradient descent?\nOf course, a full analysis of the time complexity of the gradient descent algorithm as a whole requires knowing how many iterations are necessary to achieve acceptable accuracy, which is a much harder problem.\n\n\nThe trick is to precompute some matrix products"
  },
  {
    "objectID": "lecture-notes/regression.html#scoring-linear-models",
    "href": "lecture-notes/regression.html#scoring-linear-models",
    "title": "Least-Squares Linear Regression",
    "section": "Scoring Linear Models",
    "text": "Scoring Linear Models\nA good, simple way to score linear models is by using the loss function directly: smaller values are better! In scikit-learn, regression models are instead scored using an affine transformation of the loss function called the coefficient of determination. The coefficient of determination is 1 when the model fits the data perfectly with no errors. It can be arbitrarily negative (e.g. -8.7) if the model fits the data very poorly.\nFor a quick illustration, here’s the scikit-learn implementation of linear regression:\n\nfrom sklearn.linear_model import LinearRegression\nLR = LinearRegression()\nLR.fit(x, y)\n\n# training score\nLR.score(x, y)\n\n0.8587136683818445\n\n\nLet’s evaluate our model on similar, unseen data:\n\nn = 100\nx_val = np.random.rand(n, 1)\ny_val = w1*x_val + w0 + 0.1*np.random.randn(n, 1)\n\n\nLR.score(x_val, y_val)\n\n0.8290675457730805\n\n\nAs usual, gaps between the training and validation scores suggest the possibility of overfitting, although further investigation is required to see whether improvement on validation data is possible."
  },
  {
    "objectID": "lecture-notes/regression.html#incorporating-features",
    "href": "lecture-notes/regression.html#incorporating-features",
    "title": "Least-Squares Linear Regression",
    "section": "Incorporating Features",
    "text": "Incorporating Features\nSometimes (most of the time), the patterns we are looking for in our data are not actually linear. For example:\n\nx = 2*np.pi*np.random.rand(n, 1)\ny = np.sin(x) + 0.2*np.random.randn(n, 1)\n\nplt.scatter(x, y)\n\n&lt;matplotlib.collections.PathCollection at 0x7fe2eabcf190&gt;\n\n\n\n\n\nJust like in the case of classification, we can use feature maps to learn nonlinear patterns. For example:\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\np = PolynomialFeatures(2)\nPHI = p.fit_transform(x)\nPHI[:5]\n\narray([[1.00000000e+00, 2.38502900e+00, 5.68836335e+00],\n       [1.00000000e+00, 3.26013588e+00, 1.06284859e+01],\n       [1.00000000e+00, 1.60992471e-01, 2.59185759e-02],\n       [1.00000000e+00, 5.25277132e-02, 2.75916065e-03],\n       [1.00000000e+00, 4.18447039e+00, 1.75097924e+01]])\n\n\nThe feature matrix \\(\\Phi\\) now plays the role of \\(\\mathbf{X}\\), and we can use the same formula as earlier:\n\nw_hat = np.linalg.inv(PHI.T@PHI)@PHI.T@y\nw_hat\n\narray([[ 0.93540932],\n       [-0.30331504],\n       [ 0.00414563]])\n\n\nThe predictions are\n\ny_hat = PHI@w_hat\n\nLet’s visualize:\n\nplt.scatter(x, y)\n\n\nx_lin = np.linspace(0, 2*np.pi, 1001)[:,np.newaxis]\nPHI_lin = p.fit_transform(x_lin)\ny_trend = PHI_lin@w_hat\n\nplt.plot(x_lin, y_trend, color = \"black\")\n\n\n\n\nHmmm, this is not a very impressive fit. Let’s wrap this process in a function and do a few experiments.\n\ndef poly_viz(deg, ax):\n    p = PolynomialFeatures(deg)\n    PHI = p.fit_transform(x)\n    w_hat = np.linalg.inv(PHI.T@PHI)@PHI.T@y\n    x_lin = np.linspace(x.min(), x.max(), 1001)[:,np.newaxis]\n    PHI_lin = p.fit_transform(x_lin)\n    y_trend = PHI_lin@w_hat\n    ax.scatter(x, y)\n    ax.plot(x_lin, y_trend, color = \"black\")\n\n\nfig, axarr = plt.subplots(2, 5, figsize = (8, 4))\n\nfor deg, ax in zip(range(1, 11), axarr.ravel()): \n    poly_viz(deg, ax)\n    ax.set(title = f\"degree = {deg}\")\n\nplt.tight_layout()\n\n\n\n\nAs in classification, the use of polynomial features makes us susceptible to overfitting, and validation or cross-validation should be used in order to select a good degree.\n\nKernel Regression\nKernel methods offer a theoretically-grounded approach for dealing with nonlinearity in both classification and regression.  In kernel methods, the feature vector corresponding to each data point \\(\\mathbf{x}_i\\) is actually in terms of all the other points:You can implement a kernel classification method in this blog post.\n\\[\n\\phi(\\mathbf{x}_i) = \\left(\\begin{matrix}\n                k(\\mathbf{x}_i, \\mathbf{x}_1) \\\\\n                k(\\mathbf{x}_i, \\mathbf{x}_2) \\\\\n                \\cdots \\\\\n                k(\\mathbf{x}_i, \\mathbf{x}_n) \\\\\n              \\end{matrix}\\right)\n\\]\nHere, \\(k:\\mathbb{R}^{p} \\times \\mathbb{R}^{p} \\rightarrow \\mathbb{R}\\) is a special function called a kernel function. Usually the kernel function is a measure of how similar two points are. A very common and useful kernel is the radial basis function (RBF) kernel\n\\[\nk(\\mathbf{x}_1, \\mathbf{x}_2) = e^{-\\gamma \\lVert \\mathbf{x}_1 - \\mathbf{x}_2 \\rVert^2}\\;.\n\\]\nThis function is largest when \\(\\mathbf{x}_1 = \\mathbf{x}_2\\), and decreases as these two vectors become farther apart. The idea of kernel methods is that the prediction can be expressed as a weighted sum of the target values at nearby points.\nKernel methods have extremely beautiful mathematics behind them and are fun to implement, but for today we can just show the implementation in scikit-learn:\n\nfrom sklearn.kernel_ridge import KernelRidge\n\nKR = KernelRidge(kernel = \"rbf\", gamma = 1)\n\nKR.fit(x, y)\n\nx_lin = np.linspace(x.min(), x.max(), 1001)[:,np.newaxis]\ny_lin = KR.predict(x_lin)\n\nplt.scatter(x, y)\nplt.plot(x_lin, y_lin, color = \"black\")\n\n\n\n\nDifferent values of gamma will result in more or less “wiggly” fits. gamma should be tuned using validation or cross validation in order to protect against overfitting."
  },
  {
    "objectID": "lecture-notes/regression.html#activity",
    "href": "lecture-notes/regression.html#activity",
    "title": "Least-Squares Linear Regression",
    "section": "Activity",
    "text": "Activity\nThe following code produces a random feature matrix \\(\\mathbf{X}\\) and weight vector \\(\\mathbf{w}\\):\n\nX = np.random.rand(10, 3)\nX = pad(X)\nw = np.random.rand(X.shape[1])\n\ny = X@w + np.random.randn(X.shape[0])\n\n\nImplement a predict function that computes y_hat from X and w (hint: don’t overthink it).\nImplement a score function that accepts X, y, and w as arguments and computes the coefficient of determination of the predictions. The coefficient of determination is \\[\nc = 1 - \\frac{\\sum_{i = 1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i = 1}^n (\\bar{y} - y_i)^2}\\;,\n\\] where \\(\\bar{y} = \\frac{1}{n} \\sum_{i = 1}^n y_i\\).\n\nYou can modify these functions and use them in the blog post on linear regression."
  },
  {
    "objectID": "lecture-notes/convex-linear-models.html",
    "href": "lecture-notes/convex-linear-models.html",
    "title": "Convex Linear Models and Logistic Regression",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/convex-linear-models.html#quick-recap",
    "href": "lecture-notes/convex-linear-models.html#quick-recap",
    "title": "Convex Linear Models and Logistic Regression",
    "section": "Quick Recap",
    "text": "Quick Recap\nLast time, we studied the perceptron algorithm for binary classification using hyperplanes. In doing so, we introduced the loss of a hyperplane, which we defined as the number of misclassifications made by the classifier based on that hyperplane.\nWe also saw that the perceptron has some major challenges associated with it. In this lecture, we’re going to extend the idea of loss to cover a broader range of models. Within this theory, we’ll be able to understand where some of the perceptron’s problems come from, and what to do about them.\nRecall that our setup for the perceptron was as follows. We have data, a pair \\((\\mathbf{X}, \\mathbf{y})\\) where\n\n\\(\\mathbf{X}\\in \\mathbb{R}^{n\\times p}\\) is the feature matrix. There are \\(n\\) distinct observations, encoded as rows. Each of the \\(p\\) columns corresponds to a feature: something about each observation that we can measure or infer. Each observation is written \\(\\mathbf{x}_1, \\mathbf{x}_2,\\ldots\\). \\[\n\\mathbf{X}= \\left[\\begin{matrix} & - & \\mathbf{x}_1 & - \\\\\n& - & \\mathbf{x}_2 & - \\\\\n& \\vdots & \\vdots & \\vdots \\\\\n& - & \\mathbf{x}_{n} & - \\end{matrix}\\right]\n\\]\n\\(\\mathbf{y}\\in \\mathbb{R}^{n}\\) is the target vector. The target vector gives a label, value, or outcome for each observation.\n\nIn the perceptron, we assumed that \\(\\mathbf{y}\\in \\{-1, 1\\}^n\\). We also assumed that we are going to try to linearly classify the points by finding a pair \\((\\mathbf{w}, b)\\) that define a hyperplane. This is the set of points \\(\\mathbf{x}\\in \\mathbb{R}^n\\) that satisfy the equation \\[\n\\langle \\mathbf{w}, \\mathbf{x} \\rangle - b = 0\\;.\n\\]\nWe saw that if we redefined \\(\\tilde{\\mathbf{x}} = (\\mathbf{x}, 1)\\) and \\(\\tilde{\\mathbf{w}} = (\\mathbf{w}, -b)\\), we could simply write this as \\[\n\\langle \\tilde{\\mathbf{w}}, \\tilde{\\mathbf{x}} \\rangle = 0\\;\n\\]\ninstead. For the remainder of these notes, we’ll simply write our feature vectors as \\(\\mathbf{x}\\) and our parameter vector as \\(\\mathbf{w}\\), assuming that the final entry of \\(\\mathbf{x}\\) is also a 1.\n\nFor a given point \\(\\mathbf{x}\\), we can make a prediction \\(\\hat{y} = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\).\nWe decide that a prediction is accurate (and give ourself one “point”) if \\(\\hat{y}\\) has the same sign as \\(y\\). This can be expressed in either of the following equivalent ways:\n\n\\(\\mathbb{1}\\left[ \\mathrm{sign}(\\hat{y}) = y \\right]\\)\n\\(\\mathbb{1}\\left[ \\hat{y}y &gt; 0 \\right]\\)\n\nThe overall accuracy or score is the accuracy rate averaged across the entire data set. We also defined the overall loss to be one minus the accuracy: \\[\n\\begin{aligned}\nA(\\mathbf{w}) &= \\frac{1}{n}\\sum_{i = 1}^n \\mathbb{1}\\left[ \\hat{y}_iy_i &gt; 0 \\right]\\\\\n        &= \\frac{1}{n}\\sum_{i = 1}^n \\mathbb{1}\\left[ (\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle)y_i &gt; 0 \\right] \\\\\nL(\\mathbf{w}) &= 1 - A(\\mathbf{w}) \\\\\n        &= \\frac{1}{n}\\sum_{i = 1}^n \\left(1 - \\mathbb{1}\\left[ \\hat{y}_iy_i &gt; 0 \\right]\\right) \\\\\n        &= \\frac{1}{n}\\sum_{i = 1}^n \\left(1- \\mathbb{1}\\left[ (\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle)y_i &gt; 0 \\right]\\right)\\;.\n\\end{aligned}\n\\]\n\nWe’d like to find \\(\\mathbf{w}\\) to minimize the loss function. That is, we’d like to solve the problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w})\n\\tag{1}\\]\nThe loss is also often called the empirical risk, and this minimization problem is often called empirical risk minimization, for reasons that we’ll discuss in a coming lecture. The perceptron algorithm was one way to attack the empirical risk minimization problem."
  },
  {
    "objectID": "lecture-notes/convex-linear-models.html#some-questions-for-empirical-risk-minimization",
    "href": "lecture-notes/convex-linear-models.html#some-questions-for-empirical-risk-minimization",
    "title": "Convex Linear Models and Logistic Regression",
    "section": "Some Questions For Empirical Risk Minimization",
    "text": "Some Questions For Empirical Risk Minimization\nIt is at this point that we need to ask some important questions with awkward answers.\n\nExistence: Does Equation 1 have any solutions?\nUniqueness: Assuming there exists a solution to Equation 1, is it unique? Or are there many different solutions?\nSearchability: Is it possible to write algorithms are guaranteed to find a solution of Equation 1?\nPerformance: Is it possible to make these algorithms fast?\n\nIn most prediction problems, what we’d really like is to be right about the true value of \\(y\\). In the context of linear classifiers, this means that we want all the points of one label to be on one side of the line, and all the points of the other label to be on the other side. The loss function that expresses this idea is the 0-1 loss function, which is, again, the loss function used in the perceptron algorithm: \\[\n\\ell(\\hat{y}, y) = 1 - \\mathbb{1}\\left[ \\hat{y}y &gt; 0 \\right]\\;.\n\\] When we graph the 0-1 loss function, it looks like this. I’ve shown versions corresponding to both values of the true label, \\(y = 1\\) and \\(y = -1\\).\n\n\nCode\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nplt.rcParams[\"figure.figsize\"] = (10, 4)\n\nfig, axarr = plt.subplots(1, 2) \ny_hat = np.linspace(-1, 1, 101)\n\nloss = lambda y_hat, y: 1 - 1*(y_hat*y &gt; 0)\n\nfor j in range(2):\n    y = [-1, 1][j]\n    axarr[j].set_title(f\"y = {y}\")\n    axarr[j].set(xlabel = r\"$\\hat{y}$\", \n                 ylabel = r\"$\\ell(\\hat{y}, y)$\")\n    \n    axarr[j].plot(y_hat, loss(y_hat, y))\n\nplt.tight_layout()\n\n\n\n\n\nLet’s now ask our four questions about empirical risk minimization for the 0-1 loss function.\nExistence: Does Equation 1 have any solutions?\n\nWe are good on this one! Specifically, the risk can take on only a finite number of possible values (values between 0 and 1 in increments of \\(1/n\\)). In any given problem there is a smallest such value obtained, and this is a solution.\n\nUniqueness: Assuming there exists a solution to Equation 1, is it unique? Or are there many different solutions?\nTo define “usually” and “just a little bit” rigorously, we need to specify a data generating distribution and do some math.\n\nUnfortunately, the solution to Equation 1 for the 0-1 loss is almost never unique. This is because, if you have one solution \\(\\mathbf{w}\\), you can “usually” jiggle it by “just a little bit” and still have a minimizing solution.\n\nSearchability: Is it possible to write algorithms are guaranteed to find a solution of Equation 1?\n\nTechnically, we could just try a very large number of choices of \\(\\mathbf{w}\\) and hope for the best, but that’s not very efficient (in fact, the problem of getting a reasonable answer this way is exponential in \\(d\\), the number of features). Can we do better?\n\nPerformance: Is it possible to make these algorithms fast?\n\nThis is where our real problem lies:\n\n\n\n\n\n\n\n\nTheorem 1 (0-1 Minimization for Linear Classifiers is NP Hard (Kearns, Schapire, and Sellie (1994))) Unless P = NP, there is no polynomial-time algorithm that can solve the 0-1 empirical risk minimization problem for linear classifiers.\n\n\n\n\nSo, if we are going to have reasonable algorithms for empirical risk minimization, we need to choose a different loss function. There are multiple choices. Before we jump into examples, we’re going to define the core mathematical concept that is going to help address our core questions of existence, uniqueness, searchability, and performance."
  },
  {
    "objectID": "lecture-notes/convex-linear-models.html#convexity",
    "href": "lecture-notes/convex-linear-models.html#convexity",
    "title": "Convex Linear Models and Logistic Regression",
    "section": "Convexity",
    "text": "Convexity\n\n\n\n\n\n\n\nDefinition 1 A set \\(S \\subseteq \\mathbb{R}^n\\) is convex if, for any two points \\(\\mathbf{z}_1, \\mathbf{z}_2 \\in S\\) and for any \\(\\lambda \\in [0,1]\\), the point \\(\\mathbf{z}= \\lambda \\mathbf{z}_1 + (1-\\lambda) \\mathbf{z}_2\\) is also an element of \\(S\\).\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Convex Functions) Let \\(S \\subseteq \\mathbb{R}^n\\) be convex. A function \\(f:S \\rightarrow \\mathbb{R}\\) is convex if, for any \\(\\lambda \\in \\mathbb{R}\\) and any two points \\(\\mathbf{z}_1, \\mathbf{z}_2 \\in S\\), we have\n\\[\nf(\\lambda \\mathbf{z}_1 + (1-\\lambda)\\mathbf{z}_2) \\leq \\lambda f( \\mathbf{z}_1 ) + (1-\\lambda)f(\\mathbf{z}_2)\\;.\n\\]\nThe function \\(f\\) is strictly convex if the inequality is strict: for all \\(\\lambda\\), \\(\\mathbf{z}_1\\), and \\(\\mathbf{z}_2\\),\n\\[\nf(\\lambda \\mathbf{z}_1 + (1-\\lambda)\\mathbf{z}_2) &lt; \\lambda f( \\mathbf{z}_1 ) + (1-\\lambda)f(\\mathbf{z}_2)\\;.\n\\]\n\n\n\n\nRoughly, a convex function is “bowl-shaped.”\n\n\n\n\n\n\n\nDefinition 3 (Local and Global Minimizers) A point \\(\\mathbf{z}\\in S\\) is a global minimizer of the function \\(f:S \\rightarrow \\mathbb{R}\\) if \\(f(\\mathbf{z}) \\leq f(\\mathbf{z}')\\) for all \\(\\mathbf{z}' \\in S\\).\nA point \\(\\mathbf{z}\\in S\\) is a local minimizer of \\(f:S \\rightarrow \\mathbb{R}\\) if there exists a neighborhood \\(T \\subseteq S\\) containing \\(\\mathbf{z}\\) such that \\(\\mathbf{z}\\) is a global minimizer of \\(f\\) on \\(T\\).\n\n\n\n\nIt’s ok if you don’t know what it means for a set to be closed – all the convex functions we will care about in this class will either be defined on sets where this theorem holds or will be otherwise defined so that the conclusions apply.\n\n\n\n\n\n\n\nTheorem 2 Let \\(f:S \\rightarrow \\mathbb{R}\\) be a convex function. Then:\n\nIf \\(S\\) is closed and bounded, \\(f\\) has a minimizer \\(\\mathbf{z}^*\\) in \\(S\\).\nFurthermore, if \\(\\mathbf{z}^*\\) is a local minimizer of \\(f\\), then it is also a global minimizer.\nIf in addition \\(f\\) is strictly convex, then this minimizer is unique.\n\n\n\n\n\n\nProof. The proof of item 1 needs some tools from real analysis. The short version is:\n\nEvery convex function is continuous.\nIf \\(S\\subseteq \\mathbb{R}^n\\) is closed and bounded, then it is compact.\nContinuous functions achieve minimizers and maximizers on compact sets.\n\nIt’s ok if you didn’t follow this! Fortunately the second part of the proof is one we can do together. Suppose to contradiction that \\(\\mathbf{z}^*\\) is a local minimizer of \\(f\\), but that there is also a point \\(\\mathbf{z}'\\) such that \\(f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)\\). Since \\(\\mathbf{z}^*\\) is a local minimizer, we can find some neighborhood \\(T\\) containing \\(\\mathbf{z}^*\\) such that \\(\\mathbf{z}^*\\) is a minimizer of \\(f\\) on \\(T\\). Let \\(\\lambda\\) be some very small number and consider the point \\(\\mathbf{z}= \\lambda \\mathbf{z}' + (1-\\lambda)\\mathbf{z}^*\\). Specifically, choose \\(\\lambda\\) small enough so that \\(\\mathbf{z}\\in T\\) (since this makes \\(\\mathbf{z}\\) close to \\(\\mathbf{z}^*\\)). We can evaluate\n\\[\n\\begin{align}\nf(\\mathbf{z}) &= f(\\lambda \\mathbf{z}' + (1-\\lambda)\\mathbf{z}^*) \\tag{definition of $\\mathbf{z}$}\\\\\n       &\\leq \\lambda f(\\mathbf{z}') + (1-\\lambda)f(\\mathbf{z}^*)  \\tag{$f$ is convex} \\\\\n       &= f(\\mathbf{z}^*) + \\lambda (f(\\mathbf{z}') - f(\\mathbf{z}^*)) \\tag{algebra}\\\\\n       &&lt; f(\\mathbf{z}^*)\\;. \\tag{assumption that $f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)$}\n\\end{align}\n\\]\nBut this is a contradiction, since we constructed \\(\\mathbf{z}\\) to be in the neighborhood \\(T\\) where \\(\\mathbf{z}^*\\) is a local minimizer. We conclude that there is no \\(\\mathbf{z}'\\) such that \\(f(\\mathbf{z}') &lt; f(\\mathbf{z}^*)\\), and therefore that \\(\\mathbf{z}^*\\) is a global minimizer.\nThe proof of the third part follows a very similar argument to the proof of the second part!\n\nThere’s two other very important math facts that we need in order to apply convexity to the empirical risk minimization problem for linear models.\nBy induction, it follows that any linear combination of convex functions with positive coefficients is convex.\n\n\n\n\n\n\n\nTheorem 3  \n\nLet \\(f_1\\) and \\(f_2\\) be convex functions with the same domain, and let \\(a\\) and \\(b\\) be nonnegative real numbers. Then, the function \\(f\\) defined by \\(f(\\mathbf{z}) = af_1(\\mathbf{z}) + bf_2(\\mathbf{z})\\) is also convex.\nLet \\(f:\\mathbb{R}^n\\rightarrow \\mathbb{R}\\) be convex. Let \\(\\mathbf{A}\\in \\mathbb{R}^{n\\times p}\\) and \\(\\mathbf{b} \\in \\mathbb{R}^n\\). Then, the function \\(f_\\mathbf{A}\\) defined by \\(f_{\\mathbf{A},\\mathbf{b}}(\\mathbf{z}) = f(\\mathbf{A}\\mathbf{z}- \\mathbf{b})\\) is convex."
  },
  {
    "objectID": "lecture-notes/convex-linear-models.html#convexity-and-empirical-risk-minimization",
    "href": "lecture-notes/convex-linear-models.html#convexity-and-empirical-risk-minimization",
    "title": "Convex Linear Models and Logistic Regression",
    "section": "Convexity and Empirical Risk Minimization",
    "text": "Convexity and Empirical Risk Minimization\nLet’s finally go back to the empirical risk minimization problem for linear models. We’re going to write it in terms of a general loss function \\(\\ell\\); the choice \\(\\ell(\\hat{y}, y) = 1 - \\mathbb{1}\\left[ \\hat{y}y &gt; 0 \\right]\\) gets us back to the 0-1 loss situation. The general empirical risk minimization problem for linear classifiers is\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)\\;.\n\\tag{2}\\]\nLet’s now assume that the loss function \\(\\ell\\) is strictly convex in its first argument: that is, for any possible value of \\(y\\) and any \\(\\lambda \\in [0,1]\\), \\[\n\\ell(\\lambda \\hat{y}_1 + (1-\\lambda)\\hat{y}, y) \\leq  \\lambda \\ell(\\hat{y}_1, y) + (1-\\lambda)\\ell(\\hat{y}, y)\\;.\n\\] Then, suddenly the following things would all also be true:\n\n\\(\\ell(\\langle \\mathbf{w}, \\mathbf{x} \\rangle, y)\\) is strictly convex as a function of \\(\\mathbf{w}\\) (Theorem 3, part 2).\nThe empirical risk \\(L(\\mathbf{w}) = \\frac{1}{n}\\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, y_i)\\) is strictly convex as a function of \\(\\mathbf{w}\\) (Theorem 3, part 1).\nIf the empirical risk \\(R(\\mathbf{w})\\) has a global minimizer, that global minimizer is unique (Theorem 2, part 3).\nThe empirical risk \\(R(\\mathbf{w})\\) has no local minimizers which are not global minimizers.\n\nThese facts have important implications for our fundamental questions on empirical risk minimization.\nExistence. Even convex functions are not guaranteed to have minimizers. However, there are lots of choices of loss function \\(\\ell\\) which do guarantee that the empirical risk has a minimizer.\nUniqueness: When the empirical risk is strictly convex, there can only be one global minimizer.\nSearchability: When the empirical risk is strictly convex, there are also no local minimizers other than the global minimizer. Algorithmically, this is the most important property of convexity. It means that if I manage to find any local minimizer at all, that point must be the global minimizer.\n Performance: Convexity significantly reduces the difficulty of our task: instead of trying to find “the best” solution, it’s sufficient for us to find any local optimum. This means that we can design our algorithms to be “greedy local minimizer hunters.” There are lots of fast algorithms to do this. An especially important class of algorithms are gradient descent methods, which we’ll discuss soon.If you’ve taken an algorithms class, one way of thinking of convexity is that it guarantees that greedy methods work for solving minimization problems."
  },
  {
    "objectID": "lecture-notes/convex-linear-models.html#demo-logistic-regression",
    "href": "lecture-notes/convex-linear-models.html#demo-logistic-regression",
    "title": "Convex Linear Models and Logistic Regression",
    "section": "Demo: Logistic Regression",
    "text": "Demo: Logistic Regression\nLet’s do a partial implementation of logistic regression to illustrate these techniques. In logistic regression, we assume that \\(y \\in \\{0,1\\}\\). Our loss function is the logistic loss:\n\\[\n\\ell(\\hat{y}, y) = -y \\log \\sigma(\\hat{y}) - (1-y)\\log (1-\\sigma(\\hat{y}))\\;,\n\\]\nwhere \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the logistic sigmoid function.\nThe logistic loss is convex in \\(\\hat{y}\\), although proving this requires a little bit of extra math that we won’t discuss. Here’s a “proof by picture” for the case when the label is \\(y = 1\\).\n\n\nCode\nz = np.linspace(0, 5, 101)\nplt.plot(z, -np.log(1/(1 + np.exp(-z)))) \nlabs = plt.gca().set(xlabel = r\"$\\hat{y}$\", ylabel = r\"$-\\log \\sigma(\\hat{y})$\")\n\n\n\n\n\nBecause the logistic loss is convex in \\(\\hat{y}\\), the empirical risk minimization problem can have at most one minimum. In fact, it’s possible to show, if the data is not linearly separable, there exists a global minimum.\nHere is some sample data for which we will try to find a good linear classifier.\n\n\nCode\nfrom sklearn.datasets import make_blobs\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nNote that this data is not linearly separable. The perceptron algorithm wouldn’t even have converged for this data set, but logistic regression will do great.\n\n\n\n\nNow let’s do an implementation. First let’s define a linear predictor function of the form \\(f(\\mathbf{x}) = \\langle w, \\mathbf{x} \\rangle\\). Note that this predictor makes the predictions on all the training data at once!\n\nimport numpy as np \nfrom scipy.optimize import minimize\n\n# logistic regression tends to involve a lot of log(0) and things that wash out in the end. \nnp.seterr(all='ignore') \n\n# add a constant feature to the feature matrix\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef predict(X, w):\n    return X@w\n\nNow we’ll define some functions to compute the empirical risk:\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# returns a vector containing the per-observation logistic loss for each observation\ndef logistic_loss(y_hat, y): \n    return -y * np.log(sigmoid(y_hat)) - (1 - y)*np.log(1 - sigmoid(y_hat))\n\n# first compute the predictions, then compute the average loss per observation\n# note that this works on the ENTIRE DATA SET AT ONCE: no for-loops\ndef empirical_risk(X, y, w, loss):\n    y_hat = predict(X, w)\n    return loss(y_hat, y).mean()\n\nFinally, we can write the function that will solve the empirical risk minimization problem for us. We’re going to use the scipy.optimize.minimize function, which is a built-in function for solving minimization problems. Soon, we’ll study how to solve minimization problems from scratch.\nThe scipy.optimize.minimize function requires us to pass it a single function that accepts a vector of parameters, plus an initial guess for the parameters.\n\ndef find_pars(X, y):\n    \n    p = X.shape[1]\n    w0 = np.random.rand(p) # random initial guess\n    \n    # perform the minimization\n    result = minimize(lambda w: empirical_risk(X, y, w, logistic_loss), \n                      x0 = w0) \n    \n    # return the parameters\n    return result.x\n\nOk, let’s try it and take a look at the parameters we obtained. Because the final column of X_ is the constant column of 1s, the final entry of w is interpretable as the intercept term b.\n\nw = find_pars(X_, y)\nw\n\narray([2.33621929, 1.83628124, 0.37074447])\n\n\nAnd, finally, we can plot the linear classifier that we learned.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\n\nplt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\n\n\n\nSince the logistic loss is convex, we are guaranteed that this solution is the unique best solution (as measured by the logistic loss). There is no other possible set of parameters that would lead to a better result (again, as measured by the logistic loss)."
  },
  {
    "objectID": "lecture-notes/perceptron.html",
    "href": "lecture-notes/perceptron.html",
    "title": "Introduction to Classification: The Perceptron",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\newcommand{\\one}[1]{\\mathbb{1}\\left[ #1 \\right]}\n\\newcommand{\\cL}{\\mathcal{L}}\n\\newcommand{\\norm}[1]{\\lVert #1 \\rVert}\n\\]\nIn this lecture, we’ll study one of the oldest machine learning algorithms: the perceptron. Invented in 1943 but not actually implemented in hardware until 1958, the perceptron is still relevant today as a fundamental building-block of modern deep neural networks. Indeed, one of the implementations of neural networks in scikit-learn is still called the “multilayer perceptron.”\nWhen first announced, the perceptron algorithm also displayed one of the first examples of AI Hype®. The New York Times uncritically repeated claims by a Navy rep that the perceptron algorithm would be the “embryo” of a computer that would “walk, talk, see, write, reproduce itself, and be conscious of its existence.” As we study and implement the perceptron, you may wish to reflect on what you are doing and decide for yourself whether you believe that you are building the “embryo” of any such capabilities yourself.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/perceptron.html#illustration",
    "href": "lecture-notes/perceptron.html#illustration",
    "title": "Introduction to Classification: The Perceptron",
    "section": "Illustration",
    "text": "Illustration\nNow let’s go ahead and run the perceptron algorithm on some data. First we should set up our feature matrix \\(\\mX\\) and target vector \\(\\vy\\).\n\n\nCode\nX = np.append(np.column_stack((X1, X2)), np.column_stack((X3, X4)), axis = 0) # feature matrix\ny = 2*(np.arange(0, 200) &gt;= 100) - 1 # target vector\n\n\nHere are the first few rows of the feature matrix:\n\n\nCode\nX[0:5, :]\n\n\narray([[-1.0856306 ,  0.64205469],\n       [ 0.99734545, -1.97788793],\n       [ 0.2829785 ,  0.71226464],\n       [-1.50629471,  2.59830393],\n       [-0.57860025, -0.02462598]])\n\n\nAnd here are the corresponding values of the target vector:\n\n\nCode\ny[0:5]\n\n\narray([-1, -1, -1, -1, -1])\n\n\n\n\nCode\nnp.random.seed(123456)\nw = np.random.rand(3)\n\nplt.rcParams[\"figure.figsize\"] = (8, 4)\nfig, axarr = plt.subplots(2, 5, sharex = True, sharey = True)\nfor ax in axarr.ravel():\n    ax.set(xlim = (-5, 5), ylim = (-5, 5))\n    plot_scatter(X1, X2, X3, X4, ax, legend = False)\n    draw_line(w, -10, 10, ax, color = \"black\", linestyle = \"dashed\")\n    w, i, loss = perceptron_update(X, y, w)    \n    ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\")\n    draw_line(w, -10, 10, ax, color = \"black\")\n    ax.set_title(f\"loss = {loss}\")\n    \nplt.tight_layout()\n\n\n\n\n\nFigure 3: Several iterations of the perceptron algorithm. In each panel, the dashed line is the hyperplane corresponding to the previous weight vector \\(\\vw^{(t)}\\), while the solid line is the hyperplane for the updated weight vector \\(\\vw^{t+1}\\). The empty circle is the point \\(i\\) used in the update; only iterations in which \\(i\\) was a mistake are shown, with the exception of the final two iterations (by which the algorithm has converged). The loss is computed as in Equation 4. (The perceptron update itself takes place using a function called perceptron_update whose implementation I have intentionally hidden – you’ll implement a version yourself in a blog post!)"
  },
  {
    "objectID": "lecture-notes/perceptron.html#convergence-of-the-perceptron-algorithm",
    "href": "lecture-notes/perceptron.html#convergence-of-the-perceptron-algorithm",
    "title": "Introduction to Classification: The Perceptron",
    "section": "Convergence of the Perceptron Algorithm",
    "text": "Convergence of the Perceptron Algorithm\nIs the perceptron algorithm guaranteed to terminate? And if so, is it guaranteed to find a weight vector \\(\\tilde{\\vw}\\) that perfectly separates the two data classes?\n\n\n\n\n\n\n\nDefinition 1 (Linear Separability) A data set with feature matrix \\(\\mX \\in \\R^{n\\times k}\\) and target vector \\(y\\in \\{0, 1\\}\\) is linearly separable if there exists a weight vector \\(\\tilde{\\vw}\\) such that, for all \\(i \\in [n]\\),\n\\[\n\\bracket{\\tilde{\\vw}, \\tilde{\\vx}_i} &gt; 0 \\Leftrightarrow y = 1\\;.\n\\]\n\n\n\n\nTake a moment to convince yourself of the following:\n\n\n\n\n\n\n\nProposition 1 (Nonconvergence of perceptron for nonseparable data) Suppose that \\((\\mX, \\vy)\\) is not linearly separable. Then, the perceptron update does not converge. Furthermore, at no iteration of the algorithm is it the case that \\(\\cL(\\tilde{\\vw}) = 0\\).\n\n\n\n\nIt’s not as obvious that, if the data is linearly separable, then the perceptron algorithm will converge to a correct answer. Perhaps surprisingly, this is also true:\n\n\n\n\n\n\n\nTheorem 1 (Convergence of perceptron for separable data) Suppose that \\((\\mX, \\vy)\\) is linearly separable. Then:\n\nThe perceptron algorithm converges in a finite number of iterations to a vector \\(\\tilde{\\vw}\\) that separates the data.\n\nDuring the running of the perceptron algorithm, the total number of updates made is no more than \\[\\frac{2 + r(\\mX)^2}{\\gamma(\\mX, \\vy)}\\;,\\]\n\nwhere \\(r(\\mX) = \\max_{i \\in [n]} \\norm{\\vx_i}\\) and \\(\\gamma(\\mX, \\vy)\\) is a geometric measure called the margin of how far apart the two label classes are.\n\n\n\n\nFor a proof of Theorem 1, see p. 37-44 of Hardt and Recht (2021)."
  },
  {
    "objectID": "lecture-notes/perceptron.html#close-out-activity",
    "href": "lecture-notes/perceptron.html#close-out-activity",
    "title": "Introduction to Classification: The Perceptron",
    "section": "Close-out Activity",
    "text": "Close-out Activity\n\n\n\n\n\n\nStart Implementing the Perceptron Algorithm\n\n\n\nSuppose that you have a numpy.Array X of features and an np.Array y of binary labels. Assume that X does NOT contain a column of 1s; that is, it corresponds to \\(\\mX\\) rather than \\(\\tilde{\\mX}\\) from the notes above. Additionally, Assume that the labels in y are 0s and 1s rather than -1s and 1s. This is not the mathematically convenient setup, but is the one that is most frequently seen in machine learning software.\nAt the board, write as much code as you can to achieve some of the following tasks. Please work with a partner. It’s ok to pick and choose which of these to try; but you should work together (i.e. in conversation) rather than in parallel.\n\nDetermine n (the number of data points) and p (the number of features) from X.\nModify X into X_ (which contains a column of 1s and corresponds to \\(\\tilde{\\mX}\\)). This one I’ll give you for free:\n\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nModify y into an array y_ of -1s and 1s, corresponding to the version we use in the notes above.\nInitialize a random weight vector w_ with appropriate dimensions, corresponding to \\(\\tilde{\\vw}^{(0)}\\).\nGenerate a random index i between 0 and n-1.\nExtract the ith row of X_, which corresponds to \\(\\tilde{\\vx}_i\\).\n\nCompute \\(\\ell_i^{(0)} = \\bracket{\\tilde{\\vw}^{(0)}, \\tilde{\\vx}_i}\\).\n\nThese items will give you a head start on the blog post in which you will construct a full implementation of the perceptron algorithm."
  },
  {
    "objectID": "lecture-notes/features-regularization.html",
    "href": "lecture-notes/features-regularization.html",
    "title": "Feature Maps, Regularization, and Generalization",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/features-regularization.html#feature-maps",
    "href": "lecture-notes/features-regularization.html#feature-maps",
    "title": "Feature Maps, Regularization, and Generalization",
    "section": "Feature Maps",
    "text": "Feature Maps\nSuppose that we were able to extract from each point its distance from the origin. In 2d, we could take a point \\(\\mathbf{x}\\) and simply compute\n\\[\nr^2 = x_1^2 + x_2^2\\;.\n\\]\nWe could then make the classification based on the value of \\(r^2\\). In this data set, it looks like the classification rule that predicts \\(1\\) if \\(r^2 &lt; 1\\) and \\(0\\) otherwise would be a pretty good one. The important insight here is that this is also a linear model, with linear predictor function\n\\[\n\\hat{y} = \\langle \\mathbf{r}, \\mathbf{w} \\rangle\\;,\n\\]\nand predicted labels \\(\\mathbb{1}[\\hat{y} &lt; 0]\\).\nwhere \\(\\mathbf{r}= (r^2, 1)\\) and \\(\\mathbf{w}= (1, -1)\\). This means that we can use empirical risk minimization for this problem if we just transform the features \\(\\mathbf{X}\\) first! We need to compute a matrix \\(\\mathbf{R}\\) whose \\(i\\)th row is \\(\\mathbf{r}_i = (r^2_i, 1) = (x_{i1}^2 + x_{i2}^2, 1)\\), and then use this matrix in place of \\(\\mathbf{X}\\) for our classification task.\nThe transformation \\((x_1, x_2) \\mapsto (x_1^2 + x_2^2, 1)\\) is an example of a feature map.\n\n\n\n\n\n\n\nDefinition 1 A feature map \\(\\phi\\) is a function \\(\\phi:D \\rightarrow \\mathbb{R}^p\\), where \\(D\\) is the set of possible data values. If \\(d\\in D\\) is a data point, we call \\(\\phi(d) = \\mathbf{x}\\in \\mathbb{R}^p\\) the feature vector corresponding to \\(d\\). For a given feature map \\(\\phi\\), we define the map \\(\\Phi:D^n \\rightarrow \\mathbb{R}^{n\\times p}\\) as\n\\[\n\\Phi(\\mathbf{d}) = \\left(\\begin{matrix}\n     - & \\phi(d_1) & - \\\\\n     - & \\phi(d_2) & - \\\\\n     \\vdots & \\vdots & \\vdots \\\\\n     - & \\phi(d_n) & - \\\\\n\\end{matrix}\\right)\n\\]\nWe’ll often write\n\\[\n\\mathbf{X}= \\Phi(\\mathbf{d})\n\\]\nto say that \\(\\mathbf{X}\\) is the feature matrix for a data set \\(\\mathbf{d}\\).\n\n\n\n\nWe can think of feature maps in two ways:\nFeature maps can represent measurement processes. For example, maybe I am trying to classify penguins by species, based on physiological measurements. The real data is the penguin, and the measurements are how I represent that penguin with numbers. In this case, I might write my feature map as \\[\\phi(🐧) = (\\mathrm{height}, \\mathrm{weight}, \\text{bill length})\\] Here, \\(D\\) is a set of many penguins \\(D = \\{🐧_1, 🐧_2, 🐧_3, 🐧_4, 🐧_5, 🐧_6, 🐧_7\\}\\), and \\(d\\in D\\) is a specific penguin. The process of transforming an object into a vector via a feature map is often called vectorization as well, especially in the context of representing digital data as vectors. We often talk about vectorizing text and images for example; this can be done using feature maps.\nFeature maps can also represent data processing, which is more like our example above. There, we’re taking some data that’s already a vector and turning it into a DIFFERENT vector that we think will be helpful for our learning task."
  },
  {
    "objectID": "lecture-notes/features-regularization.html#feature-maps-and-linear-separability",
    "href": "lecture-notes/features-regularization.html#feature-maps-and-linear-separability",
    "title": "Feature Maps, Regularization, and Generalization",
    "section": "Feature Maps and Linear Separability",
    "text": "Feature Maps and Linear Separability\nWe often think of feature maps as taking us from a space in which the data is not linearly separable to a space in which it is. For example, consider the feature map\n\\[\n(x_1, x_2) \\maps_to (x_1^2, x_2^2)\\;.\n\\]\nThis map is sufficient to express the radius information, since we can represent the radius as\n\\[\nr^2 = \\langle (1, 1), (x_1^2, x_2^2) \\rangle\\;.\n\\]\nLet’s see how this looks. We’ll again show the failed linear separator, and we’ll also show a successful separator in a transformed feature space:\n\nfig, axarr = plt.subplots(1, 2, figsize=(8, 4))\n\nplot_decision_regions(X, y, clf = LR, ax = axarr[0])\nscore = axarr[0].set_title(f\"Accuracy = {LR.score(X, y)}\")\n\nLR2 = LogisticRegression()\n\nX_ = X**2\nLR2.fit(X_, y)\nplot_decision_regions(X_, y, clf = LR2, ax = axarr[1])\nscore = axarr[1].set_title(f\"Accuracy = {LR2.score(X_, y)}\")"
  },
  {
    "objectID": "lecture-notes/features-regularization.html#feature-maps-in-practice",
    "href": "lecture-notes/features-regularization.html#feature-maps-in-practice",
    "title": "Feature Maps, Regularization, and Generalization",
    "section": "Feature Maps in Practice",
    "text": "Feature Maps in Practice\nGoing back to our example of trying to classify the two nested circles, we could just compute the radius. In practice, however, we don’t really know which features are going to be most useful, and so we just compute a set of features. In our case, the square of the radius is an example of a polynomial of degree 2: \\[\nr^2 = x_1^2 + x_2^2\\;.\n\\] So, instead of just assuming that the radius is definitely the right thing to compute, we more frequently just compute all the monomials of degree 2 or lower. If \\(\\mathbf{x}= (x_1, x_2)\\), then this is\n\\[\n\\phi(\\mathbf{x}_i) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)\\;.\n\\]\nWe then use a linear model to solve the empirical risk minimization problem\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{w} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{w}, \\phi(\\mathbf{x}_i) \\rangle, y_i)\\;.\n\\]\nThe important point to keep track of is that the new feature matrix \\(\\mathbf{X}' = \\Phi(\\mathbf{X})\\) has more columns than \\(\\mathbf{X}\\). In this case, for example, \\(\\mathbf{X}\\) had just 2 columns but \\(\\Phi(\\mathbf{X})\\) has 6. This means that \\(\\hat{\\mathbf{w}}\\) has 6 components, instead of 2!\nLet’s now run logistic regression with degree-2 polynomial features on this data set. The most convenient way to make this happen in the scikit-learn framework is with at Pipeline. The Pipeline first applies the feature map and then calls the model during both fitting and evaluation. We’ll wrap the pipeline in a simple function for easy reuse.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(degree, **kwargs):\n    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n                    (\"LR\", LogisticRegression(**kwargs))])\n    return plr\n\ndef viz_plr(plr, X, y):  \n    plot_decision_regions(X, y, clf = plr)\n    score = plt.gca().set_title(f\"Training Accuracy = {plr.score(X, y)}\")  \n\n\nplr = poly_LR(degree = 2)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nLet’s check the coefficients of the model:\n\nplr.named_steps[\"LR\"].coef_.round(2)\n\narray([[-0.  ,  0.06, -0.13, -4.92, -0.07, -4.64]])\n\n\nNotice that two coefficients are much larger than the others, and approximately equal. These are the coefficients for the features \\(x_1^2\\) and \\(x_2^2\\). The fact that these are approximately equal means that our model is very close to using the square radius \\(r^2 = x_1^2 + x_2^2\\) for this data, just like we’d expect. The benefit is that we didn’t have to hard-code that in; the model just detected on its own the right pattern to find.\nPart of the reason this might be beneficial is that for some data sets, we might not really know what specific features we should try. For example, here’s another one where a linear classifier doesn’t do so great (degree 1 corresponds to no transformation of the features).\n\nnp.random.seed(123)\nX, y = make_moons(200, shuffle = True, noise = 0.2)\n\nplr = poly_LR(degree = 1)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nIt’s not as obvious that we should use the radius or any other specific feature for our feature map. Fortunately we don’t need to think too much about it – we can just increase the degree and let the model figure things out:\n\nplr = poly_LR(degree = 5)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nMuch nicer!"
  },
  {
    "objectID": "lecture-notes/features-regularization.html#generalization-feature-selection-regularization",
    "href": "lecture-notes/features-regularization.html#generalization-feature-selection-regularization",
    "title": "Feature Maps, Regularization, and Generalization",
    "section": "Generalization, Feature Selection, Regularization",
    "text": "Generalization, Feature Selection, Regularization\nSo, why don’t we just use as many features as it takes to get perfect accuracy on the training data? Here’s an example where we get perfect accuracy on the training data:\n\nplr = poly_LR(degree = 15, penalty = \"none\", max_iter = 1000000)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nI’ve had to change some parameters to the LogisticRegression in order to ensure that it fully ran the optimization procedure for this many polynomials.\nThe problem here is that, although this classifier might achieve perfect training accuracy, it doesn’t really look like it’s captured “the right” pattern. This means that if we ask it to classify similar new data, it’s unlikely to do as well:\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nWhoops! We have overfit: our model was so flexible that it was able to learn both some real patterns that we wanted it to learn and some noise that we didn’t. As a result, when it made a prediction on new data, the model’s predictions were imperfect, reflecting the noise it learned in the training process.\nIn machine learning practice, we don’t actually want our models to get perfect scores on the training data – we want them to generalize to new instances of unseen data. Overfitting is one way in which a model can fail to generalize.\nLet’s do an experiment in which we see what happens to the model’s generalization ability when we increase the number of polynomial features:\n\nimport pandas as pd\nnp.random.seed()\n\ndegs = range(0, 11)\n\ndf = pd.DataFrame({\"deg\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .4)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .4)\n\n    for deg in degs:\n        plr = poly_LR(degree = deg, penalty = \"none\", max_iter = int(1e3))\n        plr.fit(X_train, y_train)\n        to_add = pd.DataFrame({\"deg\" : [deg],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n\n        \nmeans = df.groupby(\"deg\").mean().reset_index()\n\nplt.plot(means[\"deg\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"deg\"], means[\"test\"], label = \"validation\")\nplt.legend()\nlabs = plt.gca().set(xlabel = \"Degree of polynomial feature\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\n\n\n\nWe observe that there is an optimal number of features for which the model is most able to generalize: around 3 or so. More features than that is actually harmful to the model’s predictive performance.\nSo, one way to promote generalization is to try to find “the right” or “the right number” of features and use them for prediction. This problem is often called feature selection.\nAnother common approach to avoid overfitting is called regularization. In regularization, we actually modify the empirical risk objective function that is to be minimized. Instead of trying to minimize Equation 1, we instead consider the modified objective function \\[\nL'(\\mathbf{w}) = L(\\mathbf{w}) + \\lambda R(\\mathbf{w})\\;,\n\\] where \\(\\lambda\\) is a regularization strength and \\(R(\\mathbf{w})\\) is a regularization function that aims to influence the entries of \\(\\mathbf{w}\\) in some way. Common choices of regularization function include the Euclidean norm \\(R(\\mathbf{w}) = \\lVert \\mathbf{w} \\rVert_2^2\\) and the \\(\\ell_1\\) norm \\(R(\\mathbf{w}) = \\sum_{j = 1}^p \\lvert x_j \\rvert\\). To see regularization in action, let’s go back to our logistic regression model with a large number of polynomial features. We can see the presence of overfitting in the excessive “wiggliness” of the decision boundary.\n\nX, y = make_moons(200, shuffle = True, noise = .3)\n\nplr = poly_LR(degree = 15, penalty = \"none\", max_iter = int(1e5))\n\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nFortunately for us, we can actually use regularization directly from inside the scikit-learn implementation of LogisticRegression. We specify the penalty (the \\(\\ell_1\\) regularization), the strength of the penalty (in the scikit-learn implementation, you specify \\(C = \\frac{1}{\\lambda}\\) so that larger \\(C\\) means less regularization) and the optimization solver (not all solvers work with all penalties).\n\nplr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = 1)\n\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nThis looks more likely to generalize! We can also increase the regularization:\n\nplr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = 0.01)\n\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nor decrease it:\n\nplr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = 100)\n\nplr.fit(X, y)\nviz_plr(plr, X, y)\n\n\n\n\nLike last time, we can conduct a search (often called a grid-search) to find the best value of the regularization strength for a given problem. We’ll hold fixed the number of features, and instead vary the regularization strength:\n\nnp.random.seed()\n\nC = 10.0**np.arange(-4, 5)\n\ndf = pd.DataFrame({\"C\": [], \"train\" : [], \"test\" : []})\n\nfor rep in range(10):\n    X_train, y_train = make_moons(100, shuffle = True, noise = .3)\n    X_test, y_test = make_moons(100, shuffle = True, noise = .3)\n\n    for c in C:\n        plr = poly_LR(degree = 15, penalty = \"l1\", solver = \"liblinear\", C = c)\n\n        plr.fit(X_train, y_train)\n\n        to_add = pd.DataFrame({\"C\" : [c],\n                               \"train\" : [plr.score(X_train, y_train)],\n                               \"test\" : [plr.score(X_test, y_test)]})\n\n        df = pd.concat((df, to_add))\n     \nmeans = df.groupby(\"C\").mean().reset_index()\n\nplt.plot(means[\"C\"], means[\"train\"], label = \"training\")\nplt.plot(means[\"C\"], means[\"test\"], label = \"validation\")\nplt.semilogx()\nplt.legend()\nlabs = plt.gca().set(xlabel = \"C\",\n              ylabel = \"Accuracy (mean over 20 runs)\")\n\n\n\n\nUsing 15 features, it looks like a regularization strength of approximately \\(C = 10\\) is a good choice for this problem."
  },
  {
    "objectID": "lecture-notes/erm.html",
    "href": "lecture-notes/erm.html",
    "title": "Models, Algorithms, and Learning",
    "section": "",
    "text": "$$\n$$\nIn this lecture, we are going to develop our core theoretical framework for supervised prediction. Supervised means that we are trying to predict something for which there is, or could be, a “ground-truth answer.” Some examples of supervised tasks are:\nIn each of these cases, you could find out whether your prediction was right or wrong just by waiting and checking. Did you predict that a person is likely to commit a crime within the next few years? Wait two years and find out whether you were right.\nIn contrast, unsupervised algorithms generate output for which there is no firm right answer. Unsupervised machine learning tasks aim to do things like “find patterns” or “generate realistic examples.” Large language models (LLMs) like ChatGPT are perhaps the most prominent examples of unsupervised models these days.\nBig picture, the goal of supervised learning is to find a function that takes in some features and uses them to make a prediction that is usually right. Heuristically, we’re looking for a function \\(f\\) that accepts some features \\(x_1,\\ldots,x_p\\) and gives a prediction \\(\\hat{y}\\) that is “close” to the real answer \\(y\\):\n\\[\nf(x_1,\\ldots,x_p) = \\hat{y} \\approx y\\;.\n\\]\nWhat does “\\(\\approx\\)” actually mean in this context? Briefly, we mean that \\(\\hat{y}\\) is usually “close” to \\(y\\) when measured in a certain way.\nThe goal of supervised learning is to “train” a “model” that will make “good” “predictions” on “data.” We need to cash out each of these terms.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/erm.html#data-generating-distributions",
    "href": "lecture-notes/erm.html#data-generating-distributions",
    "title": "Models, Algorithms, and Learning",
    "section": "Data Generating Distributions",
    "text": "Data Generating Distributions\nIntuitively, a data generating distribution is an expression of our expectations of what the world looks like. For example, let’s suppose we are trying to predict whether someone enjoys skiing. So, we do a survey in which we ask two questions:\n\nOn a scale from 1 to 10, how much do you enjoy being outdoors in cold weather?\nDo you enjoy skiing? (yes/no)\n\nHere, we have a single feature \\(x\\in [10]\\) and a binary outcome \\(y \\in \\{0,1\\}\\). We might imagine that when \\(x\\) is larger, it is more likely for \\(y\\) to be equal to 1. Here’s a probabilistic model that expresses this idea:\n\\[\n\\begin{align}\n    \\mathbb{P}\\left[X = x\\right] &= 1/10 \\quad \\forall x \\in [10] \\\\\n    \\mathbb{P}\\left[Y = 1 | X\\right] &= \\frac{x}{11}\\;.\n\\end{align}\n\\]\nThis probabilistic model is an example of a data generating distribution. The reason it’s called this is that you could “generate” a data point from it: first pick a random value of \\(x\\) uniformly between 1 and 10. Then, to generate \\(y\\), flip a weighted coin with probability of heads equal to \\(x/11\\).\nHere’s some “data” sampled from this probabilistic model:\n\n\nCode\nimport numpy as np \nfrom matplotlib import pyplot as plt \n\nn = 100\nx = np.random.randint(1, 11, n)\ny = np.random.rand(n) &lt; x/11\n\n\nf = plt.scatter(x, y + 0.2*np.random.rand(n) - 0.1, alpha = 0.5)\nl = plt.xlabel(\"Enjoys cold weather\") \nl = plt.ylabel(\"Enjoys skiing\")\n\n\n\n\n\nFigure 1: Data sampled from the probabilistic model of enjoyment of skiing based on enjoyment of cold weather. The vertical axis has been jittered so for legibility.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Data Generating Distribution) An observation generating distribution is a probability distribution describing the likelihood of a single observation \\((\\mathbf{x}, y)\\). We’ll usually call this distribution \\(p_\\mathcal{D}\\), so that the likelihood of realizing the pair \\((\\mathbf{x}, y)\\) is \\(p_\\mathcal{D}(\\mathbf{x}, y)\\).\nA data generating distribution is a probability distribution describing the likelihood of a feature matrix-target vector pair \\((\\mathbf{X}, \\mathbf{y})\\) with \\(n\\) observations. In this class we’ll always assume that the observations are independent and identically distributed (i.i.d.) according to \\(p_\\mathcal{D}\\), and so the data generating distribution can be written\n\\[\nP_\\mathcal{D}(\\mathbf{X}, \\mathbf{y}) = \\prod_{i = 1}^n p_\\mathcal{D}(\\mathbf{x}_i, y_i)\\;.\n\\]\n\n\n\n\nIf we knew the data distribution, then it would be easy to make good predictions. In the supervised learning framework, we don’t usually assume that we know the probability model, but we do usually assume that there is one. Our job is to help our models find the patterns encoded in the data generating distribution.\nRecall that in the case of the perceptron, we assumed that we were dealing with linearly separable data like the ones shown below:\n\n\nCode\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom hidden.perceptron import perceptron_update, draw_line\n\nnp.random.seed(123)\n\nplt.rcParams[\"figure.figsize\"] = (4, 4)\n\nX1 = np.random.normal(0, 1, 100)\nX2 = np.random.normal(0, 1, 100)\n\nX3 = np.random.normal(0, 1, 100)*.5+3\nX4 = np.random.normal(0, 1, 100)*.5+3\n\nfig, ax = plt.subplots(1, 1)\n\ndef plot_scatter(X1, X2, X3, X4, ax, legend = True):\n    \n    s = ax.scatter(X1, X2, color = \"#ED90A4\", alpha = 0.5, label = r\"$y_i = -1$\")\n    s = ax.scatter(X3, X4, color = \"#00C1B2\", alpha = 0.5, label = r\"$y_i = 1$\")\n    l = ax.set(xlabel = r\"$x_{i1}$\")\n    l = ax.set(ylabel = \"$x_{i2}$\")\n    if legend:\n        l = ax.legend()\n    \nplot_scatter(X1, X2, X3, X4, ax)\n\n\n\n\n\nFigure 2: 200 data points in the 2d plane, each of which has one of two labels.\n\n\n\n\n\n\n\n\n\n\nActivity 1\n\n\n\nCan you write down some ideas for an observation generating distribution that would generate data that looks like this? Feel free to use a weight vector \\(\\mathbf{w}\\), a bias \\(b\\), and anything else you might need."
  },
  {
    "objectID": "lecture-notes/erm.html#empirical-risk-minimization",
    "href": "lecture-notes/erm.html#empirical-risk-minimization",
    "title": "Models, Algorithms, and Learning",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\nWe are now prepared to formulate a fundamental paradigm for prediction problems in machine learning.\n\n\n\n\n\n\n\nDefinition 8 (Empirical Risk Minimization) An empirical risk minimization problem involves:\n\nA choice of model family \\(\\mathcal{M} = \\{f_{\\boldsymbol{\\theta}}\\}\\).\nA choice of loss function \\(\\ell: \\mathbb{R}\\times \\mathbb{R}\\rightarrow \\mathbb{R}\\).\n\nA data set \\((\\mathbf{X}, \\mathbf{y})\\).\n\nThe empirical risk problem is then to find the parameter vector \\(\\boldsymbol{\\theta}\\) (which corresponds to a choice of model) that minimizes the empirical risk:\n\\[\n\\begin{align}\n\\hat{\\boldsymbol{\\theta}} &= \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\theta}} \\; \\hat{R}(f_\\boldsymbol{\\theta})  \\\\\n&= \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol{\\theta}} \\; \\frac{1}{n} \\sum_{i = 1}^n \\ell(f_\\boldsymbol{\\theta}(\\mathbf{x}_i), y_i)\\;.\n\\end{align}\n\\]\n\n\n\n\nA large number of classification and regression algorithms that we study in this course are instances of empirical risk minimization."
  },
  {
    "objectID": "lecture-notes/erm.html#back-to-perceptron",
    "href": "lecture-notes/erm.html#back-to-perceptron",
    "title": "Models, Algorithms, and Learning",
    "section": "Back to Perceptron",
    "text": "Back to Perceptron\nWe are now able to situate the perceptron algorithm within the framework of empirical risk minimization:\n\nThe model family \\(\\mathcal{M}\\) is the set of all functions of the form \\(f_{\\mathbf{w}, b}(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle - b\\). We can take the parameter vector to be \\(\\boldsymbol{\\theta} = (\\mathbf{w}, b)\\).\nThe per-observation loss function used is the so-called 0-1 loss function given by \\[\n\\ell(\\hat{y}, y) = 1 - \\mathbb{1}\\left[ \\hat{y}y &gt; 0 \\right]\\;.\n\\]\nThe empirical risk is \\[\n\\hat{R}(f_{\\mathbf{w}, b}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\hat{y}_i,y_i) = \\sum_{i= 1}^n \\left[1 - \\mathbb{1}\\left[ \\hat{y}_iy_i &gt; 0 \\right]\\right]\\;.\n\\]\nThe perceptron algorithm attempts to find a pair \\((\\mathbf{w}, b)\\) that reduces the empirical risk by updating the parameters using one data point at a time.\nThe perceptron convergence theorem for separable data says that if there exists a pair \\((\\mathbf{w}, b)\\) such that \\(\\hat{R}(f_{\\mathbf{w}, b}) = 0\\), then the perceptron algorithm is guaranteed to find such a pair after a number of steps that can be bounded in terms of the data."
  },
  {
    "objectID": "lecture-notes/erm.html#the-point",
    "href": "lecture-notes/erm.html#the-point",
    "title": "Models, Algorithms, and Learning",
    "section": "The Point",
    "text": "The Point\nWhat’s the point of developing all this theory for empirical risk minimization? The reason that this is helpful for us is that most modern prediction algorithms are doing versions of empirical risk minimization. We can even derive important, new algorithms just by modifying the loss function \\(\\ell\\). For example:\nBinary logistic regression is empirical risk minimization in which the labels \\(y \\in \\{0,1\\}\\), the predictions are of the form \\(\\hat{y} = \\langle \\mathbf{w}, \\mathbf{x} \\rangle - b\\) and the loss function is the logistic loss \\[\\ell(\\hat{y}, y) = y \\log \\sigma(\\hat{y}) + (1-y) \\log (1-\\sigma(\\hat{y}))\\;,\\] where \\(\\sigma(z)\\triangleq \\frac{1}{1 + e^{-z}}\\) is the logistic sigmoid function.\nLeast-squares linear regression is a form of empirical risk minimization which is more suitable for predicting a number \\(y \\in \\mathbb{R}\\) than a label like \\(y\\in \\{0,1\\}\\). In linear least-squares, the predictions are of the form \\(\\hat{y} = \\langle \\mathbf{w}, \\mathbf{x} \\rangle - b\\) and the loss function is the squared error \\[\\ell(\\hat{y}, y) = (y - \\hat{y})^2\\;.\\]\nSupport vector machines were the state-of-the-art classification method in the late 90s and early 2000s, and are still in use today. Support vector machines still use linear predictions of the form \\(\\hat{y} = \\langle \\mathbf{w}, \\mathbf{x} \\rangle - b\\). If the labels are \\(y \\in \\{-1,1\\}\\), then the loss function is the hinge loss written as \\[\\ell(\\hat{y}, y) = \\max\\{1 - y\\hat{y}, 0\\}\\;.\\]\nAs it turns out, the logistic loss, squared error, and hinge loss are all “better” losses than the \\(0-1\\) loss, for reasons which will become clear when we get to writing numerical algorithms for risk minimization."
  },
  {
    "objectID": "lecture-notes/erm.html#the-empirical-risk-minimization-workflow",
    "href": "lecture-notes/erm.html#the-empirical-risk-minimization-workflow",
    "title": "Models, Algorithms, and Learning",
    "section": "The Empirical Risk Minimization Workflow",
    "text": "The Empirical Risk Minimization Workflow\nFundamentally, an empirical risk minimization algorithm consists of:\n\nA family \\(\\mathcal{M}\\) of predictor functions \\(f_\\boldsymbol{\\theta}\\) that generates predictions \\(\\hat{y} = f_\\boldsymbol{\\theta}(\\mathbf{x})\\).\nA loss function \\(\\ell\\) that compares the prediction and the real value \\(\\ell(\\hat{y}, y)\\).\nAn algorithm for minimizing the empirical risk (Equation 2) with respect to the parameters \\(\\boldsymbol{\\theta}\\) of \\(\\mathcal{M}\\).\n\nMany empirical risk minimization algorithms also admit certain guarantees on how close the minimized empirical risk is to the actual risk under certain assumptions about the probability generating distribution. These kinds of guarantees are an extremely active area of ongoing machine learning research. They usually require some probability tools that are beyond the scope of this course."
  },
  {
    "objectID": "lecture-notes/erm.html#closeout-activity",
    "href": "lecture-notes/erm.html#closeout-activity",
    "title": "Models, Algorithms, and Learning",
    "section": "Closeout Activity",
    "text": "Closeout Activity\n\n\n\n\n\n\nBack to the Coin Flipping Game\n\n\n\nLet’s go back to the warmup corresponding to these lecture notes. We can think of the problem of choosing \\(\\hat{p}\\) as a prediction problem in which we observe 0 features! Identify and write down formulae for:\n\nThe data generating distribution.\nThe true risk corresponding to prediction \\(\\hat{p}\\).\nThe empirical risk corresponding to prediction \\(\\hat{p}\\).\n\nAdditionally: which loss function are we using in this game? It’s one of the ones in the notes above!\n\n\nIf your team gets all the way through those questions, please move on to the following coding exercise:\n\n\n\n\n\n\nExperiment\n\n\n\nThe following code simulates 100 random flips of a biased coin and computes the minimizer \\(\\hat{p}\\) of the empirical risk after each flip. It then plots the empirical and actual risk to show how these evolve as we observe more data.\nIt then computes arrays of the empirical risk R_hat and actual risk R associated the estimates \\(p_hat\\). For example, if \\(\\hat{p}_k\\) is the prediction after \\(k\\) observations, then the corresponding entry of \\(\\hat{R}_k\\) should have value \\(\\frac{1}{k} \\sum_{i = 1}^k \\left[-y \\log \\hat{p}_k - (1- y)\\log (1-\\hat{p}_k)\\right]\\) (hint: np.log. Second hint: simplify this expression a little using \\(\\hat{p}_k\\). ).\nHowever, there are two lines missing: the computation of the empirical and actual risks. Fill these in and run the experiment in a Jupyter notebook.\n\n# imports \nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# simulate flips. The array flips now contains 0s and 1s. \nn_flips = 100\np = 0.7 # bias of coin\nflips = 1*(np.random.rand(n_flips) &lt; p)\n\n# minimizing value of p_hat after each flip\np_hat = np.cumsum(flips)/np.arange(1, len(flips)+1) \n\nR_hat = # empirical risk: fill in\nR =     # true risk:      fill in\n\n# construct the plot \nplt.rcParams[\"figure.figsize\"] = (6, 3)\nfig, axarr = plt.subplots(1, 2)\naxarr[0].plot(R_hat, label = \"Empirical risk\")\naxarr[0].plot(R, label = \"Risk\")\naxarr[0].legend()\naxarr[0].set(xlabel = \"iteration\", ylabel = \"risk (nats)\")\n\naxarr[1].plot(p_hat, label = \"Prediction\", color = \"grey\")\naxarr[1].plot(p*np.ones(n_flips), label = \"True value\", color = \"black\")\naxarr[1].set(xlabel = \"iteration\", ylabel = r\"prediction $p$\")\nplt.tight_layout()\n\naxarr[1].legend()\nYour result should look something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscuss\n\n\n\nTry running your experiment a few times. Comment qualitatively: how accurate is the empirical risk as an estimate of the true risk? How accurate is \\(\\hat{p}\\) as an estimate of \\(p\\)? In this case, how many coin flips would you want to see before you felt comfortable with your predictive model? Why?"
  },
  {
    "objectID": "lecture-notes/intro-deep.html",
    "href": "lecture-notes/intro-deep.html",
    "title": "Introduction to Deep Learning",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/intro-deep.html#sidebar-notation-update",
    "href": "lecture-notes/intro-deep.html#sidebar-notation-update",
    "title": "Introduction to Deep Learning",
    "section": "Sidebar: Notation Update",
    "text": "Sidebar: Notation Update\nBefore we proceed further, let’s make a notation update. It is about to become very convenient for us to abuse mathematical notation slightly by “vectorizing functions over rows of matrices.” What this means is that, if \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}^q\\) we will define things like this: This kind of definition is sometimes called vectorizing the function \\(f\\), not to be confused with our discussion of vectorizing data in a different lecture.\n\\[\n\\begin{aligned}\nf(\\mathbf{X}) &\\triangleq \\left[\\begin{matrix} f(\\mathbf{x}_1) \\\\ f(\\mathbf{x}_2) \\\\ \\vdots \\\\ f(\\mathbf{x}_n) \\end{matrix}\\right]\n\\end{aligned}\n\\]\nSo, evaluating \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}^q\\) on \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times p}\\) produces a matrix \\(f(\\mathbf{X}) \\in \\mathbb{R}^{n\\times q}\\).\nSimilarly, we’ll define\n\\[\n\\begin{aligned}\n    \\phi(\\mathbf{X}) \\triangleq \\left[\\begin{matrix}\\phi(\\mathbf{x}_1) \\\\ \\phi(\\mathbf{x}_2) \\\\ \\vdots \\\\ \\phi(\\mathbf{x}_n)\\end{matrix}\\right] \\quad \\text{and}\n    \\quad \\ell(\\hat{\\mathbf{Y}}, \\mathbf{Y}) &\\triangleq \\left(\\begin{matrix} \\ell(\\hat{\\mathbf{y}}_1, \\mathbf{y}_1) \\\\ \\ell(\\hat{\\mathbf{y}}_2, \\mathbf{y}_2) \\\\ \\vdots \\\\ \\ell(\\hat{\\mathbf{y}}_n, \\mathbf{y}_n) \\end{matrix}\\right)\\;.\n\\end{aligned}\n\\]\nThis allows us to write our prediction rule compactly as \\[\n\\begin{aligned}\n\\hat{\\mathbf{Y}} = f(\\mathbf{X}) = \\left[\\begin{matrix} f(\\mathbf{x}_1) \\\\ f(\\mathbf{x}_2) \\\\ \\vdots \\\\ f(\\mathbf{x}_n) \\end{matrix}\\right]\n       = \\left[\\begin{matrix} \\phi(\\mathbf{x}_1)\\mathbf{W}\\\\ \\phi(\\mathbf{x}_2)\\mathbf{W}\\\\ \\vdots \\\\ \\phi(\\mathbf{x}_n)\\mathbf{W}\\end{matrix}\\right]\n       = \\left[\\begin{matrix} \\phi(\\mathbf{x}_1) \\\\ \\phi(\\mathbf{x}_2) \\\\ \\vdots \\\\ \\phi(\\mathbf{x}_n) \\end{matrix}\\right] \\mathbf{W}\n       = \\phi(\\mathbf{X})\\mathbf{W}\n\\end{aligned}\n\\]\nLet’s also define \\(\\mathcal{L}(\\hat{\\mathbf{Y}}, \\mathbf{Y}) = \\sum_{i = 1}^n \\ell(\\hat{\\mathbf{y}}_i, \\mathbf{y}_i)\\). Then, we can write our generalized empirical risk as\n\\[\nR = \\mathcal{L}(\\phi(\\mathbf{X})\\mathbf{W}, \\mathbf{Y})\\;,\n\\]\nand our learning problem is\n\\[\n\\hat{\\mathbf{W}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{W}} \\; \\mathcal{L}(\\phi(\\mathbf{X})\\mathbf{W}, \\mathbf{Y})\\;,\n\\]"
  },
  {
    "objectID": "lecture-notes/intro-deep.html#the-network-view",
    "href": "lecture-notes/intro-deep.html#the-network-view",
    "title": "Introduction to Deep Learning",
    "section": "The Network View",
    "text": "The Network View\nWe can express the process of obtaining a prediction from logistic regression using a computational graph. Here’s an example of a computational graph for logistic regression in which we have two input features and wish to perform 3-way classification by outputing for each input \\(\\mathbf{x}\\) a vector \\(\\mathbf{p}\\) of probabilities for each label:\n\n\n\n\nflowchart LR\n  subgraph input\n    direction TB  \n    x_1 \n    x_2\n  end\n\n  subgraph feature_map[features]\n    f_1\n    f_2\n  end\n\n  subgraph raw predictions[raw predictions/logits]\n    y_hat_1\n    y_hat_2 \n    y_hat_3\n  end\n\n  subgraph label_preds[label probabilities]\n    p_1\n    p_2\n    p_3\n  end\n\n  x_1 & x_2 --&gt; x\n\n  x --phi--&gt; f_1 & f_2\n\n  f_1 --w_1--&gt; y_hat_1\n  f_2 --w_1--&gt; y_hat_1\n  f_1 --w_2--&gt; y_hat_2\n  f_2 --w_2--&gt; y_hat_2\n  f_1 --w_3--&gt; y_hat_3\n  f_2 --w_3--&gt; y_hat_3\n\n  y_hat_1 & y_hat_2 & y_hat_3 --&gt; softmax\n\n  softmax --&gt; p_1 & p_2 & p_3"
  },
  {
    "objectID": "lecture-notes/intro-deep.html#back-to-recap",
    "href": "lecture-notes/intro-deep.html#back-to-recap",
    "title": "Introduction to Deep Learning",
    "section": "Back to Recap",
    "text": "Back to Recap\nWe have a problem that we never addressed in a fully satisfactory way. On the one hand, in order to describe complex, nonlinear patterns in our data, we want a lot of features.  However, the more features we engineer, the more likely we are to encounter overfitting.Kernel methods like kernel logistic regression or support vector machine can actually use infinitely many features!\nOne of the most common approaches to this problem around 15 years ago was to engineer many features but regularize, which forced entries of \\(\\mathbf{w}\\) to remain small. This is a useful approach that is still used today, especially in statistics and econometrics. However, when our data points are very complex (audio files, images, text), it might still be very difficult for us to manually engineer the right sets of features to use even in this approach.\nThe fundamental idea of deep learning is to take a different tack: instead of only learning \\(\\mathbf{w}\\) in the empirical risk minimization problem, we are also going to learn the feature map \\(\\phi\\). That is, we want to find\n\\[\n\\hat{\\mathbf{W}}, \\hat{\\phi} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{W}, \\phi} \\; \\mathcal{L}(\\phi(\\mathbf{X})\\mathbf{W}, \\mathbf{Y})\\;,\n\\]\nand our learned predictor is \\(f(\\mathbf{X}) = \\hat{\\phi}(\\mathbf{X})\\hat{\\mathbf{W}}\\).\nThe need to learn the feature map \\(\\phi\\) as well as weights \\(\\mathbf{w}\\) makes this problem much harder, both mathematically and computationally. A major enabler of the deep learning revolution has been the development of hardware that is up to the task (especially GPUs), as well as algorithms that make good use of this hardware."
  },
  {
    "objectID": "lecture-notes/clustering-live.html",
    "href": "lecture-notes/clustering-live.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "from sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(12345)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\nX, y = make_blobs(n_samples=100, n_features=2, \n                                centers=2, random_state=1)\n\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nThis data looks a lot like the kind of data that we used for classification. This time, however, we ignore \\(\\vy\\) (imagine our data didn’t come with any labels). We can still look at the plot and see that it apparently contains two groups or “clusters” of data. The clustering task is identify clusters that “fit” the data well, according to some criterion of “fit.” The k-means algorithm is one algorithm that attempts to perform this task. On this particular data, k-means does pretty well:\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters = 2)\nkm.fit(X)                    # NOTE: fit does not use y!! \n\nclusters = km.predict(X)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1], c = clusters, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nNote that what “pretty well” means here is subjective; because we don’t have any labels \\(\\vy\\), we can’t say that the clustering found by the model is “good” according to any objective criterion. Often we just have to eyeball the groups, or attempt to interpret them based on some prior knowledge we might have about the data.\nOur aim is to find the cluster centroids \\(\\vm_j\\), \\(j = 1,\\ldots, \\ell\\) and labels \\(\\vz\\) that separate the data “well.” What does “well” mean? As usual, in order to define this, we need to define a loss function. We’re outside our usual supervised framework, so we need our loss function to come from a different place. The standard loss function for k-means comes from a simple intuition: a good clustering is one in which the points in each group are close to their group centroid. This leads us to the problem\nThis is a nice mathematical formulation with a deep, dark secret: it’s not convex! This means that we can’t expect to find the “best” \\(\\hat{\\mM}\\) and \\(\\hat{\\vz}\\); we can only find “pretty good ones,” where “pretty good” means “locally optimal.” There’s a beautifully simple algorithm that we can use to perform this task:\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\n\ndef k_means_step(X, M, z):\n    # compute the distances between all points and all centroids\n    D = pairwise_distances(X, M)\n    # each point's new group is its closest centroid\n    z = np.argmin(D, axis = 1)\n    # each centroid's new value is the mean of all the points in its group\n    \n    for j in range(M.shape[0]):\n        M[j,:] = X[z == j].mean(axis = 0)\n        \n    return M, z\n\ndef k_means(X, ell):\n\n    # random initialization\n    n, p = X.shape\n    M = np.random.rand(ell, p)\n    z_prev = np.random.randint(0, ell, n)\n    done = False\n\n    # do k_means_step until z stops changing\n    while not done: \n        M, z = k_means_step(X, M, z_prev)\n        if np.all(z_prev == z):\n            done = True\n        z_prev = z\n    \n    # return the centroid matrix and cluster labels\n    return M, z\nLet’s run this algorithm and plot the results.\nM, z = k_means(X, 2)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1], c = z, alpha = 0.4, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nax.scatter(M[:,0], M[:,1], s = 50, color = \"black\")\n\n&lt;matplotlib.collections.PathCollection at 0x7ff1a8e2b880&gt;\nOriginal data is shown in blue and yellow according to the cluster learned via the k-means algorithm. Centroids of each cluster are shown in black.\nLooks ok! We have segmented the data into two clusters and found reasonable looking centroids, even though we didn’t begin with any true labels.\nThe following result is the fundamental theorem for k-means:\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/clustering-live.html#laplacian-spectral-clustering",
    "href": "lecture-notes/clustering-live.html#laplacian-spectral-clustering",
    "title": "K-Means Clustering",
    "section": "Laplacian Spectral Clustering",
    "text": "Laplacian Spectral Clustering\nK-means is a useful algorithm with an important limitation: it works best when the data is approximately linearly separable! So, we wouldn’t expect k-means to do very well at all if we wanted to separate the two rings in a data set like this:\n\nnp.random.seed(1234)\n\nn = 500\nX, y = make_circles(n_samples=n, shuffle=True, noise=0.07, random_state=None, factor = 0.5)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nLet’s check:\n\nM, z = k_means(X, 2)\n\nfig, ax = plt.subplots(1, figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1], c = z, alpha = 0.4, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\nax.scatter(M[:,0], M[:,1], s = 50, color = \"black\")\n\n&lt;matplotlib.collections.PathCollection at 0x7ff1a908d280&gt;\n\n\n\n\n\nYou can see what k-means was going for here, but the result doesn’t distinguish the two features that most of us would pick out by eye.\nWe’ll now develop Laplacian spectral clustering, a clustering method that uses matrix eigenvectors to cluster the data. Spectral clustering is a very beautiful and effective technique for nonlinear clustering of data. It has two primary limitations: it is most effective for binary clustering, and it can be computationally expensive for larger data sets."
  },
  {
    "objectID": "lecture-notes/clustering-live.html#nearest-neighbors-graph",
    "href": "lecture-notes/clustering-live.html#nearest-neighbors-graph",
    "title": "K-Means Clustering",
    "section": "Nearest-Neighbors Graph",
    "text": "Nearest-Neighbors Graph\nLaplacian clustering begins by computing the \\(k\\)-nearest-neighbors graph. In the \\(k\\)-nearest-neighbors graph, we draw a connecting edge between each point and the \\(k\\) points that are nearest to it in distance.\n\nfrom sklearn.neighbors import NearestNeighbors\n\nk = 10\nnbrs = NearestNeighbors(n_neighbors = k).fit(X)\nA = nbrs.kneighbors_graph().toarray()\n\nA = A + A.T\nA[A &gt; 1] = 1\n\nHere, the ith row of A contains a 1 in each column j such that j is one of the five nearest neighbors of i. The following function will draw this graph for us:\n\nimport networkx as nx\n\ndef plot_graph(X, A, z = None, ax = None, show_edge_cuts = True):\n    G = nx.from_numpy_array(A)\n    if z is None:\n        nx.draw(G, pos = X, alpha = .4, node_color = \"grey\", node_size = 20, ax = ax)\n    else: \n        if show_edge_cuts:\n            colors = [\"red\" if z[i] != z[j] else \"grey\" for i, j in G.edges()]\n            widths = [2 if z[i] != z[j] else 1 for i, j in G.edges()]\n        else:\n            colors = \"black\"\n            widths = 1\n        \n        nx.draw(G, pos = X, alpha = .4, node_color = z, node_size = 20, edge_color = colors, width = widths, ax = ax, cmap=plt.cm.cividis)\n\n    plt.gca().set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\nfig, ax = plt.subplots(figsize = (4, 4))\nplot_graph(X, A)\n\n\n\n\nWe can now frame our task as one of finding the “pieces” of this graph. Defining a “piece” of a graph mathematically takes a bit of work. To think about this problem mathematically, let’s first define a label vector \\(\\vz \\in [0,1]^n\\). As usual, our machine learning problem is actually a mathematical minimization problem: we want to define an objective function \\(f\\) such that \\(f(\\vz)\\) is small when the labels of \\(\\vz\\) are “good.”"
  },
  {
    "objectID": "lecture-notes/clustering-live.html#cut-based-clustering",
    "href": "lecture-notes/clustering-live.html#cut-based-clustering",
    "title": "K-Means Clustering",
    "section": "Cut-Based Clustering",
    "text": "Cut-Based Clustering\nA reasonable first pass at this problem is to define a clustering to be good when it doesn’t “cut” too many edges. An edge is “cut” if it has two nodes in different clusters. For example, here are two possible clusterings, with the edges cut by each clustering shown in red.\n\nfig, axarr = plt.subplots(1, 2, figsize = (8, 4))\ny_bad = np.random.randint(0, 2, n)\n\nplot_graph(X, A, z = y, ax = axarr[0])\nplot_graph(X, A, z = y_bad, ax = axarr[1])\n\n\n\n\nThe righthand plot has many more red edges, which connect nodes in two different proposed clusters. In contrast, the lefthand plot has many fewer.\nHere are the cut values of these two clusterings:\n\n# implement cut\n    \ndef cut(A, z): # number of edges in A cut by z\n    D = pairwise_distances(z.reshape(-1, 1))\n    return (A*D).sum()/2\n    \nprint(f\"good labels cut = {cut(A, z = y)}\") \nprint(f\"bad labels cut = {cut(A, z = y_bad)}\") \n\ngood labels cut = 7.0\nbad labels cut = 1475.0\n\n\nSo, it might seem as though a good approach would be to minimize the cut value of a label vector. Unfortunately, cut minimization has a fatal flaw: the best cut is always the one that assigns all nodes to the same label! This clustering always achieves a cut score of 0. So, we need to try something different."
  },
  {
    "objectID": "lecture-notes/clustering-live.html#normalized-cut",
    "href": "lecture-notes/clustering-live.html#normalized-cut",
    "title": "K-Means Clustering",
    "section": "Normalized Cut",
    "text": "Normalized Cut\nTo define a better objective, we need a little notation. The volume of cluster \\(j\\) in matrix \\(\\mA\\) with clustering vector \\(\\vz\\) is defined as\n\\[\n\\vol{j}{\\mA}{\\vz} = \\sum_{i = 1}^n \\sum_{i' = 1}^n \\one{z_i = j} a_{ii'}\n\\]\nHeuristically, \\(\\vol{j}{\\mA}{\\vz}\\) is the number of edges that have one node in cluster \\(j\\). The normalized cut objective function is\n\\[\nf(\\vz, \\mA) = \\cut{\\mA}{\\vz}\\left(\\frac{1}{\\vol{0}{\\mA}{\\vz}} + \\frac{1}{\\vol{1}{\\mA}{\\vz}}\\right)\n\\]\nThe normcut of our preferable clustering is still better than the random one:\n\n# implement vol and normcut\n\ndef vol(j, A, z):\n    return A[z == j,:].sum()\n\nprint(f\"good labels normcut = {normcut(A, z = y)}\") \nprint(f\"bad labels normcut = {normcut(A, z = y_bad)}\") \n\n\n\n\n\n\n\nHow do the \\(\\frac{1}{\\vol{j}{\\mA}{\\vz}}\\) terms stop the normcut from favoring the clustering in which one cluster contains no nodes?"
  },
  {
    "objectID": "lecture-notes/clustering-live.html#spectral-approximation",
    "href": "lecture-notes/clustering-live.html#spectral-approximation",
    "title": "K-Means Clustering",
    "section": "Spectral Approximation",
    "text": "Spectral Approximation\nGreat! How do we choose \\(\\vz\\) to minimize the normcut?\n\n\n\n\n\n\n\nTheorem 2 The problem of finding a binary vector \\(\\vz \\in \\{0,1\\}^n\\) that minimizes the normcut objective is NP-hard.\n\n\n\n\nWhoops! In fact, we can’t minimize the normcut exactly, so we need to do so approximately.\nHere is the trick. Suppose that we have a binary clustering vector \\(\\vz\\). We’re going to associate \\(\\vz\\) with a modified clustering vector \\(\\tilde{\\vz}\\) with entries\n\\[\n\\tilde{z}_i = \\begin{cases}\n    &\\frac{1}{\\vol{0}{\\mA}{\\vz}} \\quad z_i = 0 \\\\\n    - &\\frac{1}{\\vol{1}{\\mA}{\\vz}} \\quad z_i = 1\\;.\n\\end{cases}\n\\]\nA direct mathematical calculation then shows that we can write the normcut objective as \\(f(\\vz) = \\tilde{f}({\\tilde{\\vz}})\\), where\n\\[\n\\tilde{f}({\\tilde{\\vz}}) = \\frac{\\tilde{\\vz}^T(\\mD - \\mA)\\tilde{\\vz}}{\\tilde{\\vz}^T\\mD\\tilde{\\vz}}\\;,\n\\tag{2}\\]\nwhere\n\\[\n\\mD = \\left[\\begin{matrix} \\sum_{i = 1}^n a_{i1} & & & \\\\\n    & \\sum_{i = 1}^n a_{i2} & & \\\\\n    &  & \\ddots & \\\\\n    & & & \\sum_{i = 1}^n a_{in}\n\\end{matrix}\\right]\\;.\n\\]\nAdditionally, we have \\[\n\\tilde{\\vz}^T\\mD \\vone = 0\\;,\n\\tag{3}\\] where \\(\\vone\\) is the vector containing all 1s. Heuristically, Equation 3 says that cluster 0 and cluster 1 contain roughly the same number of edges within them.\nThis cheat is common enough that it has a name: we call it the spectral relaxation.\nAt this stage we do a cheat: we forget about the requirement that \\(\\tilde{\\vz}\\) have the form we stated above. Instead, we simply solve the optimization problem\n\\[\n\\begin{aligned}\n\\tilde{\\vz} =& \\argmin_{\\tilde{\\vz}} \\frac{\\tilde{\\vz}^T(\\mD - \\mA)\\tilde{\\vz}}{\\tilde{\\vz}^T\\mD\\tilde{\\vz}} \\\\\n& \\text{such that }\\tilde{\\vz}^T\\mD \\vone = 0\n\\end{aligned}\n\\]\nA beautiful theorem from linear algebra states that there is an explicit solution to this problem: \\(\\vz\\) should be the eigenvector with the second-smallest eigenvalue of the matrix \\(\\mL = \\mD^{-1}[\\mD - \\mA]\\). This matrix \\(\\mL\\) is called the normalized Laplacian, and it is the reason that this clustering algorithm is called Laplacian spectral clustering.\n\nfrom hidden.spectral import second_laplacian_eigenvector\nfig, ax = plt.subplots(figsize = (4, 4))\nz_ = second_laplacian_eigenvector(A)\nplot_graph(X, A, z = z_, show_edge_cuts = False, ax = ax)\n\n\n\n\nDarker shades correspond to nodes on which the eigenvector is negative, while lighter shades correspond to nodes on which the eigenvector is positive. We can get a final set of cluster labels just by assigning nodes to the cluster depending on the sign of the estimated \\(\\tilde{\\vz}\\):\n\nz = z_ &gt; 0\nfig, ax = plt.subplots(figsize = (4, 4))\nplot_graph(X, A, z, show_edge_cuts = True, ax = ax)\n\n\n\n\nOur method of distinguishing the two rings using Laplacian spectral clustering seems to have worked! Let’s try with another data set:\n\nfrom sklearn.datasets import make_moons\n\nX, z = make_moons(n_samples=100, random_state=1, noise = .1)\nfig, ax = plt.subplots(figsize = (4, 4))\na = ax.scatter(X[:, 0], X[:, 1])\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nNow we’ll use a complete implementation of Laplacian spectral clustering, which you can complete in an optional blog post.\n\nfrom hidden.spectral import spectral_cluster\n\nfig, ax = plt.subplots(figsize = (4, 4))\nz = spectral_cluster(X, n_neighbors = 6)\na = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\na = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\")\n\n\n\n\nLooks pretty good! Note, however, that we chose a pretty specific value of n_neighbors, the number of neighbors to use when forming the nearest-neighbors graph. This is a hyperparameter that needs to be tuned in some way. Unfortunately, cross-validation isn’t really an option here, as we don’t have a loss function or training data to use for validation purposes.\nThe performance of Laplacian spectral clustering can depend pretty strongly on this parameter. In this data set, small values can lead to oddly fragmented clusters, while larger values lead to results that don’t look too different from what we might expect from k-means:\n\nfig, axarr = plt.subplots(2, 3, figsize = (6, 4))\n\ni = 2\nfor ax in axarr.ravel():\n    z = spectral_cluster(X, n_neighbors = i)\n    a = ax.scatter(X[:, 0], X[:, 1], c = z, cmap = plt.cm.cividis)\n    a = ax.set(xlabel = \"Feature 1\", ylabel = \"Feature 2\", title = f\"{i} neighbors\")\n    i += 1\n\nplt.tight_layout()\n\n\n\n\nAs usual, there are ways to address these limitations, but these are beyond our scope for today."
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html",
    "href": "lecture-notes/classification-in-practice.html",
    "title": "Introduction to Classification in Practice",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html#but",
    "href": "lecture-notes/classification-in-practice.html#but",
    "title": "Introduction to Classification in Practice",
    "section": "But…",
    "text": "But…\nThere are actually a lot of practicalities to consider here as well! Where does our data come from? How do we prepare it for analysis? If we are going to use a feature map \\(\\phi\\) for things like polynomial features, how do we choose the right feature map? If our model has hyperparameters for things like regularization, how do we choose the right hyperparameters? All of these are questions that we need to handle in the practice of machine learning.\nOur purpose in this lecture is to actually work through some of the common steps of the standard machine learning workflow."
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html#prediction-question",
    "href": "lecture-notes/classification-in-practice.html#prediction-question",
    "title": "Introduction to Classification in Practice",
    "section": "Prediction Question",
    "text": "Prediction Question\nThe standard prediction question for this data set is:\n\nCan we predict whether or not a given passenger survived the crash of the Titanic, given information about them and their position on the ship?"
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html#data-inspection",
    "href": "lecture-notes/classification-in-practice.html#data-inspection",
    "title": "Introduction to Classification in Practice",
    "section": "Data Inspection",
    "text": "Data Inspection\nBefore modeling, it’s usually beneficial to learn about your data. It’s not always possible to do this without modeling, for example if your data is very high-dimensional. Because this data set has a relatively small number of features, we can learn a lot about it just through summaries. Let’s ask a few questions:\n\nWhat percentage of passengers in the training set survived?\n\n\ny_train.mean()\n\n0.3921015514809591\n\n\nApproximately 40% of passengers in the training set survived. It’s important to keep this in mind because it sets the base rate for our problem. The base rate is the accuracy rate of a trivial model that doesn’t use the features. In this case, the trivial model is the model that always predicts that a passenger died. This base model would be right about 60% of the time.\n\nHow wealthy were the passengers on the Titanic?\n\nWe can’t know for certain, but we can learn about how much was paid for each passenger class:\n\nX_train.groupby('Pclass')[['Fare']].aggregate([np.mean, len]).round(2)\n\n\n\n\n\n\n\n\nFare\n\n\n\nmean\nlen\n\n\nPclass\n\n\n\n\n\n\n1\n87.59\n173\n\n\n2\n20.67\n146\n\n\n3\n13.79\n390\n\n\n\n\n\n\n\n\nThe average price of 88 pounds for a first-class ticket corresponds to nearly $15,000 USD today.\nThe second-class ticket corresponds to roughly $3,500\nThe third class ticket corresponds to roughly $2,500.\n\nWe can safely assume that the first-class passengers were indeed substantially more wealthy on average than the others.\n\nDid wealth disparities make a difference for who was most likely to survive?\n\nWe can segment out survival rates by passenger class to learn more about this:\n\ndf_train.groupby(['Pclass'])[['Survived']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n\n\n\nSurvived\n\n\n\nmean\nlen\n\n\nPclass\n\n\n\n\n\n\n1\n0.63\n173\n\n\n2\n0.50\n146\n\n\n3\n0.25\n390\n\n\n\n\n\n\n\nIndeed, the higher passenger classes had significantly higher survival rates.\nThis difference is even starker if we also segment out the data by the sex of the passenger:\n\ndf_train.groupby(['Pclass', 'Sex'])[['Survived']].aggregate([np.mean,len ]).round(2)\n\n\n\n\n\n\n\n\n\nSurvived\n\n\n\n\nmean\nlen\n\n\nPclass\nSex\n\n\n\n\n\n\n1\nfemale\n0.97\n72\n\n\nmale\n0.39\n101\n\n\n2\nfemale\n0.91\n64\n\n\nmale\n0.18\n82\n\n\n3\nfemale\n0.51\n109\n\n\nmale\n0.14\n281\n\n\n\n\n\n\n\nThis table reflects the famous maritime tradition of prioritizing women and children first into the lifeboats, resulting in vastly higher survival rates among women in these data. Note the role of class: a 1st-class woman was twice as likely to survive as a third class woman, and a 1st-class man was nearly three times as likely to survive as a 3rd class man. Based on these observations, we might expect that passenger sex and Pclass might be useful features for us to incorporate into algorithms."
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html#data-preparation",
    "href": "lecture-notes/classification-in-practice.html#data-preparation",
    "title": "Introduction to Classification in Practice",
    "section": "Data Preparation",
    "text": "Data Preparation\nSo far, we’ve been working with 2d numpy arrays (matrices) of features and 1d numpy arrays (vectors) of target variables. We can treat pandas data frames of numbers like matrices, and we can treat pandas columns of numbers like vectors. For example, our y_train is already in a format that we can use:\n\ny_train.head() # first few entries\n\n0    1\n1    0\n2    0\n3    0\n4    0\nName: Survived, dtype: int64\n\n\nOn the other hand, our data frame has one column that we can’t use: the Sex column contains strings representing categories, rather than numbers. ML algorithms only understand numbers, and so we need to encode the Sex of the passengers as a number. We use so-called “one-hot encoding” for this, in which each category is represented by a binary column, with a 1 indicating that the passenger fell into that category. The Pandas function get_dummies() is an extremely convenient way to achieve this:\n\nX_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train.head()\n\n\n\n\n\n\n\n\nPclass\nAge\nSiblings/Spouses Aboard\nParents/Children Aboard\nFare\nSex_male\n\n\n\n\n0\n2\n29.0\n0\n0\n10.500\n0\n\n\n1\n3\n4.0\n3\n2\n27.900\n1\n\n\n2\n3\n6.0\n4\n2\n31.275\n0\n\n\n3\n3\n33.0\n1\n1\n20.525\n1\n\n\n4\n3\n22.0\n0\n0\n9.000\n1\n\n\n\n\n\n\n\nThis looks better! We can now treat X_train as a matrix of features and use it as an input for any of our machine learning algorithms."
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html#modeling",
    "href": "lecture-notes/classification-in-practice.html#modeling",
    "title": "Introduction to Classification in Practice",
    "section": "Modeling",
    "text": "Modeling\nNow we’re ready to do some modeling! You may know how to implement logistic regression (and maybe you’ve already done it!), but for today we’ll use the scikit-learn implementation. We can already go ahead and fit our model. In sklearn, the score of a classification model is just the accuracy rate.\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\nLR.score(X_train, y_train)\n\n0.7968970380818053\n\n\nSo, our model achieves about 80% accuracy on the training data, which is much better than the 60% we could have achieved by random guessing.\nLet’s take a look at the optimal parameter vector \\(\\mathbf{w}\\). This is stored in LR in the coef_ instance variable:\n\npd.DataFrame({\n  \"column\" : X_train.columns, \n  \"coefficient\" : LR.coef_.ravel()\n  })\n\n\n\n\n\n\n\n\ncolumn\ncoefficient\n\n\n\n\n0\nPclass\n-1.135432\n\n\n1\nAge\n-0.040872\n\n\n2\nSiblings/Spouses Aboard\n-0.387547\n\n\n3\nParents/Children Aboard\n-0.137851\n\n\n4\nFare\n0.001768\n\n\n5\nSex_male\n-2.623910\n\n\n\n\n\n\n\nThe way to read these coefficients is that when the number in the corresponding column gets larger, the odds of survival decrease. For example, the negative coefficient of Pclass means that someone with a larger value of Pclass (e.g. 3) has a lower chance of survival in the model than someone with a lower value (e.g. 1). Note that very strongly negative coefficient of Sex_male, which expresses the much lower survival rate of men.\nAt this point we could just go ahead and and evaluate our model’s predictive capabilities by downloading the test set and checking our predictive accuracy. However, we should ask ourselves:\n\nIs this the best we can do?\n\nWe have all kinds of different choices that we can make that may help us improve our models. For example:\n\nFrom our first model it looks like Fare may not be an especially strong predictor because of its small coefficient. Maybe our model would generalize better if we just didn’t include it?\nShould we try incorporating some feature transformations, like polynomial features?\nShould we try regularizing our logistic regression?\n\nWe can’t exhaustively explore all possibilities, but let’s try to address one of these. Should we try incorporating polynomial features, and if so, of what degree?\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\nLet’s write a simple function that will construct a model with polynomial features for us:\n\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = \"none\", max_iter = int(1e3)))])\n\nWe can use it like this:\n\nplr = poly_LR(3)\nplr.fit(X_train, y_train)\nplr.score(X_train, y_train)\n\n0.7842031029619182\n\n\nIs that better or worse than the simple logistic model without polynomial features? Unfortunately we don’t really know; the reason is again that accuracy on the training isn’t usually a reliable indicator of predictive performance. In order to make an assessment, we can instead simulate the process of fitting the model and evaluating on “test” data by witholding parts of our training data to use as testing. We split the data into chunks and withold each chunk, using the other chunks to train the data. This is called cross-validation, and it is illustrated in this figure:\n\nWe could do this with a janky for-loop, but the nice scikit-learn developers have implemented this for us. Here’s an example of cross-validation with 5 folds. This can take a little while, as there are actually 5 calls to plr.fit() happening under the hood.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(plr, X_train, y_train, cv=5)\ncv_scores\n\narray([0.70422535, 0.76056338, 0.83098592, 0.83098592, 0.73049645])\n\n\nEach of these scores represents the model’s performance when used to predict one of the 5 folds of data after having been fit on the other 4. We often just average them to get an overall metric:\n\ncv_scores.mean()\n\n0.7714514034561983\n\n\nNow we can try using cross-validation to get a sense for what degree of polynomial feature we should use. Degree 0 is actually the baseline model, and degree 1 corresponds to simple logistic regression without a polynomial feature map.\n\nfor deg in range(4):\n  plr = poly_LR(deg = deg)\n  cv_scores = cross_val_score(plr, X_train, y_train, cv=5)\n  mean_score = cv_scores.mean()\n  print(f\"Polynomial degree = {deg}, score = {mean_score.round(3)}\")\n\nPolynomial degree = 0, score = 0.608\nPolynomial degree = 1, score = 0.783\nPolynomial degree = 2, score = 0.805\nPolynomial degree = 3, score = 0.771\n\n\nIt looks like it doesn’t make a huge difference, but degree-2 polynomial features might be our best bet according to cross-validation. Let’s try go ahead and fit a single copy of this model on the entire training data:\n\nplr = poly_LR(deg = 2)\nplr.fit(X_train, y_train)\n\nPipeline(steps=[('poly', PolynomialFeatures()),\n                ('LR', LogisticRegression(max_iter=1000, penalty='none'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('poly', PolynomialFeatures()),\n                ('LR', LogisticRegression(max_iter=1000, penalty='none'))])PolynomialFeaturesPolynomialFeatures()LogisticRegressionLogisticRegression(max_iter=1000, penalty='none')\n\n\nLet’s finally see how we do on the test set. We need to download the test set and process it in the same way that we did the training set.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/test.csv\"\n\ndf_test, X_test, y_test = read_titanic_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"], drop_first=\"if_binary\")\n\nNow we’re finally ready to compute the score. Drumroll please!\n\nplr.score(X_test, y_test).round(4)\n\n0.8483\n\n\nWe achieve roughly 85% accuracy on the test data!\n\n\nIn case you’re wondering, our original logistic regression without polynomial features does almost as well on the test data:\n\nLR.score(X_test, y_test).round(4)\n\n0.8427"
  },
  {
    "objectID": "lecture-notes/classification-in-practice.html#breaking-down-accuracy",
    "href": "lecture-notes/classification-in-practice.html#breaking-down-accuracy",
    "title": "Introduction to Classification in Practice",
    "section": "Breaking Down Accuracy",
    "text": "Breaking Down Accuracy\nWhen evaluating the performance of our algorithms, it’s not usually enough to just compute an overall score. The confusion matrix of a classifier on the test data is a convenient way to understand the kind of mistakes that your model most frequently makes. To construct a confusion matrix, we can use the confusion_matrix function from sklearn.metrics.\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = plr.predict(X_test)\n\nconfusion_matrix(y_test, y_pred)\n\narray([[100,  14],\n       [ 13,  51]])\n\n\nThis matrix compares the real values of the label on each data point to their predicted values. There are two possibilities for the labels and we are comparing to their predicted values, so we have four possibilities.\n\nTrue positive (TP): \\(y_i = 1\\) and \\(\\hat{y}_i = 1\\). There are 51 true positives for this predictor on the test set.\nTrue negative (TN): \\(y_i = 0\\) and \\(\\hat{y}_i = 0\\). There are 100 true negatives for this predictor on the test set.\nFalse positive (FP): \\(y_i = 0\\) and \\(\\hat{y}_i = 1\\). There are 14 false positives for this predictor on the test set.\nFalse negative (FN): \\(y_i = 1\\) and \\(\\hat{y}_i = 0\\). There are 13 false negatives for this predictor on the test set.\n\nIt’s possible to normalize the confusion matrix in order to compute some quantities of frequent interest, like the true positive rate, the false positive rate, the true negative rate, and the false negative rate.\nThe true positive rate is the proportion of the time that the classifier correctly categorized a positive instance, out of all positive instances.\n\\[\n\\text{TPR} = \\frac{\\text{\\#TP}}{\\text{\\#TP} + \\text{\\#FN}}\n\\]\nThe false positive rate is the fraction of the time that the classifier incorrectly predicted a positive instance, out of all negative instances. \\[\n\\text{FPR} = \\frac{\\text{\\#FP}}{\\text{\\#FP} + \\text{\\#TN}}\n\\]\nThe true negative rate and false negative right are defined similarly. Normalizing the confusion matrix allows us to read off these rates:\n\nconfusion_matrix(y_test, y_pred, normalize = \"true\")\n\narray([[0.87719298, 0.12280702],\n       [0.203125  , 0.796875  ]])\n\n\nWe observe that not only does our model make mistakes, it makes different kinds of mistakes. Not only that – it makes different kinds of mistakes on different groups! For example, let’s compare the model’s confusion matrices on test data for female and male passengers:\n\nix = X_test[\"Sex_male\"] == 0\nprint(\"Female passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nix = X_test[\"Sex_male\"] == 1\nprint(\"Male passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nFemale passengers:\n[[0.3 0.7]\n [0.  1. ]]\n\nMale passengers:\n[[1.         0.        ]\n [0.86666667 0.13333333]]\n\n\n\nA few observations: on the test set…\n\n…when a female passenger survives, the model always correctly predicts this.\n…when a female passenger perishes, however, the model is actually still more likely to incorrectly predict that she did survive.\n…when a male passenger survives, the model almost always (87% of the time) instead incorrectly predicts that he perished.\n…when a male passenger perishes, the model always correctly predicts this.\n\nWe’ll go into much more detail on these rates in an upcoming lecture.\n\n\n\n\n\n\nDiscussion\n\n\n\nYou have gotten into the lucrative business of selling luxury cruise life insurance for passengers on ships like the Titanic. Here’s how your insurance works: if a passenger who has bought your insurance perishes, then you will make a payout of $100,000 to their named beneficiary.\nPer usual practices in insurance pricing, you plan to charge different prices for different passengers. You plan to set the price using the machine learning model we just trained on the Titanic data set. You will give passenger information to the model, and then base the price on the model’s prediction\n\nIf a passenger is predicted to survive, the price is $500.\nIf a passenger is predicted to perish, the price is $5,000.\n\nPlease discuss the following questions:\nAs we saw above, the model is more likely to incorrectly predict male passengers to perish, and more likely to incorrectly predict female passengers to survive. As a result, the insurance prices for men are significantly higher. Does your insurance scheme have an anti-male bias? Please consider both of the following points of view: - No, there is no bias because the model is simply doing its best to replicate the patterns found in the data. It’s not the model’s fault that men tended not to survive the crash! - Yes, there is bias because the model is creating higher prices for men on the basis of information that includes their sex.\nSuppose now that we train and evaluate a version of the model that doesn’t include passenger sex at all:\n\n# versions of the training and test data with sex column removed\nX_train_ = X_train.drop(\"Sex_male\", axis = 1)\nX_test_   = X_test.drop(\"Sex_male\", axis = 1)\n\n# fit the model\nplr.fit(X_train_, y_train)\n\n# extract a prediction\ny_pred = plr.predict(X_test_)\n\n# print confusion matrices\nix = X_test[\"Sex_male\"] == 0\nprint(\"Female passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nix = X_test[\"Sex_male\"] == 1\nprint(\"Male passengers:\")\n# print((y_test[ix] == y_pred[ix]).mean())\nprint(confusion_matrix(y_test[ix], y_pred[ix], normalize = \"true\"))\nprint(\"\")\n\nFemale passengers:\n[[0.9        0.1       ]\n [0.46938776 0.53061224]]\n\nMale passengers:\n[[0.82978723 0.17021277]\n [0.46666667 0.53333333]]\n\n\n\nThis looks more even, but now we have a different discrepancy: male passengers are more likely than female passengers to be incorrectly predicted to survive, so now the male insurance prices are lower than the female ones. Discuss the following two propositions:\n\nThis version of the model cannot have gender bias because gender was not a feature on which the model was trained.\nThis version of the model still has gender bias because it leads to higher insurance prices for men."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html",
    "title": "Introduction: Unsupervised Learning",
    "section": "",
    "text": "So far in this course, we’ve considered the supervised learning framework. In the supervised framework, we have a matrix of predictive features \\(\\mX \\in \\R^{n\\times p}\\) and a vector of targets \\(\\vy \\in \\R^n\\). Our aim is to find a function \\(f:\\R^p\\rightarrow \\R\\) such that \\(f(\\vx) \\approx y\\); that is, \\(f\\) can be used to make reasonable predictions of a new target \\(y\\) on the basis of new predictors \\(\\vx\\).\nUnsupervised learning refers to machine learning techniques in which we do not have access to any targets \\(\\vy\\). Instead, we only have the features \\(\\mX\\). In general, the aim of unsupervised learning is not to make predictions, but rather to “find some structure” in the features \\(\\mX\\).\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#dimensionality-reduction",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#dimensionality-reduction",
    "title": "Introduction: Unsupervised Learning",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nOn the other hand, dimensionality reduction is the task of identifying similar or related features (columns of \\(\\mX\\)). This often allows us to identify patterns in the data that we wouldn’t be able to spot without algorithmic help. Dimensionality reduction is our topic for this lecture, and we’ll discuss clustering in the next one."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#approximation-the-frobenius-norm",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#approximation-the-frobenius-norm",
    "title": "Introduction: Unsupervised Learning",
    "section": "Approximation: the Frobenius Norm",
    "text": "Approximation: the Frobenius Norm\nIn PCA, \\(\\mX \\approx \\mU \\mW\\) means that \\(\\norm{\\mX - \\mU \\mW}_F\\) is small, where \\(\\norm{\\cdot}_F\\) is the Frobenius norm. The Frobenius norm of a matrix is just the square root of the sum of its squared entries: \\[\n\\norm{\\mA}_F = \\sqrt{\\sum_{i = 1}^n \\sum_{j = 1}^n a_{ij}^2}\\;.\n\\]"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#choice-of-k",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#choice-of-k",
    "title": "Introduction: Unsupervised Learning",
    "section": "Choice of \\(k\\)",
    "text": "Choice of \\(k\\)\nFor PCA, we leave the choice of \\(k\\) to the user. There are heuristic ways for choosing \\(k\\), but we won’t discuss them in this course."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#constraints-on-mu-and-mw",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#constraints-on-mu-and-mw",
    "title": "Introduction: Unsupervised Learning",
    "section": "Constraints on \\(\\mU\\) and \\(\\mW\\)",
    "text": "Constraints on \\(\\mU\\) and \\(\\mW\\)\nIn PCA, we place a certain special structure on the factorization matrices \\(\\mU\\) and \\(\\mW\\) that help us to interpret them:\n\n\\(\\mW \\in \\R^{k \\times p}\\)\n\\(\\mU = \\mX \\mW^T\\). Note that this ensures that \\(\\mU \\in \\R^{n\\times k}\\).\n\\(\\mW \\mW^T = \\mI\\).\n\nThese assumptions can be derived statistically from the problem of finding the subspace (spanned by \\(\\mW\\)) that maximizes the variance of the data when projected onto that subspace."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#the-pca-model",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#the-pca-model",
    "title": "Introduction: Unsupervised Learning",
    "section": "The PCA Model",
    "text": "The PCA Model\nWith these choices, the PCA optimization problem becomes:\n\\[\n\\begin{aligned}\n\\hat{\\mW} =& \\argmin_{\\mW \\in \\R^{n\\times k}} \\norm{\\mX - \\mX\\mW^T\\mW }_{F} \\\\\n          &\\text{Such that }  \\mW\\mW^T = \\mI\n\\end{aligned}\n\\]\nNow, we could aim to solve this problem with gradient descent (in the entries of \\(\\mW\\)) or some similar approach. This, however, is more complicated than needed. As it turns out, some nice theory from linear algebra gives us a formula for finding \\(\\mW\\) in terms of the eigenvalues of the matrix \\(\\mX^T\\mX\\). In particular: Take a moment to convince yourself that \\(\\hat{\\mW}\\) has dimensions \\(k \\times p\\) as required.\n\\[\n\\hat{\\mW} = \\left[\\begin{matrix}\n              - & \\vv_1 & - \\\\\n              - & \\vv_2 & - \\\\\n              \\vdots & \\vdots & \\vdots \\\\\n              - & \\vv_k & - \\\\\n\\end{matrix}\\right]\n\\]\nwhere \\(\\vv_i\\) is the \\(i\\)th eigenvector of \\(\\mX^T\\mX\\)."
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#implementing-pca",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#implementing-pca",
    "title": "Introduction: Unsupervised Learning",
    "section": "Implementing PCA",
    "text": "Implementing PCA\nLet’s go ahead and implement PCA. We’ll do this using a term-document matrix that I derived from Lewis Caroll’s famous book Alice’s Adventures in Wonderland. Each row of the data corresponds to a chapter of the book, and each column corresponds to a word that may appear in that chapter:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport nltk\nfrom nltk.corpus import gutenberg\n# nltk.download('gutenberg') # need to run once\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ns = gutenberg.raw(\"carroll-alice.txt\")\nchapters = s.split(\"CHAPTER\")[1:]\ndf = pd.DataFrame({\n    \"chapter\" : range(1, len(chapters) + 1),\n    \"text\" : chapters\n})\ndf\n\n\n\n\n\n\n\n\n\nchapter\ntext\n\n\n\n\n0\n1\nI. Down the Rabbit-Hole\\n\\nAlice was beginnin...\n\n\n1\n2\nII. The Pool of Tears\\n\\n'Curiouser and curio...\n\n\n2\n3\nIII. A Caucus-Race and a Long Tale\\n\\nThey we...\n\n\n3\n4\nIV. The Rabbit Sends in a Little Bill\\n\\nIt w...\n\n\n4\n5\nV. Advice from a Caterpillar\\n\\nThe Caterpill...\n\n\n5\n6\nVI. Pig and Pepper\\n\\nFor a minute or two she...\n\n\n6\n7\nVII. A Mad Tea-Party\\n\\nThere was a table set...\n\n\n7\n8\nVIII. The Queen's Croquet-Ground\\n\\nA large r...\n\n\n8\n9\nIX. The Mock Turtle's Story\\n\\n'You can't thi...\n\n\n9\n10\nX. The Lobster Quadrille\\n\\nThe Mock Turtle s...\n\n\n10\n11\nXI. Who Stole the Tarts?\\n\\nThe King and Quee...\n\n\n11\n12\nXII\\n\\n Alice's Evidence\\n\\n\\n'Here...\n\n\n\n\n\n\n\n\nvec = CountVectorizer(max_df = 0.5, min_df = 0, stop_words = \"english\")\n\ncounts = vec.fit_transform(df['text'])\ncounts = counts.toarray()\nM = pd.DataFrame(counts, columns = vec.get_feature_names_out())\nM\n\n\n\n\n\n\n\n\n_i_\nabide\nable\nabsence\nabsurd\nacceptance\naccident\naccidentally\naccount\naccounting\n...\nyear\nyears\nyelled\nyelp\nyer\nyesterday\nyoung\nyouth\nzealand\nzigzag\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n4\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n6\n0\n1\n\n\n5\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n11\n1\n0\n0\n0\n0\n0\n2\n1\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n12 rows × 2146 columns\n\n\n\n\nX = counts\nnp.sqrt((X**2).sum())\n\n160.9409829720199\n\n\nNow we’re ready to implement PCA. Because there is an explicit solution in terms of the eigenvalues of a matrix, our implementation can be very short. This is a very slow implementation of PCA; faster methods involve numerical methods for computing singular values.\n\ndef pca(M, k): \n    X  = np.array(M)\n    ev = np.linalg.eigh(X.T@X)\n    W  = ev[1][:,-k:].T\n    return X@W.T, W # U, W\n\nHere’s PCA in action:\n\nk = 4\nU, W = pca(M, k)\n\n\nU.shape, W.shape\n\n((12, 4), (4, 2146))"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#interpreting-pca",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#interpreting-pca",
    "title": "Introduction: Unsupervised Learning",
    "section": "Interpreting PCA",
    "text": "Interpreting PCA\nOk, so why did we do this? Both the matrices \\(\\mU\\) and \\(\\mW\\) can be used to give us information about the structure of the text. In the context of text analysis, we usually interpret \\(\\mU\\) and \\(\\mW\\) as expressing information about some latent topics. The \\(j\\) th column of \\(\\mW\\) gives us a weight for each term in the text; this tells us how present each term is in the topic. The following function can be used to inspect the top words in each topic:\n\ndef top_words_pca(M, W, component, num_words):\n    orders = np.argsort(np.abs(W), axis = 1)\n    important_words = np.array(M.columns)[orders]\n    return important_words[component][-num_words:]\n\nfor i in range(k):\n  print(f\"topic {i}: {top_words_pca(M, W, i, 10)}\")\n\ntopic 0: ['cats' 'jury' 'court' 'baby' 'mad' 'footman' 'caterpillar' 'mouse' 'cat'\n 'king']\ntopic 1: ['gryphon' 'soldiers' 'tea' 'hare' 'march' 'cat' 'dormouse' 'king'\n 'hatter' 'queen']\ntopic 2: ['tea' 'queen' 'hare' 'march' 'gryphon' 'king' 'mock' 'turtle' 'dormouse'\n 'hatter']\ntopic 3: ['march' 'course' 'soup' 'dormouse' 'hatter' 'king' 'queen' 'gryphon'\n 'mock' 'turtle']\n\n\nOn the other hand, the matrix \\(\\mU = \\mX \\mW^T\\) tells us about how present each of the \\(k\\) topics are in each of the documents (in this case, chapters of the book). We can visualize the topic weight in each chapter over time:\n\nfig, ax = plt.subplots(1)\n\nfor i in range(k):\n    ax.plot(np.arange(1, M.shape[0]+1), U[:,i], label = top_words_pca(M, W, i, 8))\n\nax.set(xlabel = \"chapter\", ylabel = \"topic weight\")\n\nax.legend(bbox_to_anchor=(1, -.15))\n\n&lt;matplotlib.legend.Legend at 0x7f8d9558d4f0&gt;"
  },
  {
    "objectID": "lecture-notes/introducing-dimensionality-reduction-live.html#limitations-of-pca",
    "href": "lecture-notes/introducing-dimensionality-reduction-live.html#limitations-of-pca",
    "title": "Introduction: Unsupervised Learning",
    "section": "Limitations of PCA",
    "text": "Limitations of PCA\nPCA is a fantastically powerful algorithm in a wide variety of settings, but it does have some important limitations. In this case, it can be difficult for us to interpret the results of PCA because some of the topics can be “negatively involved” in a chapter. That is, the words in that topic are “especially absent” from the chapter. This is confusing at best. So, we have a mismatch between what this specific matrix factorization method does and our interpretability needs. This is a very common situation, and is one of the most frequent motivators of new methods. For more on PCA, including some settings in which it is more effective, see this section of the Python Data Science Handbook by Jake VanderPlas."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html",
    "href": "lecture-notes/intro-allocative-bias-live.html",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "",
    "text": "$$\n$$\nToday we are going to study an extremely famous investigation into algorithmic decision-making in the sphere of criminal justice by Angwin et al. (2016), originally written for ProPublica. This investigation significantly accelerated the pace of research into bias and fairness in machine learning, due in combination to its simple message and publicly-available data.\nYou’ve already read about the COMPAS algorithm in the original article at ProPublica. Our goal today is to reproduce some of the main findings of this article and set the stage for a more systematic treatment of bias and fairness in machine learning.\nParts of these lecture notes are inspired by the original ProPublica analysis and Allen Downey’s expository case study on the same data.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html#data-preparation",
    "href": "lecture-notes/intro-allocative-bias-live.html#data-preparation",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\nLet’s first obtain the data. I’ve hosted a copy on the course website, so we can download it using a URL.\n\nimport pandas as pd\nimport seaborn as sns\ncompas_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/compas-scores-two-years.csv\"\ncompas = pd.read_csv(compas_url)\ncompas\n\n\n\n\n\n\n\n\nid\nname\nfirst\nlast\ncompas_screening_date\nsex\ndob\nage\nage_cat\nrace\n...\nv_decile_score\nv_score_text\nv_screening_date\nin_custody\nout_custody\npriors_count.1\nstart\nend\nevent\ntwo_year_recid\n\n\n\n\n0\n1\nmiguel hernandez\nmiguel\nhernandez\n2013-08-14\nMale\n1947-04-18\n69\nGreater than 45\nOther\n...\n1\nLow\n2013-08-14\n2014-07-07\n2014-07-14\n0\n0\n327\n0\n0\n\n\n1\n3\nkevon dixon\nkevon\ndixon\n2013-01-27\nMale\n1982-01-22\n34\n25 - 45\nAfrican-American\n...\n1\nLow\n2013-01-27\n2013-01-26\n2013-02-05\n0\n9\n159\n1\n1\n\n\n2\n4\ned philo\ned\nphilo\n2013-04-14\nMale\n1991-05-14\n24\nLess than 25\nAfrican-American\n...\n3\nLow\n2013-04-14\n2013-06-16\n2013-06-16\n4\n0\n63\n0\n1\n\n\n3\n5\nmarcu brown\nmarcu\nbrown\n2013-01-13\nMale\n1993-01-21\n23\nLess than 25\nAfrican-American\n...\n6\nMedium\n2013-01-13\nNaN\nNaN\n1\n0\n1174\n0\n0\n\n\n4\n6\nbouthy pierrelouis\nbouthy\npierrelouis\n2013-03-26\nMale\n1973-01-22\n43\n25 - 45\nOther\n...\n1\nLow\n2013-03-26\nNaN\nNaN\n2\n0\n1102\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7209\n10996\nsteven butler\nsteven\nbutler\n2013-11-23\nMale\n1992-07-17\n23\nLess than 25\nAfrican-American\n...\n5\nMedium\n2013-11-23\n2013-11-22\n2013-11-24\n0\n1\n860\n0\n0\n\n\n7210\n10997\nmalcolm simmons\nmalcolm\nsimmons\n2014-02-01\nMale\n1993-03-25\n23\nLess than 25\nAfrican-American\n...\n5\nMedium\n2014-02-01\n2014-01-31\n2014-02-02\n0\n1\n790\n0\n0\n\n\n7211\n10999\nwinston gregory\nwinston\ngregory\n2014-01-14\nMale\n1958-10-01\n57\nGreater than 45\nOther\n...\n1\nLow\n2014-01-14\n2014-01-13\n2014-01-14\n0\n0\n808\n0\n0\n\n\n7212\n11000\nfarrah jean\nfarrah\njean\n2014-03-09\nFemale\n1982-11-17\n33\n25 - 45\nAfrican-American\n...\n2\nLow\n2014-03-09\n2014-03-08\n2014-03-09\n3\n0\n754\n0\n0\n\n\n7213\n11001\nflorencia sanmartin\nflorencia\nsanmartin\n2014-06-30\nFemale\n1992-12-18\n23\nLess than 25\nHispanic\n...\n4\nLow\n2014-06-30\n2015-03-15\n2015-03-15\n2\n0\n258\n0\n1\n\n\n\n\n7214 rows × 53 columns\n\n\n\nFor today we are only going to consider a subset of columns.\n\ncols = [\"sex\", \"race\", \"decile_score\", \"two_year_recid\"]\ncompas = compas[cols]\ncompas\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n0\nMale\nOther\n1\n0\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n4\nMale\nOther\n1\n0\n\n\n...\n...\n...\n...\n...\n\n\n7209\nMale\nAfrican-American\n7\n0\n\n\n7210\nMale\nAfrican-American\n3\n0\n\n\n7211\nMale\nOther\n1\n0\n\n\n7212\nFemale\nAfrican-American\n2\n0\n\n\n7213\nFemale\nHispanic\n4\n1\n\n\n\n\n7214 rows × 4 columns\n\n\n\nWe are also only going to consider white (Caucasian) and Black (African-American) defendants:\n\n# boolean vectors (technically, pd.Series)\n\nis_white = compas[\"race\"] == \"Caucasian\"\nis_black = compas[\"race\"] == \"African-American\"\ncompas = compas[is_white | is_black]\n\ncompas = compas.copy()\n\nOur data now looks like this:\n\ncompas\n\n\n\n\n\n\n\n\nsex\nrace\ndecile_score\ntwo_year_recid\n\n\n\n\n1\nMale\nAfrican-American\n3\n1\n\n\n2\nMale\nAfrican-American\n4\n1\n\n\n3\nMale\nAfrican-American\n8\n0\n\n\n6\nMale\nCaucasian\n6\n1\n\n\n8\nFemale\nCaucasian\n1\n0\n\n\n...\n...\n...\n...\n...\n\n\n7207\nMale\nAfrican-American\n2\n1\n\n\n7208\nMale\nAfrican-American\n9\n0\n\n\n7209\nMale\nAfrican-American\n7\n0\n\n\n7210\nMale\nAfrican-American\n3\n0\n\n\n7212\nFemale\nAfrican-American\n2\n0\n\n\n\n\n6150 rows × 4 columns"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html#preliminary-explorations",
    "href": "lecture-notes/intro-allocative-bias-live.html#preliminary-explorations",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Preliminary Explorations",
    "text": "Preliminary Explorations\nLet’s do some quick exploration of our data. How many defendants are present in this data of each sex?\n\ncompas.groupby(\"sex\").size()\n\nsex\nFemale    1219\nMale      4931\ndtype: int64\n\n\nWhat about race?\n\ncompas.groupby(\"race\").size()\n\nrace\nAfrican-American    3696\nCaucasian           2454\ndtype: int64\n\n\nThe decile score is the algorithm’s prediction. Higher decile scores indicate that, according to the COMPAS model, the defendant has higher likelihood to be charged with a crime within the next two years. In the framework we’ve developed in this class, you can think of the decile score as related to quantities like \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\), which is a large number when the algorithm has high confidence in predicting a 1 label. Here, a decile score of 10 indicates high confidence in predicting a 1 (= recidivating) label.\nThe easiest way to see how this looks is with a bar chart, which we can make efficiently using the seaborn (sns) package.\n\ncounts = compas.groupby([\"race\", \"decile_score\"]).size().reset_index(name = \"n\")\nsns.barplot(data = counts, x = \"decile_score\", y = \"n\", hue = \"race\")\n\n&lt;AxesSubplot: xlabel='decile_score', ylabel='n'&gt;\n\n\n\n\n\nFinally, let’s take a look at the recidivism rate in the data:\n\ncompas[\"two_year_recid\"].mean()\n\n0.4661788617886179\n\n\nSo, in this data, approximately 47% of all defendants went on to be charged of another crime within the next two years. We can also compute the recidivism rate by race:\n\ncompas.groupby(\"race\")[\"two_year_recid\"].mean()\n\nrace\nAfrican-American    0.514340\nCaucasian           0.393643\nName: two_year_recid, dtype: float64"
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html#the-propublica-findings",
    "href": "lecture-notes/intro-allocative-bias-live.html#the-propublica-findings",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "The ProPublica Findings",
    "text": "The ProPublica Findings\nWe’re going to treat the COMPAS algorithm as a binary classifier, but you might notice a problem: the algorithm’s prediction is the decile_score column, which is not actually a 0-1 label. Following the analysis of Angwin et al. (2016), we are going to construct a new binary column in which we say that a defendant is predicted_high_risk if their decile_score is larger than 4.\n\ncompas[\"predicted_high_risk\"] = 1*(compas[\"decile_score\"] &gt; 4)\n\nNow we have a binary prediction, and we can compute things like confusion matrices:\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(compas[\"two_year_recid\"], compas[\"predicted_high_risk\"], normalize = \"true\")\n\narray([[0.64849223, 0.35150777],\n       [0.34635507, 0.65364493]])\n\n\n\n(compas[\"two_year_recid\"] == compas[\"predicted_high_risk\"]).mean()\n\n0.6508943089430894\n\n\nWe can normalize this confusion matrix to get things like the false positive and false negative rates:\nWe see that the algorithm (predicting recidivism if decile_score is 5 or above) is right about 65% of the time. A bit more specifically, both the true positive (TP) and true negative (TN) rates are approximately 65%. Both the false positive (FP) and false negative (FN) rates are approximately 35%.\nWe can also check the overall accuracy:\nThe accuracy is relatively consistent even when we break things down by race:\nHowever, and this was the main finding of the ProPublica study, the FPR and FNR are very different when we break down the data by race. Here’s the confusion matrix for Black defendants:\n\nconfusion_matrix(compas[\"two_year_recid\"][is_black], \n                 compas[\"predicted_high_risk\"][is_black], \n                 normalize = \"true\")\n\narray([[0.55153203, 0.44846797],\n       [0.27985271, 0.72014729]])\n\n\nAnd here it is for white defendants:\n\nconfusion_matrix(compas[\"two_year_recid\"][is_white], \n                 compas[\"predicted_high_risk\"][is_white], \n                 normalize = \"true\")\n\narray([[0.76545699, 0.23454301],\n       [0.47722567, 0.52277433]])\n\n\nThe ProPublica study focused on the false positive rate (FPR), which is in the top right corner of the confusion matrices. The FPR of 44% for Black defendants means that, out of every 100 Black defendants who in fact will not commit another crime, the algorithm nevertheless predicts that 44 of them will. In contrast, the FPR of 23% for white defendants indicates that only 23 out of 100 non-recidivating white defendants would be predicted to recidivate.\nThere are a few ways in which we can think of this result as reflecting bias:\n\nThe algorithm has learned an implicit pattern wherein Black defendants are intrinsically more “criminal” than white defendants, even among people who factually never committed another crime. This is a bias in the patterns that the algorithm has learned in order to formulate its predictions. This is related to representational bias, which we’ll discuss more later in the semester.\nRegardless of how the algorithm forms its predictions, the impact of the algorithm being used in the penal system is that more Black defendants will be classified as high-risk, resulting in more denials of parole, bail, early release, or other forms of freedom from the penal system. So, the algorithm has disparate impact on people. We might claim this as a form of allocative bias: bias in how resources or opportunities (in this case, freedom) are allocated between groups.\n\nSometimes predictive equality is also defined to require that the false negative rates (FNRs) be equal across the two groups as well.\nIn the language of Corbett-Davies et al. (2017), an algorithm that has equal FPRs across two groups satisfies predictive equality with respect to those two groups. So, the COMPAS algorithm fails to possess predictive equality. The idea of error rate balance in Chouldechova (2017) and balance for the positive/negative class in Kleinberg, Mullainathan, and Raghavan (2016) are similar to predictive equality.\nIn summary, the ProPublica argument was:\n\nSince the FPR differs across racial groups in ways that reinforce the oppression of Black people, the COMPAS algorithm possesses racial bias."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html#calibration",
    "href": "lecture-notes/intro-allocative-bias-live.html#calibration",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Calibration",
    "text": "Calibration\nIs that the end of the story? Emphatically not! Angwin et al. (2016) kicked off a vigorous discussion about what it means for an algorithm to fair and how to measure deviations from bias. For example, Corbett-Davies et al. (2017) consider a different idea of fairness. While predictive equality requires that the FPRs for white and Black defendants be equal, calibration expresses a different intuition:\n\nA white defendant and a Black defendant who each receive the same score should both have the same risk of recidivating.\n\n Another way to say this is that a score of 7 means the same thing, no matter the race of the defendant.Compare: an “A” in CS 201 means the same thing for your future success in CS, no matter your gender.\nWe can compute the recidivism rates for each race at each decile score using some Pandas .groupby magic:\n\ncompas.groupby([\"race\", \"predicted_high_risk\"])[\"two_year_recid\"].mean()\n\nrace              predicted_high_risk\nAfrican-American  0                      0.349540\n                  1                      0.629715\nCaucasian         0                      0.288125\n                  1                      0.591335\nName: two_year_recid, dtype: float64\n\n\nThe actual recidivism rate at each risk score is roughly the same between Black and white defendants, especially for decile scores past 5 or so."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html#calibration-for-binary-classifiers",
    "href": "lecture-notes/intro-allocative-bias-live.html#calibration-for-binary-classifiers",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Calibration for Binary Classifiers",
    "text": "Calibration for Binary Classifiers\nSo far in this course, we have primarily studied binary classifiers that produce a single 0-1 predicted label, rather than a score like a decile. For these classifiers, calibration means that the fraction of predicted recidivists who actually recidivated is the same across groups. If we follow the Angwin et al. (2016) approach and say that the algorithm predicts someone as high risk if their decile score is 4 or above, we would obtain the following results:\nThere are arguments to be had here, but from the perspective of calibration at the decile score threshold of 4, the algorithm might appear to be biased in the other direction: of those who were predicted high risk, slightly more Black than white defendants were arrested within the next two years. In most of the published literature, scholars have considered that the two rates are sufficiently close that we should instead simply say that COMPAS appears to be reasonably well calibrated."
  },
  {
    "objectID": "lecture-notes/intro-allocative-bias-live.html#overcoming-bias",
    "href": "lecture-notes/intro-allocative-bias-live.html#overcoming-bias",
    "title": "Introduction to Bias and Fairness in Classification",
    "section": "Overcoming Bias?",
    "text": "Overcoming Bias?\nOk, so COMPAS is reasonably calibrated, but does not satisfy predictive equality. Couldn’t we just find a way to fix it so that it could be both calibrated and predictively equitable? A little fine-tuning here and there maybe? Sadly, no: this is not just difficult, but actually mathematically impossible, as shown by Chouldechova (2017).\nKleinberg, Mullainathan, and Raghavan (2016) give some other definitions of fairness in algorithmic decision-making, again concluding that several concepts of fairness mathematically exclude other ones."
  },
  {
    "objectID": "lecture-notes/vectorization.html",
    "href": "lecture-notes/vectorization.html",
    "title": "Data and Vectorization",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/vectorization.html#introduction",
    "href": "lecture-notes/vectorization.html#introduction",
    "title": "Data and Vectorization",
    "section": "Introduction",
    "text": "Introduction\nSo far in this course, we’ve considered the general supervised learning scenario, in which we are given a feature matrix \\(\\mathbf{X}\\in \\mathbb{R}^{n\\times p}\\) and a target vector \\(\\mathbf{y}\\in \\mathbb{R}^n\\). We then solve the empirical risk minimization problem in order to choose model parameters that minimize a loss function on the training data. The exact structure of this loss function depends on things like whether we are doing classification or regression, what our computational resources are, and other considerations.\nBut feature matrices \\(\\mathbf{X}\\) and target vectors \\(\\mathbf{y}\\) don’t just exist in the world: they are collected and measured. We can think of data collection and measurement as posing three fundamental questions:\n\nData collection: Which rows (observations) exist in \\(\\mathbf{X}\\) and \\(\\mathbf{y}\\)?\nMeasurement: which columns (features) exist in \\(\\mathbf{X}\\)?\nMeasurement: what is the target \\(\\mathbf{y}\\) and how is it measured?\n\nBroadly, we can think of the complete machine learning workflow as having phases corresponding to problem definition, data collection + measurement, modeling, and evaluation. Here’s roughly how this looks:\n\n\n\n\nflowchart TB\n\n    subgraph problem[problem definition]\n        need[identify need]--&gt;design_collection[design data collection]\n    end\n    subgraph measurement[data collection + measurement]\n        training[training data] \n        testing[testing data]\n    end\n    subgraph modeling\n        explore[explore data] --&gt; engineer[engineer features]\n        engineer --&gt; design[design model]\n    end\n    subgraph assessment\n        test --&gt; audit\n        audit --&gt; deploy\n        deploy--&gt;evaluate\n    end\n    design_collection--&gt;measurement\n    training --vectorization--&gt; modeling\n    design --&gt; assessment\n    testing --vectorization--&gt; assessment\n    need--&gt;assessment\n\n\n\n\n\n\nSo far, we’ve spent most of our time in the “modeling” module, especially the last two steps. We’ve also studied some of the ways to test and audit algorithms. Today we’re going to discuss vectorization. We can think of vectorization as what happens between the collection of raw data and the use of that data as input for models.\n\n\n\n\n\n\n\nDefinition 1 (Vectorization) Vectorization is the act of assigning to each data observation a vector \\(\\mathbf{x}\\), thus forming a feature matrix \\(\\mathbf{X}\\). Formally, a vectorization map is a function \\(v:\\mathcal{D}\\rightarrow \\mathbb{R}^p\\) such that, if \\(d \\in \\mathcal{D}\\) is a data observation, then \\(\\mathbf{x}= v(d)\\) is a set of features corresponding to \\(d\\).\n\n\n\n\nThe reason that vectorization is necessary is that machine learning models only understand numbers. So, if our data isn’t numbers, we need to convert it into numbers in order to use it for modeling."
  },
  {
    "objectID": "lecture-notes/vectorization.html#what-data-needs-vectorization",
    "href": "lecture-notes/vectorization.html#what-data-needs-vectorization",
    "title": "Data and Vectorization",
    "section": "What Data Needs Vectorization?",
    "text": "What Data Needs Vectorization?\nMost of it!\n\nIf your data comes to you as a table or matrix containing only numbers, in which each row corresponds to exactly one observation, then you may not need to vectorize.\nIf your data comes to you in any other form, then you need to vectorize.\n\nSome data that usually require vectorization:\n\nImages\nText\nAudio files\nMost genomic data\nEtc. etc.\n\nThere are tons of ways of vectorizing different kinds of data, and we’re not going to cover all of them. Instead, we’re going to go a little more in depth on text vectorization. We’ll discuss image vectorization much more when we get to convolutional neural networks. For your projects, depending on the data you want to work with, you may need to research vectorization schemes appropriate to your data."
  },
  {
    "objectID": "lecture-notes/vectorization.html#sketchy-labels",
    "href": "lecture-notes/vectorization.html#sketchy-labels",
    "title": "Data and Vectorization",
    "section": "Sketchy Labels",
    "text": "Sketchy Labels\nThese tweets were labeled manually by the original collector of the data. As with any setting in which humans need to make subjective decisions, there is considerable possibility for debate. For example, here is one tweet that was labeld “extremely positive”:\n\nprint(df_train.iloc[[40338]][\"OriginalTweet\"].iloc[0])\n\nWE NEED COVID-19 TESTING FOR EVERYONE TODAY!\nI have never been afraid to leave my house for a trip to the grocery store in my life. Now I am. I don't want to bring home a virus to my loved ones. It's not me, it's them.\n#StayHomeSaveLives\n\n\nChallenges that can cause sketchy labels include:\n\nSpeed of labeling (it takes a LONG time to make high-quality labels)\nLanguage familiarity\nAmbiguity in the target language\nLots more!\n\nAlmost always, when working with real-world data sets, we need to keep in mind that not only is our model approximate and our data incomplete, but the data may also be contaminated with errors that we aren’t really able to control. See Northcutt, Athalye, and Mueller (2021) for much more on label errors in common machine learning benchmarks."
  },
  {
    "objectID": "lecture-notes/vectorization.html#target-vectorization",
    "href": "lecture-notes/vectorization.html#target-vectorization",
    "title": "Data and Vectorization",
    "section": "Target Vectorization",
    "text": "Target Vectorization\nOur aim is to predict the Sentiment in terms of the text of the OriginalTweet. However, neither the text OriginalTweet nor the target Sentiment are numbers. So, we need to vectorize.\nThe possible values of the Sentiment column are\n\nimport numpy as np\nnp.unique(df_train[\"Sentiment\"])\n\narray(['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral',\n       'Positive'], dtype=object)\n\n\nVectorizing the target Sentiment is simple (although there are multiple ways). We’ll construct a new target vector which is 1 if the sentiment is Positive or Extremely Positive and 0 otherwise:\n\ntarget = 1*df_train[\"Sentiment\"].str.contains(\"Positive\")\ntarget.head()\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: Sentiment, dtype: int64\n\n\nVectorizing the predictor OriginalTweet is much more complicated, and here we face a number of choices.\n\nTerm Frequency (TF) Vectorization\nIn natural language processing (NLP), a data set of text is often called a corpus, and each observation is often called a document. Here, each document is a tweet.\nOne standard vectorization technique is to construct a term-document matrix. In a term-document matrix, each row corresponds to a document and each column corresponds to a “term” (usually a word) that is present in the document. The entry \\(x_{ij}\\) of this matrix is the number of terms that term \\(j\\) appears in document \\(i\\), which we’ll call \\(\\mathrm{tf}_{ij}\\). To construct a term-document matrix, we can use the CountVectorizer from sklearn.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_df = 0.2, min_df = 0.001, stop_words = 'english')\n\nHere, max_df and min_df specify a range of frequencies to include. If a term is present in almost all documents (like “the” or “of”), then this term may not be a good indication of sentiment. On the other hand, if a term appears in only one or two documents, we probably don’t have enough data to figure out whether it matters. Finally, the choice of stop_words tells our vectorizer to ignore common English words that are unlikely to carry much emotional meaning, like “and” or “if”.\n\nf = cv.fit(df_train[\"OriginalTweet\"])\n\n\ncounts = cv.transform(df_train[\"OriginalTweet\"])\ntdm = pd.DataFrame(counts.toarray(), columns = cv.get_feature_names())\n\n/Users/philchodrow/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\nHere’s our term-document matrix. Note that most of the entries are 0 because tweets are so short!\n\ntdm\n\n\n\n\n\n\n\n\n00\n000\n10\n100\n11\n12\n13\n14\n15\n16\n...\nyear\nyears\nyes\nyesterday\nyork\nyoung\nyoutube\nyouâ\nyâ\nzero\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41152\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41153\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41154\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41155\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n41156\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n41157 rows × 2322 columns\n\n\n\nThe function below summarizes our entire data prep pipeline, which we’ll need for when we get to the test set.\n\ndef prep_tweets(df, vectorizer, train = True):\n    if train: \n        vectorizer.fit(df_train[\"OriginalTweet\"])\n    X = vectorizer.transform(df[\"OriginalTweet\"]) # term-document matrix\n    y = 1*df[\"Sentiment\"].str.contains(\"Positive\")\n\n    return X, y\n\n\nX_train_cv, y_train = prep_tweets(df_train, cv, train = True)\n\n\n\nFirst Model\nLet’s check on the base rate:\n\ny_train.mean()\n\n0.4384673324100396\n\n\nSo, always guessing that a tweet is not positive would be correct 56% of the time. Let’s see if we can beat this using logistic regression.\n\nfrom sklearn.linear_model import LogisticRegression\nLR_cv = LogisticRegression()\nLR_cv.fit(X_train_cv, y_train)\nLR_cv.score(X_train_cv, y_train)\n\n0.8755011298199578\n\n\nThis model achieves 87% accuracy on the training data.\n\n\nInverse Document Frequency Weighting\nSimple term-document matrices are good for some tasks, but in other cases it is useful to downweight terms according to their frequency in the overall training corpus. This allows our models to place greater emphasis on rarer terms, which might be more expressive of strong emotions.\nIn term-frequency-inverse-document-frequency (TF-IDF) weighting, the entry for term \\(j\\) in document \\(i\\) is Exact details of TF-IDF weightings differ; this is the one implemented by default in sklearn.\n\\[\n\\tilde{\\mathrm{x}}_{ij} = \\overbrace{\\mathrm{tf}_{ij}}^{\\text{Term frequency}}\\times \\underbrace{\\mathrm{idf}_i}_{\\text{inverse document frequency}}\\;.\n\\]\nHere, the term frequency \\(\\mathrm{tf}_{ij}\\) is again the number of times that term \\(i\\) appears in document \\(j\\), while the inverse document frequency \\(\\mathrm{idf}_i\\) is computed with the formula\n\\[\n\\mathrm{idf}_i = \\log \\frac{1+n}{1+\\mathrm{df}_i} + 1\\;\n\\] with \\(\\mathrm{df}_i\\) being the total number of documents in which term \\(i\\) appears. Finally, each row of \\(\\tilde{\\mathrm{x}}_{ij}\\) is normalized to have unit length:\n\\[\nx_{ij} = \\frac{x_{ij}}{\\sqrt{\\sum_{j}x_{ij}^2}}\n\\]\nThese \\(x_{ij}\\) are then collected to form the feature matrix \\(\\mathbf{X}\\). Let’s try constructing a model using TF-IDF vectorization:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidfv = TfidfVectorizer(max_df = 0.2, min_df = 0.001, stop_words = 'english')\nX_train_tfidf, y_train = prep_tweets(df_train, tfidfv, train = True)\n\n\nLR_tfidf = LogisticRegression()\nLR_tfidf.fit(X_train_tfidf, y_train)\nLR_tfidf.score(X_train_tfidf, y_train)\n\n0.8623077483781617\n\n\nOur TF-IDF model got a lower training score. At this stage, one good approach would be to choose which vectorization to use (as well as the vectorization parameters) using cross-validation. For now, we’ll just go ahead and grab the test set:\n\ndf_test = grab_tweets(data_set = \"test\")\nX_test_cv, y_test = prep_tweets(df_test, vectorizer = cv, train = False)\nX_test_tfidf, y_test = prep_tweets(df_test, vectorizer = tfidfv, train = False)\n\nAnd evaluate!\n\nprint(\"Term-Document Frequency\")\nprint(LR_cv.score(X_test_cv, y_test))\nprint(\"TF-IDF\")\nprint(LR_tfidf.score(X_test_tfidf, y_test))\n\nTerm-Document Frequency\n0.8412322274881516\nTF-IDF\n0.8370194839389152\n\n\nIn this case, TF-IDF did a little worse than term-document frequency vectorization on the test set."
  },
  {
    "objectID": "lecture-notes/vectorization.html#model-inspection",
    "href": "lecture-notes/vectorization.html#model-inspection",
    "title": "Data and Vectorization",
    "section": "Model Inspection",
    "text": "Model Inspection\nLet’s take a moment to learn more about how our term-document frequency-based model looks at the data. One good way to do this is by looking at the confusion matrices:\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = LR_cv.predict(X_test_cv)\nconfusion_matrix(y_test, y_pred, normalize = \"true\")\n\narray([[0.89120782, 0.10879218],\n       [0.23156533, 0.76843467]])\n\n\nThe false negative rate is higher than the true positive rate, suggesting that our model tends to tilt negative. Let’s take a look at some tweets that our model labeled as negative even though the label was positive:\n\nfalse_negs = df_test[(y_pred == 0) & (y_test == 1)][\"OriginalTweet\"]\n\nfor t in false_negs.iloc[:5]: \n    print(\"\\n-------------------\\n\")\n    print(t)\n\n\n-------------------\n\nThat's about a week from now. A bit optimistic.  Probably it will take another month.  Supply chain may be recovering, demand chain will be non-existent in US and Europe for the next month or two.\n$spx $qqq $es $nq https://t.co/yXcOfL0BnI\n\n-------------------\n\nControl over stocks and gold is lost...gold coming back very nicely! Loves wallbridge and Balmoral and warns listeners about #coronavirus Sprott Money Ltd. recently put in money to $OCG $GENM $MMG and many more... https://t.co/3aURZ2e4Sj\n\n-------------------\n\n#Coronavirus is \"an exposure of all the holes in the social safety net,\" says NELP Government Affairs Director Judy Conti\n\n#UI #Unemployment #PaidLeaveForAll\nhttps://t.co/BrCY9IJWSv\n\n-------------------\n\nIf you have booked a ticket to an event as part of a package holiday you will be offered an alternative or a refund by your travel provider, if it has been cancelled due to #Coronavirus.\n\nCheck ABTA's consumer Q&amp;A at: https://t.co/oUB4MNmrNA\n\n#COVID19 https://t.co/kMHJehS2JH\n\n-------------------\n\nOk if #COVID2019 is nothing to panic about why is Italy imposing the biggest restrictions on the civilian population since WW2? \nHow will the supermarkets be able to provide food if all the workers are told to stay at home? \nSame with any other Bussiness.\n\n\nAt this point we might have some further questions for the producer of this data set about how he did the labeling: don’t some of these tweets look like they “really” should be negative?"
  },
  {
    "objectID": "lecture-notes/vectorization.html#word-based-sentiment-analysis",
    "href": "lecture-notes/vectorization.html#word-based-sentiment-analysis",
    "title": "Data and Vectorization",
    "section": "Word-Based Sentiment Analysis",
    "text": "Word-Based Sentiment Analysis\nA nice feature of linear models like logistic regression is that we can actually check the coefficient for each word in the model. This coefficient can give us important information about which words the model believes are most positive or most negative. One easy way to get at this information is to construct a data frame with the coefficients and the words:\n\ncoef_df = pd.DataFrame({\"coef\" : LR_cv.coef_[0], \"word\" : cv.get_feature_names()})\n\n/Users/philchodrow/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n  warnings.warn(msg, category=FutureWarning)\n\n\nNow we can obtain positive and negative words by sorting. Here are some of the good ones:\n\ncoef_df.sort_values('coef', ascending = False).head(10)\n\n\n\n\n\n\n\n\ncoef\nword\n\n\n\n\n223\n3.832914\nbest\n\n\n2280\n3.470211\nwon\n\n\n849\n3.327824\nfriend\n\n\n945\n3.304033\nhand\n\n\n241\n3.265563\nbonus\n\n\n916\n3.261603\ngreat\n\n\n1541\n3.153919\npositive\n\n\n701\n3.145394\nenjoy\n\n\n565\n3.088850\ndedicated\n\n\n2303\n3.066329\nwow\n\n\n\n\n\n\n\nOn the other hand, here are some of the negative ones:\n\ncoef_df.sort_values('coef', ascending = True).head(10)\n\n\n\n\n\n\n\n\ncoef\nword\n\n\n\n\n975\n-3.691077\nhell\n\n\n1791\n-3.417773\nscams\n\n\n519\n-3.254192\ncrisis\n\n\n588\n-2.997627\ndied\n\n\n1134\n-2.988952\nkill\n\n\n1136\n-2.915697\nkilling\n\n\n2228\n-2.910622\nwar\n\n\n1536\n-2.847513\npoor\n\n\n526\n-2.832577\ncrude\n\n\n1789\n-2.780784\nscam\n\n\n\n\n\n\n\nA common use for these coefficients is to assign sentiment scores to sentences. Here’s a function that does this. It works by first stripping the punctuation and capitalization from a string, and then looking up each of its individual words in a dictionary.\n\nfrom string import punctuation \n\n\nd = {coef_df[\"word\"].loc[i] : coef_df[\"coef\"].loc[i] for i in coef_df.index}\n\ndef sentiment_of_string(s):\n    no_punc = s\n    for punc in punctuation:\n        no_punc = no_punc.replace(punc, \"\")\n    \n    words = no_punc.lower().split()\n    return np.mean([d[word] for word in words if word in d ])\n\n\ns1 = \"I love apples.\"\ns2 = \"I don't like this pandemic; it's too sad.\"\n\nprint(sentiment_of_string(s1))\nprint(sentiment_of_string(s2))\n\n2.8312000405630475\n0.27823576549007195\n\n\nThis approach is the basis of The Hedonometer, a large-scale Twitter sentiment analysis tool from our friends at the University of Vermont."
  },
  {
    "objectID": "lecture-notes/vectorization.html#activity-1",
    "href": "lecture-notes/vectorization.html#activity-1",
    "title": "Data and Vectorization",
    "section": "Activity",
    "text": "Activity\nThere is a very important kind of information that is not captured by term-document matrices, even with inverse-document-frequency weighting. Consider the following two sentences:\n\n“I like pears, not apples.”\n“I like apples, not pears.”\n\nWould these sentences have different representations in a term-document matrix?"
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html",
    "href": "lecture-notes/more-on-classification-live.html",
    "title": "More Classifiers and More Labels",
    "section": "",
    "text": "$$\n$$\nIn this set of notes, we’ll introduce a few new classifiers at a high level, including classifiers that go beyond the framework of convex linear models.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html#multiple-class-labels",
    "href": "lecture-notes/more-on-classification-live.html#multiple-class-labels",
    "title": "More Classifiers and More Labels",
    "section": "Multiple Class Labels",
    "text": "Multiple Class Labels\nSo far, we’ve treated binary classification, especially in the setting where the labels \\(y \\in \\{0,1\\}\\). We’d like to do multiclass classification, where, for example, \\(y \\in \\{0, 1, 2\\}\\). This is the setting, for example, that you encounter in the blog post on penguin classification. The transition from binary classification to multiple class labels is not too complex, if we allow ourselves to think of the target label \\(y\\) as encoding a target vector \\(\\tilde{\\mathbf{y}}\\) with zeros in all entries except the \\(y\\)th entry. Let \\(k\\) be the number of possible classes. Then, if \\(k = 3\\) and \\(y = 1\\), then \\(\\tilde{\\mathbf{y}} = (0, 1, 0)\\).  For this to work, we have to make a few other modifications as well:This is often called one-hot encoding.\n\nPrediction Vectors\nOur prediction model \\(f(\\mathbf{x})\\) can’t just spit out a real number any more – it needs to spit out something that we can compare with \\(\\tilde{\\mathbf{y}}\\). So, things like \\(f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\) don’t work anymore! We usually assume that \\(f:\\mathbb{R}^p \\rightarrow \\mathbb{R}^k\\), that is, \\(\\hat{\\mathbf{y}} = f(\\mathbf{x})\\) is a vector of the same length as \\(\\tilde{\\mathbf{y}}\\). As one important example of this, we might assume that \\[\nf(\\mathbf{x}) = \\mathbf{W}\\mathbf{x}\\;,\n\\] where now \\(\\mathbf{W}\\in \\mathbb{R}^{k \\times p}\\) is a matrix of weights. This is a direct generalization of our previous setting: if \\(f(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle\\), then we can think of \\(\\mathbf{w}\\) as being a \\(p\\times 1\\) matrix.\n\n\nLoss Function\nWe also need to modify our loss function so that we can compute things like \\[\n\\ell(\\hat{\\mathbf{y}}, \\tilde{\\mathbf{y}})\n\\] when both \\(\\hat{\\mathbf{y}}\\) and \\(\\tilde{\\mathbf{y}}\\) are vectors. One common way we do this is via the categorical cross-entropy. First, define the softmax function \\(\\boldsymbol{\\sigma}:\\mathbb{R}^k\\rightarrow \\mathbb{R}^k\\) by the formula\n\\[\n\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})_h = \\frac{e^{\\hat{y}_h}}{\\sum_{h' = 1}^k e^{\\hat{y}_{h'}}}\\;.\n\\]\nThe vector \\(\\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})\\) is a probability vector: all its entries are nonnegative and sum to 1. For convenience, write \\(\\hat{\\mathbf{p}} = \\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})\\). Then, then categorical cross-entropy is\n\\[\n\\ell(\\hat{\\mathbf{y}}, \\tilde{\\mathbf{y}}) = -\\sum_{h = 1}^k \\tilde{y}_h \\log \\boldsymbol{\\sigma}(\\hat{\\mathbf{y}})_h\\;.  \n\\tag{1}\\]\nThe categorical cross-entropy is a generalization of the logistic loss."
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html#multiclass-empirical-risk",
    "href": "lecture-notes/more-on-classification-live.html#multiclass-empirical-risk",
    "title": "More Classifiers and More Labels",
    "section": "Multiclass Empirical Risk",
    "text": "Multiclass Empirical Risk\nWe can now write the general empirical risk (not assuming linearity or convexity) as\n\\[\n\\sum_{i = 1}^n \\ell(f(\\mathbf{x}_i), \\tilde{\\mathbf{y}}_i)\\;.\n\\]\nAs usual, we’d like to find a prediction rule \\(f\\) that makes the empirical risk small, although we need to be aware of possible issues related to overfitting."
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html#multinomial-logistic-regression",
    "href": "lecture-notes/more-on-classification-live.html#multinomial-logistic-regression",
    "title": "More Classifiers and More Labels",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\nIn multinomial logistic regression, \\(f(\\mathbf{x}_i) = \\mathbf{W}\\mathbf{x}_i\\) and the loss function is the categorical cross-entropy from Equation 1. An important feature of multinomial logistic regression is that it has linear decision boundaries.\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\nspecies = [s.split()[0] for s in le.classes_]\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\ny_train\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\n\ndef training_decision_regions(model, cols, **kwargs):\n    m = model(**kwargs)\n    m.fit(np.array(X_train[cols]), y_train)\n    plot_decision_regions(np.array(X_train[cols]), y_train, clf = m)\n    ax = plt.gca()\n    ax.set(xlabel = cols[0], \n                  ylabel = cols[1], \n                  title = f\"Training accuracy = {m.score(np.array(X_train[cols]), y_train).round(2)}\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, \n              species, \n              framealpha=0.3, \n              scatterpoints=1)\n\n\ntraining_decision_regions(LogisticRegression, cols)\n\n\n\n\nIf we fit an individual logistic regression model, we’ll be able to see how its predictions work:\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\nNow that we’ve fit the model, we can inspect the weight matrix:\nThis weight matrix multiplies the feature matrix to get the prediction matrix \\(\\hat{\\mathbf{Y}}\\).\nThe built-in method LR.predict_proba will compute the predictions after having passed them through the softmax function. The advantage of this is that we can interpret each entry as the probability of class membership:\n\nP_hat = LR.predict_proba(X_train[cols])\nP_hat[0,:]\n\narray([4.32383911e-06, 5.37250378e-03, 9.94623172e-01])\n\n\n\ny_train[0]\n\n2\n\n\nHere’s a heatmap of the first 20 individuals and their predicted labels. Brighter yellow means greater predicted probability of belonging to the specified class.\n\np = plt.imshow(P[0:20, :].T)\n\n\n\n\nAlmost all of the individuals are clearly predicted in just one of the classes, while the model is less confident about the membership of the penguin with index 10."
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html#support-vector-machine",
    "href": "lecture-notes/more-on-classification-live.html#support-vector-machine",
    "title": "More Classifiers and More Labels",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\nThe support vector machine classification problem for binary classification is a convex linear model in which we use the so-called hinge loss. In the notation from our previous lectures, it can be written like this:\n\\[\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\left[\\sum_{i = 1}^n \\max \\{1 - y_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle, 0\\} + \\frac{1}{2C}\\sum_{\\ell = 1}^p w_\\ell^2\\right]\\;.\n\\]\nMathematically, the support vector machine is an exceptionally beautiful algorithm, primarily because it admits a “kernel trick.” The kernel trick allows us to use infinite-dimensional nonlinear features for prediction, which can significantly enhance the expressive power of our models.  To my knowledge, unfortunately, the support vector machine doesn’t handle multiclass classification very well. What scikit-learn does is split the problem into a sequence of binary problems (“blue or not blue”) to obtain the final result. Here’s an example:For more on the kernel trick, see Hardt and Recht, p. 58-62.\n\nfrom sklearn.svm import SVC\ntraining_decision_regions(SVC, cols, kernel = \"rbf\", gamma = 0.1)\n\n\n\n\nHere, the rbf kernel can be changed according to user preferences. gamma controls how wiggly the decision boundary is allowed to be:\n\ntraining_decision_regions(SVC, cols, kernel = \"rbf\", gamma = 10)\n\n\n\n\nCross-validation or other tools should be used in order to determine a value of \\(\\gamma\\) that has good expressive power while avoiding overfitting."
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html#multilayer-perceptron",
    "href": "lecture-notes/more-on-classification-live.html#multilayer-perceptron",
    "title": "More Classifiers and More Labels",
    "section": "Multilayer Perceptron",
    "text": "Multilayer Perceptron\nLogistic regression and support vector machine are both still in the convex linear model framework. Let’s now move beyond this framework for the first time. We’ll consider\n\nA new nonconvex linear model.\nA nonconvex nonlinear model.\n\nWe’ve already seen a nonconvex linear model: perceptron! To create a more useful one, let’s consider the following idea: we’re going to just stack logistic regressions on top of each other, like this: \\[\n\\mathbf{Z}= \\boldsymbol{\\sigma}(\\mathbf{X}\\mathbf{W})\n\\]\nThat is, the matrix \\(\\mathbf{Z}\\) is the result of computing the matrix product \\(\\mathbf{X}\\mathbf{W}\\) and then applying the softmax function row-wise. If \\(\\mathbf{W}\\) is \\(p\\times \\ell\\), then \\(\\mathbf{X}\\mathbf{W}\\) is an \\(n\\times \\ell\\) matrix, as is \\(\\mathbf{Z}\\). This is essentially multinomial logistic regression. Now, here’s the thing: what if we just used \\(\\mathbf{Z}\\) as the input to another logistic regression? That is, we compute \\[\n\\hat{\\mathbf{Y}} = \\boldsymbol{\\sigma}(\\mathbf{Z}\\mathbf{W}')\\;,\n\\]\nwhere \\(\\mathbf{W}'\\) is a new matrix of weights and \\(\\hat{\\mathbf{Y}}\\) is our matrix of predictions that we will assess using the categorical cross-entropy or another such function. Then, the empirical risk minimization problem is \\[\n\\hat{\\mathbf{W}}, \\hat{\\mathbf{W}}' = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{W}, \\mathbf{W}'} \\sum_{i = 1}^n \\ell(\\boldsymbol{\\sigma}(\\boldsymbol{\\sigma}(\\mathbf{X}\\mathbf{W})\\mathbf{W}')_i, \\tilde{\\mathbf{y}}_i) \\;.\n\\]\nThis problem is no longer convex, but we can still try to optimize it with gradient descent.\nWe often call the computation of \\(\\mathbf{Z}\\) a hidden layer because it is neither the feature matrix \\(\\mathbf{X}\\) nor the target \\(\\tilde{\\mathbf{y}}\\). So, we have created a model with a single hidden layer. The idea of stacking together simple linear transformations with simple nonlinearities is the fundamental idea of modern deep learning.\nscikit-learn implements models like this under the heading of “multilayer perceptron” (the name is mostly historical). We can create a multilayer perceptron like this:\n\nfrom sklearn.neural_network import MLPClassifier\ntraining_decision_regions(MLPClassifier, cols, activation = \"logistic\", hidden_layer_sizes = (100, 100))\n\n\n\n\nWe observe apparently linear decision boundaries in the data set this time, although in principle the model could also have generated nonlinear boundaries."
  },
  {
    "objectID": "lecture-notes/more-on-classification-live.html#decision-tree-classifiers",
    "href": "lecture-notes/more-on-classification-live.html#decision-tree-classifiers",
    "title": "More Classifiers and More Labels",
    "section": "Decision Tree Classifiers",
    "text": "Decision Tree Classifiers\nDecision tree classifiers still do empirical risk minimization, but they are both nonlinear and nonconvex. The best way to see what a decision tree classifier does is to train one and visualize it:\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nDTC = DecisionTreeClassifier(max_depth = 3)\nDTC.fit(X_train[cols], y_train)\n\np = plot_tree(DTC, feature_names = cols, filled = True, class_names = species)\n\n\n\n\nWe observe that the decision tree works by making a sequence of decisions that sort the data into progressively finer buckets. You can implement a decision tree as nothing more than a sequence of nested if-else statements, although the algorithms to actually train them can be trickier. The decision regions for decision trees look “boxy,” composed of vertical and horizontal segments:\n\ntraining_decision_regions(DecisionTreeClassifier, cols, max_depth = 5)\n\n\n\n\nDecision trees are very flexible models, but it is easy for them to overfit if the depth is too high:\nFor this reason, it is common to choose the depth through cross validation:\nIt looks like a depth of roughly 6 might be about right for this data set:\n\nRandom Forest\nA random forest is essentially a collection of many decision trees that have been trained on random subsets of the data. Random forest classifiers have some very good properties that help them be fairly resistent to overfitting – they usually work pretty well “out of the box.”\n\nfrom sklearn.ensemble import RandomForestClassifier\ntraining_decision_regions(RandomForestClassifier, cols)"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Index of Assignments",
    "section": "",
    "text": "Process Reflections\n\n\n\n            \n        \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            Reflective Goal-Setting\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 27, 2023\n                    \n                \n                \n                    \n                        We plan our goals for learning, engagement, and achievement over the course of the semester.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Reflection\n            \n        \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            Mid-Course Reflection\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Apr 5, 2023\n                    \n                \n                \n                    \n                        We reflect on our learning, engagement, and achievement in the first part of the semester.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Reflection\n            \n        \n                \n                    \n                        \n                            Reflection\n                         \n                    \n                    \n                        \n                            End-Of-Course Reflection\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        May 19, 2023\n                    \n                \n                \n                    \n                        We reflect on our learning, engagement, and achievement over the course of the semester.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Reflection\n            \n        \n\n\nNo matching items\n\n\n\n\nBlog Posts\n\n\n\n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Implementing the Perceptron Algorithm\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Feb 22, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Implementation\n                                | \n                            Navigation\n                                | \n                            Experimentation\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Optimization for Logistic Regression\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 1, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement several first-order methods: optimization algorithms based on the gradients of functions. You'll implement simple gradient descent, a momentum method, and stochastic gradient descent, comparing their performance for training logistic regression.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Theory\n                                | \n                            Implementation\n                                | \n                            Experimentation\n            \n        \n            \n        \n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Blog Post: Classifying Palmer Penguins\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 8, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll work through a complete example of the standard machine learning workflow. Your primary goal is to determine the smallest number of measurements necessary to confidently determine the species of a penguin.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Experimentation\n                                | \n                            Navigation\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Kernel Logistic Regression\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 8, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Theory\n                                | \n                            Implementation\n                                | \n                            Navigation\n                                | \n                            Experimentation\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Implementing Linear Regression\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 15, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement least-squares linear regression, and experiment with LASSO regularization for overparameterized problems.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Theory\n                                | \n                            Implementation\n                                | \n                            Experimentation\n                                | \n                            Navigation\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Auditing Allocative Bias\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 29, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll create a machine learning model that predicts an individual characteristic like employment status or income on the basis of other demographic characteristics. You'll then perform a fairness audit in order to assess whether or not your algorithm displays bias with respect demographic characteristics like race or sex, and discuss your findings.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Social Responsibility\n                                | \n                            Navigation\n                                | \n                            Experimentation\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Limits of the Quantitative Approach to Bias and Fairness\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Mar 29, 2023\n                    \n                \n                \n                    \n                        This blog post is actually an essay -- no math or coding is involved. In this blog post, you'll discuss the limits of the quantitative approach to bias and fairness in allocative decision-making.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Social Responsibility\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Unsupervised Learning with Linear Algebra\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Apr 12, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement and experiment with two simple ML approaches for image compression and image segmentation.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Theory\n                                | \n                            Implementation\n                                | \n                            Experimentation\n            \n        \n                \n                    \n                        \n                            Blog Post\n                         \n                    \n                    \n                        \n                            Optimization with Adam\n                        \n                    \n                    \n                    Best By:   \n                    \n                    \n                        Apr 19, 2023\n                    \n                \n                \n                    \n                        In this blog post, you'll implement the Adam optimizer and perform some simple experiments.\n                    \n                \n            \n                \n                        Learning Objectives: \n                        \n                            Theory\n                                | \n                            Experimentation\n            \n        \n            \n        \n\n\nNo matching items\n\n\n\n\nProject\n\n\n\n\nProject\nProject Proposal\nBest By:  \nApr 7, 2023\n\n\n\n\n\nLearning Objectives: Project\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-erm.html",
    "href": "assignments/blog-posts/blog-post-erm.html",
    "title": "Empirical Risk Minimization For Classification",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n\n\n1 Introduction\nLet’s do empirical risk minimization!\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-limits-of-quantitative.html",
    "href": "assignments/blog-posts/blog-post-limits-of-quantitative.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "This one of two possible blog posts for this week. This blog post is a research essay on the limitations of the quantitative approach to analyzing bias and discrimination. If you’d rather work with some data and perform a bias audit of a data set, see the alternative assignment.\n\n\n\n\nWhat You Should Do\nQuantitative methods for assessing discrimination and bias include things like:\n\nFormal (mathematical) definitions of bias and fairness in terms.\nAudits of machine learning algorithms, including things like confusion matrices and false positive rates.\nStatistical tests of significance for effects related to race, gender, or other protected attributes.\n\nIn a recent speech, Narayanan (2022, 25) asserts that\n\n“currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good.”\n\nIn a carefully-structured essay of approximately 1,500 words, engage at least 5 scholarly sources (in addition to Narayanan’s speech) to discuss this claim. Your essay should include:\n\nA careful explanation of Narayanan’s position.\nA careful explanation of the uses or benefits of quantitative methods, as described in one of your scholarly sources.\nAppropriate supporting points from your other scholarly sources.\nAn argument in which you stake out a position on Narayanan’s point of view. Do you agree? Disagree? Agree with qualifications? Which ones? Why?\n\n\n\nReferences in Quarto\nTo manage references in Quarto, you need to create a .bib file (you can call it refs.bib). This file should live in the same directory as your blog post. Your .bib file is essentially a database of document information. Here’s an example of a a refs.bib file:\n@article{hardt2021patterns,\n  title  = {Patterns, predictions, and actions: A story about machine learning},\n  author = {Hardt, Moritz and Recht, Benjamin},\n  journal= {arXiv preprint arXiv:2102.05242},\n  year   = {2021}\n}\n\n@article{kearns1994toward,\n  title     = {Toward Efficient Agnostic Learning},\n  author    = {Kearns, Michael J and Schapire, Robert E and Sellie, Linda M},\n  journal   = {Machine Learning},\n  volume    = {17},\n  pages     = {115--141},\n  year      = {1994},\n  publisher = {Citeseer}\n}\n\n@misc{narayanan2022limits,\n  author       = {Narayanan, Arvind},\n  howpublished = {Speech},\n  title        = {The limits of the quantitative approach to discrimination},\n  year         = {2022}\n}\nThe simplest way to get entries for your references is to look them up on Google Scholar.\n\nSearch for the document you want.\nClick the “Cite” link underneath and choose “Bibtex” from the options at the bottom.\nCopy and paste the contents of the new page to your refs.bib file.\n\nOnce you’ve assembled your references, add the following line to your document metadata (the stuff in the top cell of your Jupyter notebook)\nbibliography: refs.bib\nOnce you’ve followed these steps, you’re ready to cite! You can reference your documents using the @ symbol and their bibliographic key, which is the first entry for each document in the refs.bib file. For example, typing\n@hardt2021patterns\nresults in the reference\nHardt and Recht (2021)\nas well as an entry in the “References” section at the end of your blog post.\nFor more on how to handle citations in Quarto, check the Quarto documentation.\n\n\n\n\n\n  © Phil Chodrow, 2023References\n\nHardt, Moritz, and Benjamin Recht. 2021. “Patterns, Predictions, and Actions: A Story about Machine Learning.” arXiv Preprint arXiv:2102.05242.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "$$\n$$\nThe folktables package allows you to download and neatly organize data from the American Community Survey’s Public Use Microdata Sample (PUMS). You can install it in your ml-0451 environment by running the following two commands in your terminal:\nYou can learn more about the folktables package, including documentation and examples, on the package’s GitHub page.\nIn this blog post, you’ll fit a classifier using data from folktables and perform a bias audit for the algorithm.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html#using-folktables",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html#using-folktables",
    "title": "Auditing Allocative Bias",
    "section": "1 Using folktables",
    "text": "1 Using folktables\nThe first thing to do is to download some data! Here’s an illustration of downloading a complete set of PUMS data for the state of Alabama.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"AL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000049\n6\n1\n1600\n3\n1\n1013097\n75\n19\n...\n140\n74\n73\n7\n76\n75\n80\n74\n7\n72\n\n\n1\nP\n2018GQ0000058\n6\n1\n1900\n3\n1\n1013097\n75\n18\n...\n76\n78\n7\n76\n80\n78\n7\n147\n150\n75\n\n\n2\nP\n2018GQ0000219\n6\n1\n2000\n3\n1\n1013097\n118\n53\n...\n117\n121\n123\n205\n208\n218\n120\n19\n123\n18\n\n\n3\nP\n2018GQ0000246\n6\n1\n2400\n3\n1\n1013097\n43\n28\n...\n43\n76\n79\n77\n80\n44\n46\n82\n81\n8\n\n\n4\nP\n2018GQ0000251\n6\n1\n2701\n3\n1\n1013097\n16\n25\n...\n4\n2\n29\n17\n15\n28\n17\n30\n15\n1\n\n\n\n\n5 rows × 286 columns\n\n\n\nThere are approximately 48,000 rows of PUMS data in this data frame. Each one corresponds to an individual citizen of the given STATE who filled out the 2018 edition of the PUMS survey. You’ll notice that there are a lot of columns. In the modeling tasks we’ll use here, we’re only going to focus on a relatively small number of features. Here are all the possible features I suggest you use:\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n19\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n1\n18\n18.0\n5\n17\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n2\n2\n6.0\n\n\n2\n53\n17.0\n5\n16\n1\nNaN\n1\n1.0\n4.0\n2\n1\n2\n2\n1.0\n1\n1\n6.0\n\n\n3\n28\n19.0\n5\n16\n2\nNaN\n1\n1.0\n2.0\n1\n1\n2\n2\n2.0\n1\n1\n6.0\n\n\n4\n25\n12.0\n5\n16\n1\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n1.0\n2\n1\n6.0\n\n\n\n\n\n\n\nFor documentation on what these features mean, you can consult the appendix of the paper that introduced the package.\nFor a few examples:\n\nESR is employment status (1 if employed, 0 if not)\nRAC1P is race (1 for White Alone, 2 for Black/African American alone, 3 and above for other self-identified racial groups)\nSEX is binary sex (1 for male, 2 for female)\nDEAR, DEYE, and DREM relate to certain disability statuses.\n\nLet’s consider the following task: we are going to\n\nTrain a machine learning algorithm to predict whether someone is currently employed, based on their other attributes not including race, and\nPerform a bias audit of our algorithm to determine whether it displays racial bias.\n\nFirst, let’s subset the features we want to use:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow we can construct a BasicProblem that expresses our wish to use these features to predict employment status ESR, using the race RAC1P as the group label. I recommend you mostly don’t touch the target_transform, preprocess, and postprocess columns. You can find examples of constructing problems in the folktables source code if you really want to carefully customize your problem.\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nThe result is now a feature matrix features, a label vector label, and a group label vector group, in convenient format with which we can work.\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(47777, 15)\n(47777,)\n(47777,)\n\n\nBefore we touch the data any more, we should perform a train-test split:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nNow we are ready to create a model and train it on the training data:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), LogisticRegression())\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression())])StandardScalerStandardScaler()LogisticRegressionLogisticRegression()\n\n\nWe can then extract predictions on the test set like this:\n\ny_hat = model.predict(X_test)\n\nThe overall accuracy in predicting whether someone is employed is:\n\n(y_hat == y_test).mean()\n\n0.7842193386354123\n\n\nThe accuracy for white individuals is\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7838255977496483\n\n\nThe accuracy for Black individuals is\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.7838630806845965\n\n\nWe can also calculate confusion matrices, false positive rates, false negative rates, positive predictive values, prevalences, and lots of other information using tools we’ve already seen."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html#what-you-should-do",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html#what-you-should-do",
    "title": "Auditing Allocative Bias",
    "section": "2 What You Should Do",
    "text": "2 What You Should Do\n\nChoose Your Problem\nChoose a prediction problem (target variable), a list of features, and a choice of group with respect to which to evaluate bias. I would suggest one of the following two possibilities:\n\n(What we just illustrated): predict employment status on the basis of demographics excluding race, and audit for racial bias.\nPredict whether income is over $50K on the basis of demographics excluding sex, and audit for gender bias.\n\n You can also pick the state from which you would like to pull your data.Do not audit for racial bias in VT, as we didn’t have enough Black individuals fill out the PUMS survey. 😬\nFinally, you should choose a machine learning model. While you can use a model like logistic regression that you’ve previously implemented, my suggestion is to use one out of the box from scikit-learn. Some simple classifiers with good performance are:\n\nsklearn.linear_model.LogisticRegression\nsklearn.svm.SVC (support vector machine)\nsklearn.tree.DecisionTreeClassifier (decision tree)\nsklearn.ensemble.RandomForestClassifier (random forest)\n\n\n\nBasic Descriptives\nUse simple descriptive analysis to address the following questions. You’ll likely find it easiest to address these problems when working with a data frame. Here’s some code to turn your training data back into a data frame for easy analysis:\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\nUsing this data frame, answer the following questions:\n\nHow many individuals are in the data?\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\nOf these individuals, how many are in each of the groups?\nIn each group, what proportion of individuals have target label equal to 1?\nCheck for intersectional trends by studying the proportion of positive target labels broken out by your chosen group labels and an additional group labe. For example, if you chose race (RAC1P) as your group, then you could also choose sex (SEX) and compute the proportion of positive labels by both race and sex. This might be a good opportunity to use a visualization such as a bar chart, e.g. via the seaborn package.\n\n\n\nTrain Your Model\nTrain your model on the training data. Please incorporate a tunable model complexity and use cross-validation in order to select a good choice for the model complexity. Some possibilities:\n\nUse polynomial features with LogisticRegression.\nTune the regularization parameter C in SVC.\nTune the max_depth of in DecisionTreeClassifier and in RandomForestClassifier.\n\n\n\nAudit Your Model\nThen, perform an audit in which you address the following questions (all on test data):\n\nOverall Measures\n\nWhat is the overall accuracy of your model?\nWhat is the positive predictive value (PPV) of your model?\nWhat are the overall false negative and false positive rates (FNR and FPR) for your model?\n\n\n\nBy-Group Measures\n\nWhat is the accuracy of your model on each subgroup?\nWhat is the PPV of your model on each subgroup?\nWhat are the FNR and FPR on each subgroup?\n\n\n\nBias Measures\nSee Chouldechova (2017) for definitions of these terms. For calibration, you can think of the score as having only two values, 0 and 1.\n\nIs your model approximately calibrated?\nDoes your model satisfy approximate error rate balance?\nDoes your model satisfy statistical parity?\n\n\n\n\nConcluding Discussion\nIn a few paragraphs, discuss the following questions:\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial use?\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-bias-allocative.html#optional-extras",
    "href": "assignments/blog-posts/blog-post-bias-allocative.html#optional-extras",
    "title": "Auditing Allocative Bias",
    "section": "3 Optional Extras",
    "text": "3 Optional Extras\n\nIntersectional Bias?\nAs an optional component of your bias audit, you could consider checking for intersectional bias in your model. For example, is the FNR significantly higher for Black women than it is for Black men or white women?\nTo address this question, you’ll likely find it is easier to work with a data frame again.\n\nimport pandas as pd\ndf = pd.DataFrame(X_test, columns = features_to_use)\ndf[\"group\"] = group_test\ndf[\"label\"] = y_test\n\n\n\nFeasible FNR and FPR Rates\nAs an optional component of your bias audit, you could reproduce Figure 5 in Chouldechova (2017) (link). This figure uses Eq. (2.6), fixing the prevalence (proportion of true positive labels) \\(p\\) for each group, as well as a desired PPV that should be the same across both groups. With these numbers fixed, eq. (2.6) then defines a line of feasible FNR and FPR rates, which you could plot. Don’t worry about reproducing the shaded regions unless you really want to."
  },
  {
    "objectID": "assignments/blog-posts/solutions/logistic/experiments.html",
    "href": "assignments/blog-posts/solutions/logistic/experiments.html",
    "title": "",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom logistic import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfrom logistic import LogisticRegression, gradient, stochastic_gradient\n\n# for step in [gradient, stochastic_gradient]:\n\n# LR = LogisticRegression()\n# LR.fit_stochastic(X, y, k = 1, max_iter = 10000)\n# plt.plot(LR.history, label = \"stochastic gradient\")\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, m_epochs = 100, momentum = True, batch_size = 1, alpha = .05)\nplt.plot(LR.history, label = \"stochastic gradient (momentum)\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, m_epochs = 100, momentum = False, batch_size = 1, alpha = .05)\nplt.plot(LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05)\nplt.plot(LR.history, label = \"gradient\")\nplt.loglog()\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7acace32e0&gt;\n\n\n\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nw = LR.w\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\n\n\n\n\n\n\n[]\n\n\n\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html",
    "href": "assignments/blog-posts/blog-post-image-processing.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "$$\n$$\nThis is a two-part blog post on linear algebra methods for unsupervised learning with two kinds of data: images and graphs. You can be a little bit choosy in which parts of this blog post you want to do. Doing either of the two main parts is fine. You can also choose to do just one of the main parts and its corresponding “optional extras.”\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#application-to-images",
    "href": "assignments/blog-posts/blog-post-image-processing.html#application-to-images",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Application to Images",
    "text": "Application to Images\nThe following function will read an image for you from a URL and save it as a numpy array.\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nI will choose an image of Maru. Maru is a Cat On The Internet who is famous for doing stuff like this:\n\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\nMy image is an RGB color image (I suggest you find an RGB image as well). In the code below, I’ll convert it to greyscale.\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nMy grey_img is now a simple (but large) matrix:\n\ngrey_img.shape\n\n(413, 640)\n\n\nThis means that I can use my SVD pipeline to construct approximations of my image. This task is called image compression and it is an important problem for storing large quantities of images on computers that may have small amounts of storage."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do",
    "href": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "What You Should Do",
    "text": "What You Should Do\nIn your blog post:\n\nAccess your favorite RGB image from the internet by its URL, download it, and convert it to greyscale using the workflow shown above.\nWrite a function called svd_reconstruct that reconstructs an image from its singular value decomposition. Your function should have two arguments: the image to reconstruct, and the number k of singular values to use.\nPerform an experiment in which you reconstruct your image with several different values of k. Your choice of k should go up at least until you can’t distinguish the reconstructed image from the original by eye. As part of your experiment, you should determine the amount of storage needed for your reconstruction as a fraction of the amount of storage needed for the original image.\n\nHint: An \\(m\\times n\\) greyscale image needs \\(mn\\) pixels (numbers) to represent it. How many numbers must be stored to reconstruct this image with k components using the SVD?\n\n\nHere’s an example of output from your experiment:\n\nfrom solutions.images import svd_reconstruct, svd_experiment\nsvd_experiment(grey_img)\n\n\n\n\nIn your blog post, include all your implementation code. Use comments and surrounding text to discuss your solution, and comment on your findings."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#optional-extras",
    "href": "assignments/blog-posts/blog-post-image-processing.html#optional-extras",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Optional Extras",
    "text": "Optional Extras\nImplement and demonstrate one or more of the following functionalities in your svd_reconstruct function:\n\nAllow the user to specify a desired compression factor and select the number of components k to use based on this selection.\nAllow the user to specify a desired threshold epsilon for the singular values. Then, only components for which the corresponding singular value is larger than epsilon are used."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#introduction",
    "href": "assignments/blog-posts/blog-post-image-processing.html#introduction",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Introduction",
    "text": "Introduction\nIn lecture, we discussed the Laplacian spectral clustering algorithm as a method for finding interesting clusters in point cloud data sets by operating on a graph. Spectral clustering doesn’t only work on point clouds, however; we can use it on any data that we can represent as a graph, like social networks.\nHere’s a famous social network:\n\nimport networkx as nx\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nFigure 1: The karate club graph\n\n\n\n\nThis graph is often called the “Karate Club Graph.” Each node (blue dot) represents an individual member of a karate club. Edges between them are measurements of social ties by the researcher Zachary; informally, you can think of two connected nodes as having interacted in a friendly social setting.\nBUT: things didn’t stay friendly for long! The reason that this data set is so famous is that it provides a relatively pure case study of the process of graph fission. In this case, the karate club studied eventually broke into two separate clubs after a conflict between the instructor (“Mr. Hi”) and the club president (“Officer”). Node 0 is Mr. Hi himself, and Node 33 is the club president. This information is present as a node attribute in the data:\n\nclubs = nx.get_node_attributes(G, \"club\")\n\nWe can draw the graph with this data like this, using the node_color attribute to control the node colors.\n\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\nFigure 2: The karate club graph with two labeled clubs after fission\n\n\n\n\nThe fundamental question of community detection is: can we predict divisions like this based only on the social ties? That is: could we have looked at Figure 1 (NOT Figure 2) and made a guess that the club might split on approximately these lines?\nA bit more abstractly, the community detection problem is to divide an observed graph into interpretable or important components, often called “communities” or “clusters.” There are lots of algorithms for this problem, but one of them is spectral clustering! We can extract an adjacency matrix for the graph like this:\n\nA = nx.adjacency_matrix(G).toarray()"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do-1",
    "href": "assignments/blog-posts/blog-post-image-processing.html#what-you-should-do-1",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "What You Should Do",
    "text": "What You Should Do\n\nImplement a function called spectral_clustering that accepts a graph G as an argument and returns a vector of binary labels that split the graph. Show your implementation in your blog post, and include comments describing each of the steps. Your implementation should be relatively short (10 lines is more than enough), but you might need to do a little research into functions like np.linalg.eig in order to better understand how they represent eigenvectors in their return value.\nShow a plot of the graph like the ones above, using the labels that you found with your algorithm.\nDiscuss the extent to which the labels found by your algorithm match the actual club division."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-image-processing.html#optional-extras-1",
    "href": "assignments/blog-posts/blog-post-image-processing.html#optional-extras-1",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "Optional Extras",
    "text": "Optional Extras\n\nImplement multiway spectral clustering. In multiway spectral clustering, our aim is to split the graph into \\(k\\) pieces, where \\(k &gt; 2\\). Then, demonstrate multiway spectral clustering on the karate club network, or any other network data you can find. Do the results look reasonable to you by eye?\n\nThere are several approaches to this problem. Here’s one way:\n\nRetrieve the \\(k\\) eigenvectors corresponding to the eigenvalues smallest in magnitude.\nTreat these \\(k\\) eigenvectors as defining a matrix \\(\\mathbf{U} \\in \\mathbb{R}^{n\\times k}\\).\nPerform k-means clustering on this matrix with \\(k\\) centroids, and return the corresponding labeling.\n\n\nPropose a measure of similarity between two categorical labelings \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\). Your measure should be 1 if \\(\\mathbf{z}_1\\) and \\(\\mathbf{z}_2\\) split the graph into exactly the same clusters, should be no smaller than 0 in any scenario. Note that a challenge for this problem is that your measure should be permutation invariant: if \\(\\mathbf{z}_1\\) is the same as \\(\\mathbf{z}_2\\) except with the 0s and 1s swapped, they still correspond to the same clustering and should be considered identical. Then, implement your measure and use it quantitatively compare the clusters found by spectral clustering with the true club split.\n\nThis problem is harder than it sounds! But there are lots of good approaches, so please feel free to be creative."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#kernel-logistic-regression",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#kernel-logistic-regression",
    "title": "Kernel Logistic Regression",
    "section": "1.1 Kernel Logistic Regression",
    "text": "1.1 Kernel Logistic Regression\nIn the kernel logistic regression problem, we instead solve empirical risk minimization with modified features. The empirical risk is now\n\\[\nL_k(\\mathbf{v}) = \\frac{1}{n} \\sum_{i = 1}^n \\ell(\\langle \\mathbf{v}, \\boldsymbol{\\kappa}(\\mathbf{x}_i) \\rangle, y_i)\\;,\n\\tag{1}\\]\nwhere \\(\\mathbf{v}\\in \\mathbb{R}^n\\) (not  \\(\\mathbb{R}^p\\)). The modified feature vector \\(\\boldsymbol{\\kappa}(\\mathbf{x}_i)\\) has entries\n\\[\n\\boldsymbol{\\kappa}(\\mathbf{x}_i) = \\left( \\begin{matrix}\n    k(\\mathbf{x}_1, \\mathbf{x}_i) \\\\\n    k(\\mathbf{x}_2, \\mathbf{x}_i) \\\\\n    \\vdots \\\\\n    k(\\mathbf{x}_n, \\mathbf{x}_i)\n\\end{matrix}\\right)\\;.\n\\]\nHere, \\(k:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\) is the kernel function. Kernel functions need to satisfy some special mathematical properties. We’re not going to code them up; instead we’re going to use some built-in functions from scikit-learn to handle the kernel functions for us.\nOnce the model has been trained and an optimal \\(\\hat{\\mathbf{v}}\\) has been obtained, one can then make a prediction using the formula\n\\[\n\\hat{y} = \\langle \\hat{\\mathbf{v}}, \\boldsymbol{\\kappa}(\\mathbf{x}) \\rangle\\;.\n\\]\nIf it is desired to return a 0-1 label instead of a real number, one can return \\(\\mathbb{1}[\\hat{y} &gt; 0]\\)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#implement-kernel-logistic-regression",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#implement-kernel-logistic-regression",
    "title": "Kernel Logistic Regression",
    "section": "2.1 Implement Kernel Logistic Regression",
    "text": "2.1 Implement Kernel Logistic Regression\nImplement a Python class called KernelLogisticRegression. You’ll be able to use it like the example in the previous section. Your class should implement the following methods:\nIf you’re not sure how to use **kwargs in Python functions and methods, you might want to check this resource.\n\n__init__(self, kernel, **kernel_kwargs) should accept a kernel function and a set of named keyword arguments called kernel_kwargs. All the __init__() method should do is to save these items as instance variables called\n\nself.kernel\nself.kernel_kwargs\n\nfit(self, X, y) will again be the method that learns the optimal parameters \\(\\hat{v}\\). The fit method is going to look a little different this time:\n\nFirst, pad X to make sure that X contains a column of 1s. Here’s a function to do this:\n\n    def pad(X):\n        return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nSave X as an instance variable called self.X_train.\nCompute the kernel matrix of X with itself. If you implemented __init__() correct, this can be done with the call\n\nkm = self.kernel(X_, X_, **self.kernel_kwargs)\n\nMinimize the empirical risk Equation 1. You might find it useful to define a separate function for computing the empirical risk. Note that the predictor is still an inner product, just with a different parameter vector \\(\\mathbf{v}\\) and a different matrix column \\(\\boldsymbol{\\kappa}(\\mathbf{x}_i)\\). This means that, if you’re careful, you can compute the entire empirical risk using just one matrix multiplication!\n\nHowever you find it, save the resulting optimal value of \\(\\mathbf{v}\\) as self.v.\nYou should still use the logistic loss for \\(\\ell\\).\nYou will probably need to choose a random initial \\(\\mathbf{v}\\). Don’t forget that \\(\\mathbf{v}\\) should have length equal to the number of data points, not the number of features.\n\nIf you’ve already implemented gradient descent for logistic regression in this blog post, then it’s not too hard to adapt your method to kernel logistic regression. However, it’s also fine to use the function scipy.optimize.minimize() as demonstrated in this lecture.\n\npredict(self, X) should accept a new feature matrix and return binary labels \\(\\{0,1\\}\\). For each row of \\(\\mathbf{X}\\), the prediction is obtained using the formula \\(\\mathbb{1}[\\langle \\hat{\\mathbf{v}}, \\boldsymbol{\\kappa}(\\mathbf{x}) \\rangle]\\). To do this:\n\nCompute the kernel matrix between self.X_train and the new feature input X. Each column of this matrix is \\(\\boldsymbol{\\kappa}(\\mathbf{x}_j)\\) for some \\(j\\).\nCompute inner products of the form \\(\\langle \\mathbf{v}, \\boldsymbol{\\kappa}(\\mathbf{x}_j) \\rangle\\). If the user supplies a matrix X with multiple columns, you should be able to compute all the predictions at once. This can be done efficiently using matrix multiplication.\nFinally, return a binary vector \\(\\hat{\\mathbf{y}}\\) whose \\(j\\)th entry is \\(\\hat{y}_j = \\mathbb{1}[\\langle \\mathbf{v}, \\mathbf{x}_j \\rangle &gt; 0]\\).\n\nscore(self, X, y) computes the accuracy of the model predictions on the feature matrix X with labels y.\n\nYou can assume that the user will always only call predict and score after calling fit. If you’d like, you’re welcome to add warnings or handle other cases in which the user may be less cooperative and attempt to call one of those methods first.\nMy complete implementation of kernel logistic regression was about 50 lines of code, excluding comments.\nDocstrings are not expected for this blog post."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#experiments",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#experiments",
    "title": "Kernel Logistic Regression",
    "section": "2.2 Experiments",
    "text": "2.2 Experiments\n\nBasic Check\nOnce you’re done, you’ll be able to import and and use your function like this.\nfrom kernel_logistic import KernelLogisticRegression # your source code\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons, make_circles\n\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nHere, the rbf_kernel is the kernel function and gamma is a parameter to that kernel function that says how “wiggly” the decision boundary should be. Larger gamma means a more wiggly decision boundary.\nYour implementation is likely correct when you can generate new synthetic versions of the data set above (just call make_moons again) and achieve accuracy consistently at or above 90%. To check that, you can just run the code block above a few times.\n\n\nChoosing gamma\nWhen we choose a very large value of gamma, we can achieve a very wiggly decision boundary with very good accuracy on the training data. For example:\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 10000)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\nplot_decision_regions(X, y, clf = KLR)\nt = title = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n1.0\n\n\n\n\n\nHere, our classifier draws a little orange blob around each orange data point: points very nearby are classified as orange while other points are classified as blue. This is sufficient to achieve 100% accuracy on the training data. But this doesn’t generalize: generate some new data and we’ll see much worse performance:\n\n# new data with the same rough pattern\nX, y = make_moons(200, shuffle = True, noise = 0.2)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nWhoops! Not so good. We say that the validation or testing accuracy of the classifier is quite low. Cases in which the validation accuracy is low even though the training accuracy is high are classic instances of overfitting.\n Design an experiment in which you fit your model for several different values of gamma. Show accuracy on both training data (the data on which the model was fit) and testing data (data generated from the same settings but which the model has never seen before). Please show your findings in the form of an attractive visualization with clear labels and a clear message.My suggestion is to choose gamma in 10**np.arange(-5, 6)\n\n\nVary the Noise\nRepeat your experiment with at least two other values of the noise parameter to make_moons. The noise determines how spread out the two crescents of points are. Do your findings suggest that the best value of gamma depends much on the amount of noise?\n\n\nTry Other Problem Geometries\nUse the make_circles function to generate some concentric circles instead of crescents. Show a few examples with varying amounts of noise. Can you find some values of gamma that look like they lead to good learning performance for this data set? Here’s an example of a fairly successful classifier: both the points and the accuracy are computed on unseen test data.\n\n\n0.96"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-kernel-logistic.html#blog-post",
    "href": "assignments/blog-posts/blog-post-kernel-logistic.html#blog-post",
    "title": "Kernel Logistic Regression",
    "section": "2.3 Blog Post",
    "text": "2.3 Blog Post\nYour blog post should describe your approach to your code and written descriptions of your experiments.\n\nPlease include a walk-through for your user of how you computed the empirical loss.\nPlease make sure that your figures are appropriately labeled and described.\nPlease make sure to include a link to the GitHub page containing your source code at the very beginning of the blog post.\n\nIn case you’re curious, it’s possible to add formal captions to your figures in Quarto. This makes things look a little fancier, but is not required!\nOnce you’re happy with how things look, render your blog, push it to GitHub, and submit a link to the URL of your blog post on Canvas."
  },
  {
    "objectID": "assignments/blog-posts/adam.html",
    "href": "assignments/blog-posts/adam.html",
    "title": "Optimization with Adam",
    "section": "",
    "text": "This is a relatively theoretical blog post on which I am intentionally giving you relatively little guidance. The purpose is to offer a challenge for students who have already done most of the previous blog posts. If you have already completed most of the previous posts, then I encourage you to give this one a try! Otherwise, I suggest that you instead focus on completing some of the prior posts.\nThe Adam optimization algorithm is a mainstay of modern deep learning. You can think of Adam as fancy gradient descent. It still uses gradient information, but processes that information in a more complex way that often produces state-of-the-art performance in modern large-scale tasks.\nAdam was introduced by Kingma and Ba (2015), in a paper which has been cited over 140,000 times.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/adam.html#what-you-should-do",
    "href": "assignments/blog-posts/adam.html#what-you-should-do",
    "title": "Optimization with Adam",
    "section": "1 What You Should Do",
    "text": "1 What You Should Do\n\nRead the Adam Paper\nStart by reading the paper that introduced Adam. You can focus on sections 1, 2, 3, and 6.1, skipping the others. It’s ok for your first reading to be relatively quick; you’ll want to go back through the sections multiple times as you do your implementation and experiments.\n\n\nImplement Adam for Logistic Regression\nExpand your implementation of logistic regression to include a version of the fit method that uses the Adam algorithm (Algorithm 1). If you implemented gradient as a separate function, you can just use that function within your implementation of Adam.\nThe authors of the paper frame their algorithm as minimizing a stochastic (random) objective function \\(f\\). This is a fancy mathematical way to talk about stochastic batch gradient descent. When they talk about evaluating/differentiating the “stochastic function” \\(f\\), you can instead think of “evaluating/differentiating \\(f\\) on a subset of the data,” just like we did in gradient descent for logistic regression. So, you can think of \\(g_t\\) in their algorithm as the gradient evaluated on a batch of the data. You can still follow the standard structure of stochastic gradient descent in which you loop through the entire data set in one epoch, reshuffle the batches, and do it again.\nYour implementation should allow the user to pass five arguments:\n\nbatch_size, the batch size for computing gradients. This works in the same way as it did in our initial implementation of stochastic gradient descent, and you can use much of the same code.\nalpha, the step-size.\nbeta_1 and beta_2, the moment estimate decay rates.\nw_0, the initial guess for the weight vector. Personally I would suggest giving this argument a default None value and, in the case that the user does not pass w_0, initialize it randomly with the correct dimensions.\n\n\n\nPerform a Basic Experiment\nRedo some of the simple experiments from implementation of logistic regression. Compare Adam optimization to standard stochastic gradient descent with a few different parameter choices. Please measure both the number of epochs and the actual amount of time required to achieve convergence.\n\n\nPerform a Digits Experiment\nWe saw an example of loading and manipulating the digits data set in the reading on k-means.\nLoad the digits dataset from scikit-learn. Then, do one of two things:\n\nFilter the digits data set so that it only contains data with two class labels (e.g. 4s vs. 8s).\nOptional challenge: extend your implementation of logistic regression to handle multiple class labels. Doing this by hand is quite challenging and I only recommend it if you are feeling very ambitious. You might be able to get some inspiration from this implementation, though please don’t copy code and acknowledge the author if you find her example helpful.\n\nThen, do a version of the experiment in Section 6.1 of the Adam paper in which you again compare the efficiency of your Adam implementation against standard stochastic gradient descent in terms of both epochs and elapsed time.\n\n\nPerform One More Experiment\nFind a data set (any data set that interests you) on which you can perform classification with your Adam model. I recommend that the number of features not be very large (maybe no larger than 100). Then, again compare the performance of Adam to standard stochastic gradient descent on the data you found.\n\n\nWrite Your Post\nIn your written blog post, please:\n\nShow your Adam implementation. Include comments corresponding to the comments in Alg. 1 of the paper so that the reader understands which lines of code correspond to which lines of math.\nDescribe and discuss the findings from your experiments.\nDon’t forget to label your axes!!\nInclude an introductory section describing the purpose of the blog post and a summary section reflecting on your findings. In your summary paragraph, please include answers to the following questions:\n\nIn the authors’ assessment, what aspects of the Adam algorithm help it to converge so efficiently? (You don’t need to go deep into the math but you should be able to identify a qualitative features from their paper).\nDid you observe highly efficient performance in your experiments when compared to other methods? If not, can you offer a hypothesis about why?\nHow did the experience of implementing this algorithm compare to the experience of implementing standard stochastic gradient descent?\n\n\nFinally, don’t forget to label your axes!!"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html",
    "href": "assignments/blog-posts/blog-post-penguins.html",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "",
    "text": "This is one of two suggested options for a blog post this week. You might want to pick this option if some of the following bullet points describe you:\n\nYou are interested in practicing with machine learning models on a real data set.\nYou are less interested in learning new machine learning theory this week.\nYou are willing to read a little more about how to perform operations on data sets with Pandas and how to use other models implemented in scikit-learn.\n\nThe alternative has a more theoretical flavor.\nThe Palmer Penguins data set is a data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. The data contains physiological measurements for a number of individuals from each of three species of penguin:\nYou can access the (training) data like this:\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\nHere’s how the data looks:\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n27\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN46A1\nYes\n11/29/07\n44.5\n14.3\n216.0\n4100.0\nNaN\n7.96621\n-25.69327\nNaN\n\n\n1\nPAL0708\n22\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN41A2\nYes\n11/27/07\n45.1\n14.5\n215.0\n5000.0\nFEMALE\n7.63220\n-25.46569\nNaN\n\n\n2\nPAL0910\n124\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN67A2\nYes\n11/16/09\n41.4\n18.5\n202.0\n3875.0\nMALE\n9.59462\n-25.42621\nNaN\n\n\n3\nPAL0910\n146\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nDream\nAdult, 1 Egg Stage\nN82A2\nYes\n11/16/09\n39.0\n18.7\n185.0\n3650.0\nMALE\n9.22033\n-26.03442\nNaN\n\n\n4\nPAL0708\n24\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN85A2\nNo\n11/28/07\n50.6\n19.4\n193.0\n3800.0\nMALE\n9.28153\n-24.97134\nNaN\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#your-challenge",
    "href": "assignments/blog-posts/blog-post-penguins.html#your-challenge",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "Your Challenge",
    "text": "Your Challenge\nWe are going to consider the problem of predicting the species of a penguin based on its measurements.\n\nExplore: Construct at least one interesting displayed figure (e.g. using seaborn) and at least one interesting displayed table (e.g. using pandas.groupby().aggregate). Make sure to include a helpful discussion of both the figure and the table. Don’t just show the result: explain what you learned about the data from these products.\nModel: Find three features of the data and a model trained on those features which achieves 100% testing accuracy. You must obtain your three features through a reproducible process. That is, you can’t just pick them: you need to code up some kind of search in order to obtain them.\n\nOne feature must be qualitative (like Island or Clutch Completion).\nThe other two features must be quantitative (like Body Mass (g) or Culmen Depth (mm)).\n\nEvaluate: Show the decision regions of your finished model, split out by the qualitative feature.\n\nI’ve supplied code to help you with several parts of this task."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#resources-and-hints",
    "href": "assignments/blog-posts/blog-post-penguins.html#resources-and-hints",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "Resources and Hints",
    "text": "Resources and Hints\n\nThis Webpage Runs\nIf you run all the code on this assignment page in order, you’ll produce the result at the bottom of the page (possibly after installing some more packages in your ml-0451 Anaconda environment). So, one good way to approach this assignment is to take this code into a Jupyter notebook and start tweaking.\n\n\nData Preparation\nYou will need to prepare the qualitative columns in the data. Feature columns like Sex and Island should be coded using pd.get_dummies (as illustrated in lecture). The label column Species should be coded differently, using a LabelEncoder. The following function handles this work for you.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\nChoosing Features\nThis is where much of the work for this blog post lies. You need to choose 3 good features! One possibility is to use some of the tools described on this page. Another approach, which is ok to use on this data set, is exhaustive search of all the features contained in the data set. For this, the combinations function from the itertools package might be helpful.\nUSE CROSS-VALIDATION! This is your simplest way to guard against overfitting issues and get a good feeling for how your model might do on unseen data.\nIf you use the Island feature, you are allowed to use all of the columns that correspond to Island.\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    # \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\n\n\n\nModel Choices\nThere are three species of penguin in the data. Most classifiers in scikit-learn will handle multi-way classification without issue. For example:\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.6640625\n\n\nEven though y_train contains three categories (labeled 0, 1, and 2), we’re able to fit a LogisticRegression() no problem.\nSince scikit-learn makes it so easy to experiment, this blog post is a great opportunity to explore some out-of-the-box models that we haven’t discussed in class. I’d suggest:\n\nfrom sklearn.tree import DecisionTreeClassifier. This one has a max_depth parameter that controls the complexity of the model. Use cross-validation to find a good value of the parameter.\nfrom sklearn.ensemble import RandomForestClassifier. State-of-the-art before the rise of neural networks.\nfrom sklearn.svm import SVC. Another state-of-the-art algorithm before the rise of neural networks. Has a parameter gamma that controls the complexity of the model. Again, use cross-validation to select gamma. It’s important to let gamma cover a wide range of values, e.g. gamma = 10**np.arange(-5, 5).\n\nYou can find a more thorough listing of models on this page.\n\n\nTesting\nTo test your model, you should download the test data set and prepare it using the prepare_data function. You’ll need to make sure that you subset it using only the features you choose. Here’s code that does this:\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.6617647058823529\n\n\n\n\nPlotting Decision Regions\nTo plot decision regions for your model, we are going to use the mlxtend package. You may need to install this package first in your ml-0451 environment. Once you’ve done so, you can use the decision_region_panel function defined below to plot your regions.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nX_train[cols]\n\n\n\n\n\n\n\n\nFlipper Length (mm)\nBody Mass (g)\nClutch Completion_No\nClutch Completion_Yes\n\n\n\n\n1\n215.0\n5000.0\n0\n1\n\n\n2\n202.0\n3875.0\n0\n1\n\n\n3\n185.0\n3650.0\n0\n1\n\n\n4\n193.0\n3800.0\n1\n0\n\n\n5\n178.0\n2900.0\n0\n1\n\n\n...\n...\n...\n...\n...\n\n\n269\n190.0\n3900.0\n0\n1\n\n\n270\n211.0\n4800.0\n0\n1\n\n\n271\n187.0\n3150.0\n1\n0\n\n\n272\n224.0\n5350.0\n0\n1\n\n\n273\n210.0\n4600.0\n0\n1\n\n\n\n\n256 rows × 4 columns\n\n\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n Here it is in action. It’s safe to ignore these particular warnings.Note: this function assumes that your first two columns in X_train are quantitative and that the ones after that are qualitative.\n\nplot_regions(LR, X_train[cols], y_train)"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#explore",
    "href": "assignments/blog-posts/blog-post-penguins.html#explore",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "Explore!",
    "text": "Explore!\nPlease feel encouraged to be creative in your choices of data visualization, predictive model, and algorithm to compute your features. I also like pictures of penguins. =)"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-penguins.html#useful-resources",
    "href": "assignments/blog-posts/blog-post-penguins.html#useful-resources",
    "title": "Blog Post: Classifying Palmer Penguins",
    "section": "Useful Resources",
    "text": "Useful Resources\n\nAn introduction to seaborn, a convenient package for data visualization with data frames.\nData Manipulation with Pandas from the Python Data Science Handbook\nYou might be interested in some of the explanations of how some other classifiers work, including decision trees and support vector machines, also from the Python Data Science Handbook."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-medical-bias.html",
    "href": "assignments/blog-posts/blog-post-medical-bias.html",
    "title": "Bias in a Medical Recommender System",
    "section": "",
    "text": "© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html",
    "href": "assignments/blog-posts/blog-post-perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#source-code",
    "href": "assignments/blog-posts/blog-post-perceptron.html#source-code",
    "title": "Implementing the Perceptron Algorithm",
    "section": "3.1 Source Code",
    "text": "3.1 Source Code\nYour class should have the following methods:\n\nPerceptron.fit(X, y) is the primary method. This method has no return value. If p is a Perceptron, then after p.fit(X, y) is called, p should have an instance variable of weights called w. This w is the vector \\(\\tilde{\\vw} = (\\vw, -b)\\) in the classifier above. Additionally, p should have an instance variable called p.history which is a list of the evolution of the score over the training period (see Perceptron.score(X, y) below.)\nPerceptron.predict(X) should return a vector \\(\\hat{\\vy} \\in \\{0,1\\}^n\\) of predicted labels. These are the model’s predictions for the labels on the data.\nPerceptron.score(X, y) should return the accuracy of the perceptron as a number between 0 and 1, with 1 corresponding to perfect classification.\n\nFeel free to add any other methods or functions that you find helpful while implementing.\n\nImplementing fit()\nTo implement fit(), it’s convenient to consider a modified version of \\(\\mX\\): \\(\\tilde{\\mX} = [\\mX, \\mathbf{1}]\\), where \\(\\mathbf{1} \\in \\R^n\\) is a column-vector of \\(1\\)s. The reason this is handy is that if we also define \\(\\tilde{\\vw} = (\\vw, -b)\\), then we can write our classification rule as\n\\[\n\\hat{y}_i = \\mathbb{1}(\\langle \\tilde{\\vw}, \\tilde{\\vx}_i\\rangle \\geq 0)\\;.\n\\]\nThis is mathematically convenient and makes it much easier for us to code up our algorithms.\nWith these definitions, the perceptron algorithm proceeds as follows:\n\nFirst, initialize a random initial weight vector \\(\\tilde{\\vw}^{(0)}\\).\nThen, until termination:\n\n\nPick a random index \\(i \\in [n]\\).\nCompute \\[\n\\tilde{\\vw}^{(t+1)} = \\tilde{\\vw}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\vw}^{(t)}, \\tilde{\\vx}_i\\rangle &lt; 0)\\tilde{y}_i \\tilde{\\vx}_i\\;.\n\\tag{1}\\]\n\nIn this expression, \\(\\tilde{y}_i = 2y_i - 1\\) is a convenient version of \\(y_i\\) that takes values \\(-1\\) and \\(1\\) instead of \\(0\\) and \\(1\\).\nThis update is performed until either a user-specified maximum number of steps is reached or until the score (accuracy) reaches 1.0.\nNote that in an iteration in which \\(\\tilde{y}_i \\langle \\tilde{\\vw}^{(t)}, \\tilde{\\vx}_i\\rangle \\geq 0\\), nothing happens. Take a moment to check that this occurs when the current weight vector \\(\\tilde{\\vw}^{(t)}\\) correctly classifies the tuple \\((\\vx_i, y_i)\\).\n\n\nOther Specifications\nYou should be able to replicate the demo in Section 2 with your source code. Feel free to use that demo as a test case – your source code may be in good shape when you are able to fully replicate the results. For perfect replication, you’ll need to include the call to np.random.seed() immediately after importing your packages.\nAn excellent solution will have exactly one for-loop, of the form:\nfor _ in range(max_steps):\n  # perform the perceptron update and log the score in self.history\nThat is, you should not do any loops over the data! Use vectorized numpy operations and matrix-vector multiplication.\nYou should also not use if statements to perform comparisons between numbers.\n\n\nFor a hint on how you can avoid doing this, you can reflect on the following two code snippets:\nprint((1 &lt; 2)*2)\nprint((1 &gt; 2)*2)\n\n\n\n\n\n\nPlease include informative docstrings for Perceptron.fit(), Perceptron.predict(), and Perceptron.score().\n\n\n\nA concise solution should likely be no more than 60 lines of compact Python code (excluding comments and docstrings)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#experiments",
    "href": "assignments/blog-posts/blog-post-perceptron.html#experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "3.2 Experiments",
    "text": "3.2 Experiments\nPlease perform experiments (with visualizations) that illustrate the following:\n\nUsing 2d data like the data in the example, if the data is linearly separable then the perceptron algorithm converges to weight vector \\(\\tilde{\\vw}\\) describing a separating line (provided that the maximum number of iterations is large enough).\n\nPlease show visualizations of the data, the separating line, and the evolution of the accuracy over training. It’s also fine for you to use the loss instead of the accuracy if you’d prefer.\n\n\nFor 2d data, when the data is not linearly separable, the perceptron algorithm will not settle on a final value of \\(\\tilde{\\vw}\\), but will instead run until the maximum number of iterations is reached, without achieving perfect accuracy.\n\nPlease show visualizations of the data, the line in the final iteration, and the evolution of the score over training.\n\nThe perceptron algorithm is also able to work in more than 2 dimensions! Show an example of running your algorithm on data with at least 5 features. You don’t need to visualize the data or the separating line, but you should still show the evolution of the score over the training period. Include a comment on whether you believe that the data is linearly separable based on your observation of the score."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-perceptron.html#writing",
    "href": "assignments/blog-posts/blog-post-perceptron.html#writing",
    "title": "Implementing the Perceptron Algorithm",
    "section": "3.3 Writing",
    "text": "3.3 Writing\nIn crafting your blog post, please include the following components:\n\nAt the very top of your blog post, a link to your source code (perceptron.py) on your GitHub repo.\nA brief walk-through of your implementation of the perceptron update (Equation 1) in your source code. Quote the function which you use to perform the update. It’s not necessary to walk the user through every single aspect of your solution class.\nFull code and English descriptions for all the experiments you perform.\nAt the end of your blog post, please address the following question:\n\n\nWhat is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points \\(n\\)? What about the number of features \\(p\\)?\n\nYou only need to consider this question in the context of a single update. The question of how many updates are required to converge is a trickier one that you don’t have to discuss in your blog post."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html",
    "href": "assignments/blog-posts/blog-post-optimization.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vy}{\\mathbf{y}}\n\\newcommand{\\mX}{\\mathbf{X}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#introduction",
    "href": "assignments/blog-posts/blog-post-optimization.html#introduction",
    "title": "Optimization for Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\nIn , we introduced the gradient descent algorithm for optimization and showed it in action for the logistic regression problem. In this blog post, you’ll:\n\nImplement gradient descent for logistic regression in an object-oriented paradigm.\nImplement a key variant of gradient descent called stochastic gradient descent, including an optional momentum feature.\nPerform several simple experiments on synthetic data to see which of these algorithms converges most quickly to a satisfactory logistic regression model."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#implement-logistic-regression",
    "href": "assignments/blog-posts/blog-post-optimization.html#implement-logistic-regression",
    "title": "Optimization for Logistic Regression",
    "section": "1. Implement Logistic Regression",
    "text": "1. Implement Logistic Regression\nIn your source file, implement a LogisticRegression() class. Your class should have similar user-facing functions as the Perceptron() class from the previous blog post. These are:\n\nLogisticRegression.fit(X, y) is the primary method. This method has no return value. If LR is a LogisticRegression object, then after LR.fit(X, y) is called, LR should have an instance variable of weights called w. This w is the vector of weights, including the bias term \\(b\\). LR should have an instance variable called LR.loss_history which is a list of the evolution of the loss over the training period (see LogisticRegression.loss(X, y) below). Finally, LR should have an instance variable called LR.score_history which is a list of the evolution of the score over the training period (see LogisticRegression.score(X, y) below).\nLogisticRegression.predict(X) should return a vector \\(\\hat{\\vy} \\in \\{0,1\\}^n\\) of predicted labels. These are the model’s predictions for the labels on the data.\nLogisticRegression.score(X, y) should return the accuracy of the predictions as a number between 0 and 1, with 1 corresponding to perfect classification.\nLogisticRegression.loss(X, y) should return the overall loss (empirical risk) of the current weights on X and y.\n\n\nGradient Descent\nYour LogisticRegression.fit method should use gradient descent as described in lecture. Allow the user to specify the learning rate \\(\\alpha\\) and the maximum number of iterations, which for this blog post we’ll call epochs. So, using the fit method might look like this:\n\nfrom solutions.logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nStochastic Gradient Descent\nNow implement an alternative version of the fit method called fit_stochastic. In this method, you will implement stochastic gradient descent. In stochastic gradient descent, we don’t compute the complete gradient\n\\[\n\\nabla L(\\vw) = \\frac{1}{n}\\sum_{i = 1}^n \\nabla \\ell(f_{\\vw}(\\vx_i), y_i)\\;.\n\\]\nInstead, we compute a stochastic gradient by picking a random subset \\(S \\subseteq [n] = \\{1, \\ldots, n\\}\\) and computing\n\\[\n\\nabla_S L(\\vw) = \\frac{1}{\\abs{S}}\\sum_{i \\in S} \\nabla \\ell(f_{\\vw}(\\vx_i), y_i)\\;.\n\\]\nThe size of \\(S\\) is called the batch size. Typically, we cycle through all the points \\(1\\) through \\(n\\) in the following way:\n\nShuffle the points randomly.\nPick the first \\(k\\) random points, compute the stochastic gradient, and then perform an update.\nPick the next \\(k\\) random points and repeat..\nWhen we have gone through all \\(n\\) points, reshuffle them all randomly and proceed again.\n\nThis process can be accomplished efficiently using the np.array_split() function, which will create batches for you. Here is some code to get you started; it will split the data into batches of size batch_size\n\nn = X.shape[0]\nfor j in np.arange(m_epochs):\n            \n    order = np.arange(n)\n    np.random.shuffle(order)\n\n    for batch in np.array_split(order, n // batch_size + 1):\n        x_batch = X[batch,:]\n        y_batch = y[batch]\n        grad = gradient(w, x_batch, y_batch) \n        # perform the gradient step\n        # ...\nFor stochastic gradient descent, only update self.loss_history at the end of each epoch. This allows us to compare to regular gradient descent, since in both algorithms at the end of an epoch we have used every single point once.\n\n\nMomentum (Optional)\nThe momentum method is described on p. 85 of Hardt and Recht. Implement the momentum method for stochastic gradient descent. My advice is to do so as an optional parameter for fit_stochastic. In my implementation, if the user sets momentum = True then I set the parameter \\(\\beta\\) from Hardt and Recht to value 0.8. Otherwise it is set to 0, and we have regular gradient descent.\n\n\nIllustration\nHere is an example plot showing the evolution of the loss function for the three algorithms:\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nEvolution of the training loss for three optimization algorithms.\n\n\n\n\nFor these settings, stochastic gradient descent with and without momentum tends to get to a “pretty good” result faster than standard gradient descent, but these random algorithms can “bounce around” near the good solution. Standard gradient descent might need more epochs to find a good solution, but quickly “settles down” once it finds it."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#perform-experiments",
    "href": "assignments/blog-posts/blog-post-optimization.html#perform-experiments",
    "title": "Optimization for Logistic Regression",
    "section": "2. Perform Experiments",
    "text": "2. Perform Experiments\nAfter you have tested and implemented your class, please perform experiments in which you show examples of the following phenomena:\n\nA case in which gradient descent does not converge to a minimizer because the learning rate \\(\\alpha\\) is too large.\nA case in which the choice of batch size influences how quickly the algorithm converges.\nIf you implemented momentum, a case in which the use of momentum significantly speeds up convergence.\n\nIn at least one of these experiments, generate some synthetic data (it’s fine to use make_blobs) for data of at least 10 feature dimensions."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#document-and-write",
    "href": "assignments/blog-posts/blog-post-optimization.html#document-and-write",
    "title": "Optimization for Logistic Regression",
    "section": "3. Document and Write",
    "text": "3. Document and Write\nPlease include informative comments throughout your source code, and a thorough docstring for each of your fit and fit_stochastic methods.\nIn your blog post, please describe both your approach to implementing your algorithm and the findings of your experiments."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#submit",
    "href": "assignments/blog-posts/blog-post-optimization.html#submit",
    "title": "Optimization for Logistic Regression",
    "section": "4. Submit",
    "text": "4. Submit\nSubmit your blog post, making sure to include a link to the online version of your source code at the top of your post."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-optimization.html#tips-and-hints",
    "href": "assignments/blog-posts/blog-post-optimization.html#tips-and-hints",
    "title": "Optimization for Logistic Regression",
    "section": "Tips and Hints",
    "text": "Tips and Hints\nMost of the major math functions are shown in our lecture on gradient descent. You’re welcome to use any of these functions as you wish; please just incorporate comments in your code and blog post to cite where they came from.\nIf you compute the gradient using matrix-vector operations in numpy (recommended, no for-loops!), you may find it useful at some point to convert an np.array() of shape (n,) to an np.array() of shape (n,1) like this:\n\nv = np.random.rand(5)\nv\n\narray([0.34225992, 0.9025147 , 0.22511529, 0.59362563, 0.0987686 ])\n\n\n\nv[:,np.newaxis]\n\narray([[0.34225992],\n       [0.9025147 ],\n       [0.22511529],\n       [0.59362563],\n       [0.0987686 ]])\n\n\nYou will find it convenient again to ensure that X contains a column of 1s prior to any major computations. I defined this function:\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\nand called it at a few strategic places in my implementation.\nMy complete implementation, including all the math functions, momentum, etc. but excluding comments, was about 100 lines of code.\nYou’re welcome to find creative ways to visualize your findings. You might also find it interesting to visualize the score (not just the loss). However, this is optional."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html",
    "href": "assignments/blog-posts/blog-post-linear-regression.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "$$\n$$\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#implement-linear-regression-two-ways",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#implement-linear-regression-two-ways",
    "title": "Implementing Linear Regression",
    "section": "1 Implement Linear Regression Two Ways",
    "text": "1 Implement Linear Regression Two Ways\nTo start this blog post, please implement least-squares linear regression in two ways.\n\nFirst, use the analytical formula for the optimal weight vector \\(\\hat{\\mathbf{w}}\\) from the lecture notes. This formula requires matrix inversion and several matrix multiplications.\nNext, use the formula for the gradient of the loss function to implement gradient descent for linear regression. You can still pass in max_iter and alpha (the learning rate) as parameters to the fit method. \n\nImplementing stochastic gradient descent would be nice thing to do but not required. Only if you want to go above and beyond!In addition to the fit method, your implementation should include a score method (see below) and a predict method (just return \\(\\mathbf{X}\\mathbf{w}\\)).\nIt’s fine for you to either define separate methods like fit_analytic and fit_gradient for these methods. It’s also fine to define a single fit method with a method argument to determine which algorithm is used.\nAs usual, place your implementation in a source file where you will be able to implement it.\n\nThe Score\nLet \\(\\bar{y} = \\frac{1}{n} \\sum_{i = 1}^ny_i\\). Then, the score we’ll use is the so-called coefficient of determination, which is\n\\[\nc = 1 - \\frac{\\sum_{i = 1}^n (\\hat{y}_i - y_i)^2}{\\sum_{i = 1}^n (\\bar{y} - y_i)^2}\\;.\n\\]\nThe coefficient of determination is always no larger than 1, with a higher value indicating better predictive performance. It can be arbitrarily negative for very bad models. Note that the numerator in the fraction is just \\(L(\\mathbf{w})\\), so making the loss small makes the coefficient of determination large.\n\n\nEfficient Gradient Descent\nFor gradient descent, please implement a score_history so that you can visualize the value of the score over epochs.\nThe formula for the gradient is \\[\n\\nabla L(\\mathbf{w}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}- \\mathbf{y})\\;.\n\\] However, you should resist the urge to compute this formula “from scratch” at every iteration. The reason is that the matrix multiplication \\(\\mathbf{X}^T\\mathbf{X}\\) has time-complexity \\(O(np^2)\\), where \\(n\\) is the number of data points and \\(p\\) is the number of features. Similarly, the matrix-vector product \\(\\mathbf{X}^T\\mathbf{y}\\) has time-complexity \\(O(np)\\). Both of these can be pretty expensive if you have a lot of data points! Fortunately, they don’t depend on the current value of \\(w\\), so you can actually just precompute them:\n\nOnce during the fit method, compute \\(\\mathbf{P}= \\mathbf{X}^T \\mathbf{X}\\) and \\(\\mathbf{q}= \\mathbf{X}^T \\mathbf{y}\\).\nThe gradient is then \\(\\nabla L(\\mathbf{w}) = 2(\\mathbf{P}\\mathbf{w}- \\mathbf{q})\\).\n\nComputing \\(\\mathbf{P}\\mathbf{w}\\) requires only \\(O(p^2)\\) steps. In other words, precomputing \\(\\mathbf{P}\\) and \\(\\mathbf{q}\\) eliminates the dependence of the runtime on the number of data points – not bad!"
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#demo",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#demo",
    "title": "Implementing Linear Regression",
    "section": "2 Demo",
    "text": "2 Demo\nThe following function will create both testing and validation data that you can use to test your implementation:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\nHere’s an example of how to use the function to generate data. Unfortunately, it’s only possible to easily visualize this problem when p_features = 1.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nOnce you’ve impmlemented your solution, you should be able to use it like this:\n\nfrom solutions.linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.3562\nValidation score = 0.5122\n\n\nThe estimated weight vector \\(\\mathbf{w}\\) is\n\nLR.w\n\narray([0.53369045, 0.33787198])\n\n\nI can get the same value for \\(\\mathbf{w}\\) using gradient descent (it would be even closer if we allowed more iterations).\n\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method = \"gradient\", alpha = 0.01, max_iter = 1e2)\nLR2.w\n\narray([0.53504079, 0.33705831])\n\n\nI can also see how the score changed over time. Because we’re not using stochastic gradient descent, the score should increase monotonically in each iteration.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nYour implementation is likely correct when you are able to reproduce results that are similar to these (although small differences are no problem)."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#experiments",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "3 Experiments",
    "text": "3 Experiments\nOnce you’ve demonstrated the behavior above, perform an experiment in which you allow p_features, the number of features used, to increase, while holding n_train, the number of training points, constant. Try increasing p_features all the way to n_train - 1. What happens to the training score? What happens to the validation score? I’d suggest showing these results on a nice plot in which the horizontal axis is the number of features, the vertical axis is the score, and the training/validation scores are shown in different colors.\nOptional: Relate your findings when p_features = n_train-1 to the existence of a solution of the equation \\(\\mathbf{X}\\mathbf{w}= \\mathbf{y}\\). What do you know about the rank of \\(\\mathbf{X}\\)? Remember that the number of columns in \\(\\mathbf{X}\\) is actually p_features + 1, since it’s still necessary to pad with a column of 1s.\nWhen discussing your findings, make sure to connect them to the idea of overfitting."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#lasso-regularization",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "4 LASSO Regularization",
    "text": "4 LASSO Regularization\nThe LASSO algorithm uses a modified loss function with a regularization term:\n\\[\nL(\\mathbf{w}) = \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2 + \\alpha \\lVert \\mathbf{w}' \\rVert_1\\;.\n\\]\nHere, \\(\\mathbf{w}'\\) is the vector composed of all the entries of \\(\\mathbf{w}\\) excluding the very last entry. The 1-norm is defined as\n\\[\n\\lVert \\mathbf{w}' \\rVert_1 = \\sum_{j = 1}^{p-1} \\lvert w_j \\rvert\\;.\n\\]\nThe effect of the regularizing term is to make the entries of the weight vector \\(\\mathbf{w}\\) small. In fact, LASSO has a nice property: it tends to force entries of the weight vector to be exactly zero.  This is very desirable in so-called overparameterized problems, when the number of features \\(p\\) is larger than the number of data points \\(n\\).The reason we exclude the final entry of \\(\\mathbf{w}\\) is that it is not desirable to penalize the weight corresponding to the constant feature in \\(\\mathbf{X}\\).\nImplementing LASSO involves some more complicated mathematical optimization than we will discuss in this course, so instead we’ll use the implementation in scikit-learn. You can import it like this:\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nHere, alpha controls the strength of the regularization (it’s not related to the learning rate in gradient descent). Let’s fit this model on some data and check the coefficients:\n\np_features = n_train - 1\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\nL.fit(X_train, y_train)\n\nLasso(alpha=0.001)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.001)\n\n\nThe score on the validation set is high, which might be different from what you found with pure linear regression.\n\nL.score(X_val, y_val)\n\n0.541222047393935\n\n\n\nWhat You Should Do\nReplicate the same experiment you did with linear regression, increasing the number of features up to or even past n_train - 1, using LASSO instead of linear regression. You might want to experiment with a few values of the regularization strength alpha. Comment on how your validation score compares to standard linear regression when the number of features used is large."
  },
  {
    "objectID": "assignments/blog-posts/blog-post-linear-regression.html#optional-bikeshare-data-set",
    "href": "assignments/blog-posts/blog-post-linear-regression.html#optional-bikeshare-data-set",
    "title": "Implementing Linear Regression",
    "section": "5 Optional: Bikeshare Data Set",
    "text": "5 Optional: Bikeshare Data Set\nThe following code will download and save a data set related to the Capital Bikeshare system in Washington DC. We use the aggregated version graciously provided by the authors of the following paper:\n\nFanaee-T, Hadi, and Gama, Joao, “Event labeling combining ensemble detectors and background knowledge”, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg, doi:10.1007/s13748-013-0040-3.\n\nThis data set includes information about the season and time of year; the weather; and the count of bicycle users on each day for two years (year 0 is 2011, year 1 is 2012). This level of information gives us considerable ability to model phenomena in the data.\nFor more on what the entries in each column means, you can consult the data dictionary here (under “Attribute Information”).\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n1\n2011-01-01\n1\n0\n1\n0\n6\n0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n331\n654\n985\n\n\n1\n2\n2011-01-02\n1\n0\n1\n0\n0\n0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n131\n670\n801\n\n\n2\n3\n2011-01-03\n1\n0\n1\n0\n1\n1\n1\n0.196364\n0.189405\n0.437273\n0.248309\n120\n1229\n1349\n\n\n3\n4\n2011-01-04\n1\n0\n1\n0\n2\n1\n1\n0.200000\n0.212122\n0.590435\n0.160296\n108\n1454\n1562\n\n\n4\n5\n2011-01-05\n1\n0\n1\n0\n3\n1\n1\n0.226957\n0.229270\n0.436957\n0.186900\n82\n1518\n1600\n\n\n\n\n\n\n\nOur aim for this case study is to plot daily usage by casual users (as opposed to registered users). The total number of casual users each day is given by the casual column, Let’s plot this over time:\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nFor this prediction task, it’s handy to work with a smaller subset of the columns, and to transform the mnth column into dummy variables.\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n\n\n\ncasual\nweathersit\nworkingday\nyr\ntemp\nhum\nwindspeed\nholiday\nmnth_2\nmnth_3\nmnth_4\nmnth_5\nmnth_6\nmnth_7\nmnth_8\nmnth_9\nmnth_10\nmnth_11\nmnth_12\n\n\n\n\n0\n331\n2\n0\n0\n0.344167\n0.805833\n0.160446\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n131\n2\n0\n0\n0.363478\n0.696087\n0.248539\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n120\n1\n1\n0\n0.196364\n0.437273\n0.248309\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n108\n1\n1\n0\n0.200000\n0.590435\n0.160296\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n82\n1\n1\n0\n0.226957\n0.436957\n0.186900\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n726\n247\n2\n1\n1\n0.254167\n0.652917\n0.350133\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n727\n644\n2\n1\n1\n0.253333\n0.590000\n0.155471\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n728\n159\n2\n0\n1\n0.253333\n0.752917\n0.124383\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n729\n364\n1\n0\n1\n0.255833\n0.483333\n0.350754\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n730\n439\n2\n1\n1\n0.215833\n0.577500\n0.154846\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n731 rows × 19 columns\n\n\n\nNow we can do a train-test split.\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\nTrain an instance of your LinearRegression class on the bikeshare training data. Then:\n\nScore your model on the test set.\nCompute the predictions for each day and visualize them in comparison to the actual ridership on the test set.\nCompare the entries w of your model to the corresponding entry of X_train.columns in order to see which features your model found to contribute to ridership. Positive coefficients suggest that the corresponding feature contributes to ridership. Can you find effects corresponding to nice weather? Summer months? Holidays? Weekends?"
  },
  {
    "objectID": "assignments/project/proposal.html",
    "href": "assignments/project/proposal.html",
    "title": "Project Proposal",
    "section": "",
    "text": "The project proposal is your first formal step toward completing your project for this course. In this assignment, you’ll (a) create the shared GitHub repository that will hold your project files and (b) collaboratively create a written description of what you will do, how you will judge its success, and what you intend to learn from the process.\nPlease note that, in order to complete the proposal, you will need to be in active communication with your project partners. Here’s my suggestion:\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/project/proposal.html#create-the-project-repository",
    "href": "assignments/project/proposal.html#create-the-project-repository",
    "title": "Project Proposal",
    "section": "1 Create the Project Repository",
    "text": "1 Create the Project Repository\nOne member of the group should go on GitHub.com and create the project repository. This is the central location that will house your project files. After creating the project repository, this group member should add all other group members as collaborators on the project (under Settings –&gt; Manage Access).\nAll group members should now clone the repository, using either GitHub Desktop or the command line.\nDo not fork the project repository, as this will lead to everyone having their own private version. Not very collaborative!\nBy the end of this step, you now have a shared repository, under version control, in which you can all collaborate. This is where your project files, including code and data, will live."
  },
  {
    "objectID": "assignments/project/proposal.html#write-your-project-proposal",
    "href": "assignments/project/proposal.html#write-your-project-proposal",
    "title": "Project Proposal",
    "section": "2 Write Your Project Proposal",
    "text": "2 Write Your Project Proposal\nWrite your project proposal in your project repository. For now, you can just use the file README.md to hold your proposal; this has the benefit that GitHub.com will automatically render it for you. Specs for your proposal are below. Here’s what I’m looking for from your proposal:\n\nExpected Sections\nYou should include sections in your proposal that address the following topics. Feel free to include additional sections as needed. Remember that you can create Markdown sections using the # character.\n\nAbstract\nIn 3-4 sentences, concisely describe:\n\nWhat problem your project addresses.\nThe overall approach you will use to solve the problem.\nHow you propose to evaluate your success against your stated goals.\n\n\n\nMotivation and Question\nDescribe your motivation for your project idea. Some (shortened) examples of good types of motivations:\n\nWe have a scientific data set for which predictive or expoloratory models would help us generate hypotheses.\nWe have user information for which predictive models would help us give users better experiences.\nWe have performance data (e.g. from sports teams) for which predictive models could help us make better decisions.\nAlgorithmic bias is an increasingly urgent challenge as machine learning products proliferate, and we want to explore it more deeply.\n\nYou should be more specific than these: describe your specific data set (if applicable); your scientific questions; the type of decisions your model could inform; etc.\n\n\nPlanned Deliverables\nConcisely state what you are aiming to create and what capabilities it will have. For most projects, I would expect the deliverable to include:\n\nA Python package containing all code used for algorithms and analysis, including documentation.\nAt least one Jupyter notebook illustrating the use of the package to analyze data.\n\nHowever, your specific idea might imply different deliverables (e.g. an essay). Consult with me if you’re not sure.\nYou should describe what your deliverable will be able to do and how you will evaluate its effectiveness. Please consider two scenarios:\n\n“Full success.” What will your deliverable be if everything works out for you exactly as you plan?\n“Partial success.” What useful deliverable will you be able to offer even if things don’t 100% work out? For example, maybe you aren’t able to get that webapp together, but you can still create a code repository that showcases the machine learning pipeline needed to use to support the app. Have a contingency plan!\n\n\nWritten Deliverables\nYou’ll also write a blog post on your project; you don’t have to discuss this post in your proposal though.\n\n\n\nResources Required\nWhat resources do you need in order to complete your project? Data? Computing power? An account with a specific service?\nPlease pay special attention to the question of data. If your project idea involves data, include at least one link to a data set you can use. If you can’t find data for your original idea, that’s ok! Think of something related to your group’s interests for which you can find data.\nMost projects should involve data in some way, but certain projects may not require data. Ask me if you’re not sure.\n\n\nWhat You Will Learn\nEach group member should return to their stated goals from the reflective goal-setting assignment at the beginning of the course. Then, in this section, please state what each group member intends to learn through working on the project, relating your intentions to your stated goals. You might be thinking of certain algorithms, software packages, version control, project management, effective teamwork, etc.\n\n\nRisk Statement\nWhat are two things that could potentially stop you from achieving the full deliverable above? Maybe it turns out that the pattern you thought would be present in the data just doesn’t exist? Or maybe your idea requires more computational power than is available to you? What particular risks might be applicable for your project?\n\n\nEthics Statement\nAll projects we undertake involve decisions about whose interests matter; which problems are important; and which tradeoffs are considered acceptable. Take some time to reflect on the potential impacts of your project on its prospective users and the broader world. Address the following questions:\n\nWhat groups of people have the potential to benefit from our project?\nWhat groups of people have the potential to be excluded from benefit or even harmed from our project?\nWill the world become an overall better place because we made our project? Describe at least 2 assumptions behind your answer. For example, if your project aims to make it easier to predict crime, your assumptions might include:\n\nCriminal activity is predictable based on other features of a person or location.\nThe world is a better place when police are able to perform their roles more efficiently.\n\n\nIf your project involves making decisions or recommendations, then you will also need to consider possible forms of algorithmic bias in your work. Here are some relevant examples:\n\nA recipe recommendation app can privilege the cuisines of some locales over others. Will your user search recipes by ingredients? Peanut butter and tomato might seem an odd combination in the context of European cuisine, but is common in many traditional dishes of the African diaspora. A similar set of questions applies to recommendation systems related to style or beauty.\nA sentiment analyzer must be trained on specific languages. What languages will be included? Will diverse dialects be included, or only the “standard” version of the target language? Who would be excluded by such a choice, and how will you communicate about your limitations?\n\n\n\nTentative Timeline\nWe will have a checkpoint for the project in Week 9 or 10, and then final presentations in Week 12. With this in mind, please describe what you expect to achieve after three and six. Your goal by the three-week check-in should be to have “something that works.” For example, maybe in three weeks you’ll ready to demonstrate the data acquisition pipeline and show some exploratory analysis, and in the last couple weeks you’ll actually implement your machine learning models. The “something that works” idea is related to the common concept of “minimum viable products” in software development, and is visually illustrated here:"
  },
  {
    "objectID": "assignments/project/proposal.html#general-expectations",
    "href": "assignments/project/proposal.html#general-expectations",
    "title": "Project Proposal",
    "section": "3 General Expectations",
    "text": "3 General Expectations\nYour proposal is acceptably complete if if:\n\nThe proposal is hosted on GitHub as the top-level README.md file in a repository hosted by one of the group members.\nEach team member has made at least two commits to this file, which in total demonstrate substantial commitments to the writing of the proposal.\nThe proposal contains thoughtful discussion in each of the required sections, which addresses all of the relevant questions posed in each one.\nThe proposal is written in clear English prose. Within reason, grammatical mistakes are not a problem.\n\nYou have submitted a URL to your GitHub repository on Canvas.\n\n\nLength\nThere is no specifically required length for the proposal. Generally speaking, I would expect a thoughtful proposal to require around 600-900 words (roughly the length of 2-3 double-spaced pages). However, any length is acceptable provided that it provides thoughtful discussion of each of the required components."
  },
  {
    "objectID": "assignments/project/proposal.html#optional-practice-collaborative-workflows-in-git",
    "href": "assignments/project/proposal.html#optional-practice-collaborative-workflows-in-git",
    "title": "Project Proposal",
    "section": "4 Optional Practice: Collaborative Workflows in Git",
    "text": "4 Optional Practice: Collaborative Workflows in Git\nYour proposal should be written on GitHub and contain commits by multiple team members. This is the same workflow that you’ll use for collaborating on your project itself. If members of your team are not familiar with collaborative workflows in Git and GitHub, then you should complete this activity with your team in order to get up to speed.\nWorking through this mini activity with your group is optional but strongly recommended.\n\nMake a Grid\nIt’s possible that your repository already has a top-level file named README.md. If not, a new group member (not the one who created the repo) should create one. Then, this group member should create a code block in the file README.md containing a 3x3 grid of dots, like the below:\n. . . \n. . .\n. . .\nSave, commit, and push. All other group members should now pull, so that they have the grid of dots as well.\n\n\nPlay Tic-Tac-Toe\nIf you have more than two people, separate into two teams – it’s ok if they are not the same size. Play a few games of Tic-Tac-Toe by replacing the dots in the grid you made with the symbol X or O. Here’s how to make a move:\n\nA member of Team X deletes a dot and replaces it with an X.\nThis member commits and pushes their change.\nAll other team members pull, so that the move is reflected in their file.\nA member of Team O deletes a dot and replaces it with an O…\n\nBy playing some Tic-Tac-Toe, you practice the fundamental pull-commit-push workflow of collaboration. Make sure that every group member gets a chance to make a move, commit their move, and push at least twice.\n\n\nMerging\nCreate a new, blank Tic-Tac-Toe game. Imagine that Team X and Team O miscommunicated about who would go first, so they both make moves simultaneously. Test out the following scenarios.\nScenario 1:\nTeam X makes the following move:\n. . . \n. . .\n. X .\nTeam O makes the following move:\n. O . \n. . .\n. . .\nBoth teams should now attempt to commit and push. One team will be prompted to pull prior to pushing. This pull will prompt a merge, since two changes were made to the same file. Observe what happens, commit, and then push. Pull as needed so that both teams have both moves in their file.\nScenario 2:\nTeam X makes the following move:\n. . . \n. X .\n. . .\nTeam O makes the following move:\n. . . \n. O .\n. . .\nThe current representative of each team should commit these respective moves, and attempt to push.\nWhoops! One team will be prompted to pull, and after pulling will be informed that there is a merge conflict in the repository. Inspect the file. Notice that the relevant part of the file now looks very weird. If you look closely, you can find lines corresponding both to the X move and to the O move. Pick one (arbitrarily), and commit/push the result.\nThis is an example of the process used to handle merge conflicts, which occur when separate team members have modified the same file in conflicting ways.\nIt’s recommended, but not required, that your team members take some time on their own to do a little reading on how merge conflicts work. This page gives a good explanation, and also covers the (optional but highly useful) topic of branching."
  },
  {
    "objectID": "assignments/process/mid-course.html",
    "href": "assignments/process/mid-course.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice. I recommend JupyterLab for this one, but you can pick.\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nBriefly review the goals you set for yourself in our goal-setting activity at the beginning of the course. You can find your goals on Canvas.\nIn the The Data section, replace the blanks with brief responses.\nIn the What You Learned and Reflecting on Goals sections, write down your reflections on your learning, achievement, and presence in CSCI 0451 in the provided markdown cells.\nTake some time to reflect on your responses so far. When you’re ready, review the soundbytes describing letter grades.\nTake some time to reflect on your responses so far. When you’re ready, propose the letter grade that you feel best reflects your learning, participation, and achievement in CSCI 0451 so far.\nOptionally, respond to the last prompt with some thoughts on how the semester is going and what we might do to help you meet your goals for the course.\nSubmit the notebook as a PDF on Canvas.\n\nWe’ll discuss your reflection and your proposed letter grade during our end-of-semester conference.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf mid-course.ipynb\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/mid-course.html#the-data",
    "href": "assignments/process/mid-course.html#the-data",
    "title": "Mid-Course Reflection",
    "section": "The Data",
    "text": "The Data\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\nPresence in Class\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.) ____\nHow often have you taken notes on the core readings ahead of the class period? ____\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ____\nHow many times have you actually presented the daily warm-up to your team? ____\nHow many times have you asked your team for help while presenting the daily warm-up? ____\nHow often have you learned something new from a teammate’s presentation of the daily warm-up? ____\nHow often have you helped a teammate during the daily warm-up presentation? ____\n\n\n\nPresence Outside of Class\n\nHow often have you attended Student Hours or Peer Help? ____\nHow often have you asked for or received help from your fellow students? ____\nHave you been regularly participating in a study group outside class? ____\nHow often have you posted questions or answers in Slack? ____\n\n\n\nAssignments and Effort\n\nHow many blog posts have you submitted? ____\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: ____\nRevisions useful: ____\nRevisions encouraged: ____\nIncomplete: ____\n\nRoughly how many hours per week have you spent on this course outside of class? ____"
  },
  {
    "objectID": "assignments/process/mid-course.html#what-youve-learned",
    "href": "assignments/process/mid-course.html#what-youve-learned",
    "title": "Mid-Course Reflection",
    "section": "What You’ve Learned",
    "text": "What You’ve Learned\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\n[your response here]"
  },
  {
    "objectID": "assignments/process/mid-course.html#reflecting-on-goals",
    "href": "assignments/process/mid-course.html#reflecting-on-goals",
    "title": "Mid-Course Reflection",
    "section": "Reflecting on Goals",
    "text": "Reflecting on Goals\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\nBlog Posts\n[your response here]\n\n\nCourse Presence (Participation)\n[your response here]\n\n\nProject\n[your response here]\n\n\nOther\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\n[your response here]\n\n\nUpdating Your Goals\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\n[your response here]"
  },
  {
    "objectID": "assignments/process/mid-course.html#grade-and-goals",
    "href": "assignments/process/mid-course.html#grade-and-goals",
    "title": "Mid-Course Reflection",
    "section": "Grade and Goals",
    "text": "Grade and Goals\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far. Here are some soundbytes to help guide your thinking:\n\nWhat a Grade Sounds Like\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of ____"
  },
  {
    "objectID": "assignments/process/mid-course.html#optional-how-to-improve",
    "href": "assignments/process/mid-course.html#optional-how-to-improve",
    "title": "Mid-Course Reflection",
    "section": "(Optional:) How to Improve?",
    "text": "(Optional:) How to Improve?\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\n[your response here]"
  },
  {
    "objectID": "assignments/process/ideas.html",
    "href": "assignments/process/ideas.html",
    "title": "Ideas for Goal-Setting",
    "section": "",
    "text": "If you’re not sure what goals you might want to set for yourself when setting your goals for the course, here are a few ideas to help you get started. Don’t limit yourself to just these ideas! They are just here to show you some possibilities and get your creative juices flowing. Note that these are not requirements and you do not have to do all of these in order to demonstrate learning in the course.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/ideas.html#blog-posts",
    "href": "assignments/process/ideas.html#blog-posts",
    "title": "Ideas for Goal-Setting",
    "section": "Blog Posts",
    "text": "Blog Posts\n\nSubmit a blog post in most weeks during the semester.\nSubmit the first draft of no more than two blog posts after the “best-by” date.\nRevise at least five blog posts to the “No Revisions Suggested” level.\nGo above and beyond in at least two blog posts by implementing more complex algorithms, discussing more advanced theory, or performing experiments significantly beyond what is requested.\nPropose and complete a blog post on a topic not covered in lecture, especially one related to your areas of focused interest.\nPropose and complete an additional blog post on a topic related to algorithmic bias and social responsibility."
  },
  {
    "objectID": "assignments/process/ideas.html#course-presence",
    "href": "assignments/process/ideas.html#course-presence",
    "title": "Ideas for Goal-Setting",
    "section": "Course Presence",
    "text": "Course Presence\n\nComplete all core readings prior to each class periods.\nComplete the optional readings that correspond to my areas of specialization.\n“Pass” at most once when asked to lead the warmup activity for my group.\nAsk questions or make suggestions for the warmup presenter in most weeks.\nPropose questions ahead of time for our guest speaker.\nOrganize a study group outside of class time to work on blog posts or other course work.\nAttend a study group outside of class time.\nFrequently attend Peer Help or Student Hours (after preparing questions and working examples).\nRegularly post questions or answers on the course Slack workspace."
  },
  {
    "objectID": "assignments/process/ideas.html#project",
    "href": "assignments/process/ideas.html#project",
    "title": "Ideas for Goal-Setting",
    "section": "Project",
    "text": "Project\n\nSubmit all project milestones (proposal, progress report, etc) on time.\nSet regular time each week to work with project partners.\nCommunicate with my group in a clear and timely manner.\nImplement algorithms or write automated checks of algorithms written by teammates.\nDraft designated sections of the project report.\nRevise sections of the project report in response to feedback.\nLead creation of the final project presentation.\nTake the lead in delivering part of the final project presentation.\nTake the lead in checking project figures for accuracy and clear labeling."
  },
  {
    "objectID": "assignments/process/goal-setting.html",
    "href": "assignments/process/goal-setting.html",
    "title": "Reflective Goal-Setting",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice. I recommend JupyterLab for this one, but you can pick.\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nIn each of the spaces provided, write down some goals describing what you believe success will look like for you in CSCI 0451. I’ve offered some ideas to help you get started in case you’re not sure, but you shouldn’t feel constrained by these.\nYou may want to look at the end-of-course reflection activity in which you’ll look back on your goals and propose a letter grade that reflects your learning, participation, and achievement in the course. You might especially want to look at the data that I’ll ask you to log and what a grade sounds like.\nSubmit the notebook as a PDF on Canvas.\n\nI’ll respond to your submission with feedback on your goals. I may ask you to display more or less ambition in some of your goals; in that case, I’ll ask you to revise and resubmit.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf goal-setting.ipynb\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/goal-setting.html#what-youll-learn",
    "href": "assignments/process/goal-setting.html#what-youll-learn",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\n[your response here]"
  },
  {
    "objectID": "assignments/process/goal-setting.html#what-youll-achieve",
    "href": "assignments/process/goal-setting.html#what-youll-achieve",
    "title": "Reflective Goal-Setting",
    "section": "What You’ll Achieve",
    "text": "What You’ll Achieve\n\nBlog Posts\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\n[your response here]\n\n\nCourse Presence (Participation)\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions. We will also have a special opportunity this semester to engage with a renowned expert in machine learning, algorithmic bias, and the ethics of artificial intelligence.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\n[your response here]\n\n\nProject\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\n[your response here]"
  },
  {
    "objectID": "assignments/process/end-of-course.html",
    "href": "assignments/process/end-of-course.html",
    "title": "End-Of-Course Reflection",
    "section": "",
    "text": "Download this notebook\nOpen the notebook in an editor of your choice. I recommend JupyterLab for this one, but you can pick.\nDelete the first two cells of the notebook (i.e. this one and the raw cell above).\nBriefly review the goals you set for yourself in our goal-setting activity at the beginning of the course. You can find your goals on Canvas.\nIn the The Data section, replace the blanks with brief responses.\nIn the What you Learned and Reflecting on Goals sections, write down your reflections on your learning, achievement, and presence in CSCI 0451 in the provided markdown cells.\nTake some time to reflect on your responses so far. When you’re ready, move on to propose the letter grade that you feel best reflects your learning, participation, and achievement in CSCI 0451.\nSubmit the notebook as a PDF on Canvas.\n\nWe’ll discuss your reflection and your proposed letter grade during our end-of-semester conference.\nThere are lots of ways to render Jupyter notebooks as PDFs. The simplest way is to run this at the command line, after you’ve navigated to the location of the notebook:\njupyter nbconvert --to pdf end-of-course.ipynb\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "assignments/process/end-of-course.html#the-data",
    "href": "assignments/process/end-of-course.html#the-data",
    "title": "End-Of-Course Reflection",
    "section": "The Data",
    "text": "The Data\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often did you attend class,” good answers include “almost always,” “I missed three times,” “about 75% of the time,” “not as often as I wanted,” etc.\n\nPresence in Class\n\nHow often did you attend class? (e.g. “almost always,” “I missed three times,” etc.) ____\nHow often did you take notes on the core readings ahead of the class period? ____\nHow often were you prepared to present the daily warm-up exercise to your team, even if you weren’t actually called? ____\nHow many times did you actually present the daily warm-up to your team? ____\nHow many times did you ask your team for help while presenting the daily warm-up? ____\nHow often did you learn something new from a teammate’s presentation of the daily warm-up? ____\nHow often did you help a teammate during the daily warm-up presentation? ____\nDid you contribute a question for our guest speaker? ____\n\n\n\nPresence Outside of Class\n\nHow often did you attend Student Hours or Peer Help? ____\nHow often did you ask for or receive help from your fellow students? ____\nDid you regularly participate in a study group outside class? ____\nHow often did you post questions or answers in Slack? ____\n\n\n\nAssignments and Effort\n\nHow many blog posts did you submit? ____\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nNo revisions suggested: ____\nRevisions useful: ____\nRevisions encouraged: ____\nIncomplete: ____\n\nRoughly how many hours per week did you spend on this course outside of class? ____"
  },
  {
    "objectID": "assignments/process/end-of-course.html#what-you-learned",
    "href": "assignments/process/end-of-course.html#what-you-learned",
    "title": "End-Of-Course Reflection",
    "section": "What You Learned",
    "text": "What You Learned\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what did you do in order to pursue your interest?\n[your response here]"
  },
  {
    "objectID": "assignments/process/end-of-course.html#reflecting-on-goals",
    "href": "assignments/process/end-of-course.html#reflecting-on-goals",
    "title": "End-Of-Course Reflection",
    "section": "Reflecting on Goals",
    "text": "Reflecting on Goals\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways did you meet your goals from the beginning of the course? Be specific: explain what the goal was and what you did to meet it.\nIn what ways did you not meet your goals from the beginning of the course? Be specific: explain what the goal was and what the gap was between what you aspired to and what happened.\nIf there’s any context you want to share about how you fared relative to your goals, please do!\n\n\nBlog Posts\n[your response here]\n\n\nCourse Presence (Participation)\n[your response here]\n\n\nProject\n[your response here]\n\n\nOther\nIs there anything else that you want to share with me about what you learned, how you participated, or what you achieved in CSCI 0451?\n[your response here]"
  },
  {
    "objectID": "posts/2023-01-01-test.html",
    "href": "posts/2023-01-01-test.html",
    "title": "My First Blog Post",
    "section": "",
    "text": "2+2\n\n4\n\n\nThis is some code that I’m writing, but also some math that I’m doing.\n\\[x = - y\\]\n\nfrom matplotlib import pyplot as plt\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "sol/logistic/experiments.html",
    "href": "sol/logistic/experiments.html",
    "title": "",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\n\n\nfrom logistic import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfrom logistic import LogisticRegression, gradient, stochastic_gradient\n\n# for step in [gradient, stochastic_gradient]:\n\n# LR = LogisticRegression()\n# LR.fit_stochastic(X, y, k = 1, max_iter = 10000)\n# plt.plot(LR.history, label = \"stochastic gradient\")\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, m_epochs = 100, momentum = True, batch_size = 1, alpha = .05)\nplt.plot(LR.history, label = \"stochastic gradient (momentum)\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, m_epochs = 100, momentum = False, batch_size = 1, alpha = .05)\nplt.plot(LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05)\nplt.plot(LR.history, label = \"gradient\")\nplt.loglog()\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7acace32e0&gt;\n\n\n\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nf1 = np.linspace(-3, 3, 101)\nw = LR.w\np = plt.plot(f1, (w[2] - f1*w[0])/w[1], color = \"black\")\n\n\n\n\n\n\n\n[]\n\n\n\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "sol/perceptron/perceptron-experiment.html",
    "href": "sol/perceptron/perceptron-experiment.html",
    "title": "",
    "section": "",
    "text": "import perceptron\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom importlib import reload\n\n\nn = 100\nX = np.random.rand(n, 2) - .5\nw_true = np.array([.5, .5])\ny = 1*(X@w_true &gt; 0)\n\n\ndf = pd.DataFrame(np.append(X, y[..., None], 1), columns=[\"x1\", \"x2\", \"y\"])\nf = sns.relplot(data = df, x = \"x1\", y = \"x2\", hue = \"y\")\n\n\n\n\n\nreload(perceptron)\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 10000)\nplt.plot(p.history)\n\n\n\n\n\np.w\n\narray([2.11140182, 2.63862367, 0.39340574])\n\n\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "CSCI 0451, “Machine Learning,” is an introductory course in computer science at Middlebury College. It is one of several ways in which you might begin a computer science major at Midd. It’s also a great way for you to develop your computational skills to support other majors.\nYou can access this page using go/cs-451.\n\n\n\n  © Phil Chodrow, 2023"
  },
  {
    "objectID": "slides/welcome.html#section",
    "href": "slides/welcome.html#section",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is the theory and practice of algorithmically learning patterns in data."
  },
  {
    "objectID": "slides/welcome.html#section-1",
    "href": "slides/welcome.html#section-1",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…automated consumer recommendations for content and shopping."
  },
  {
    "objectID": "slides/welcome.html#section-2",
    "href": "slides/welcome.html#section-2",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…generating realistic synthetic text, images, and code.\n\n\n\n\n\nAsk chatGPT to condemn itself in the tone of Shakespeare and it looks hilarious. pic.twitter.com/T785FbGmUX\n\n— Deqing Fu (@DeqingFu) December 5, 2022"
  },
  {
    "objectID": "slides/welcome.html#section-3",
    "href": "slides/welcome.html#section-3",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…predictions and recommendations for life-changing decisions: housing, healthcare, criminal justice."
  },
  {
    "objectID": "slides/welcome.html#section-4",
    "href": "slides/welcome.html#section-4",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Machine learning is used for…\n…search engines, smart homes, computer vision, speech-to-text, scientific discovery, driver assistance systems…"
  },
  {
    "objectID": "slides/welcome.html#section-5",
    "href": "slides/welcome.html#section-5",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Can you list the times in which you interacted with a machine learning system yesterday?"
  },
  {
    "objectID": "slides/welcome.html#section-6",
    "href": "slides/welcome.html#section-6",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "How is this course going to go?"
  },
  {
    "objectID": "slides/welcome.html#section-7",
    "href": "slides/welcome.html#section-7",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "CSCI 0451 is….\nCoding\n\nNumerical array programming\nObject-oriented interfaces\nExperiments and visualization\n\nMath\n\nLinear algebra\nOptimization (\\(\\implies\\) calculus)\nA bit of probability\n\nReading, writing, discussion\n\nTechnical methods\nBias, fairness, and impact of ML"
  },
  {
    "objectID": "slides/welcome.html#section-8",
    "href": "slides/welcome.html#section-8",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "ML, Ethics, Bias, and Fairness\nAlgorithmic bias is the tendency of automated systems to reproduce structural privilege and oppression, especially in relation to race, gender, and sexuality.\nMost systems that impact people in any way have at least a risk of algorithmic bias. Active mitigation is usually needed.\n\n\n\n\n\nSave the Date\n\nMonday April 24th\nDr. Timnit Gebru will be virtually visiting our class for a Q&A session and giving a talk at 7pm.\nDr. Gebru is one of the world’s leading experts in intersectional bias in AI."
  },
  {
    "objectID": "slides/welcome.html#section-9",
    "href": "slides/welcome.html#section-9",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Rough, tentative plan for the semester\n\n\nFundamentals of prediction (~6 weeks)\n\nCore math concepts\nOptimization (“the algorithms”)\nHow to help models generalize\nFormal definitions of bias and fairness\n\nUnsupervised methods (~1 week)\n\nClustering\nDimensionality reduction\n\nDeep Learning (~4 weeks)\n\nImage classification\nText generation\nWord embedding\n\nProject Presentations (~1 week)"
  },
  {
    "objectID": "slides/welcome.html#section-10",
    "href": "slides/welcome.html#section-10",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Most Days\n\n\nWarmup Activity\n\nComplete ahead of time.\nReinforces content from readings and connects them to lecture.\nPresent in groups of 4-5.\nRandom presenter presents to the group.\n\nLecture\n\nMath\nLive-coding + experiments\nYour questions and ideas!\n\nClose-Out Activity\n\nIn same groups as warmup.\nPractices content from lecture, discussion"
  },
  {
    "objectID": "slides/welcome.html#section-11",
    "href": "slides/welcome.html#section-11",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Activities and assignments\n\n\nBlog Posts\n\nAim for ~1 per week.\nInvolves implementation, experiments, and discussion.\nPublished on your blog.\n\nDaily Warmup Activities\n\nRelatively quick when you’ve done the readings.\nOne (random) person each day will present to your team.\nConnects readings to lecture.\n\nProject\n\nIn groups of your choosing.\nWork on it throughout the semester, presentations in last week.\nWe’ll have activities etc. to help you pick a path."
  },
  {
    "objectID": "slides/welcome.html#section-12",
    "href": "slides/welcome.html#section-12",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Blog Posts\n\nImplement algorithms in source (.py) files.\nPerform experiments in Jupyter notebooks.\nCreate figures, add expository prose, etc.\nRender your notebooks into a blog using the Quarto publishing engine.\nHost source code and rendered blog on GitHub."
  },
  {
    "objectID": "slides/welcome.html#section-14",
    "href": "slides/welcome.html#section-14",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Feedback on blog posts via Hypothes.is"
  },
  {
    "objectID": "slides/welcome.html#section-15",
    "href": "slides/welcome.html#section-15",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Feedback on source code"
  },
  {
    "objectID": "slides/welcome.html#section-16",
    "href": "slides/welcome.html#section-16",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Readings and Warmups\nDo them! Some readings are optional.\nLet’s practice a warmup activity"
  },
  {
    "objectID": "slides/welcome.html#section-17",
    "href": "slides/welcome.html#section-17",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Your Affinity Vegetable\n \n1. Split into teams\n2. Go around and share your name and:\nIf you were a vegetable, which vegetable would you be and why?"
  },
  {
    "objectID": "slides/welcome.html#section-18",
    "href": "slides/welcome.html#section-18",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Your Affinity Vegetable\n \n3. Team leader: lead your team in finding a delicious dish that incorporates all of your vegetables.\nBe ready to share!"
  },
  {
    "objectID": "slides/welcome.html#section-19",
    "href": "slides/welcome.html#section-19",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading"
  },
  {
    "objectID": "slides/welcome.html#section-20",
    "href": "slides/welcome.html#section-20",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading\n\n\nInitialization:\n\n\nYou set goals for your learning and achievement (in week 2).\n\n\nMain Loop:\n\n\nYou attend class, participate in activities, and complete assignments.\n\nYou get feedback on your assignments from me and the TAs, and you revise.\nYou reflect on your learning and achievement at different points throughout the course.\n\n\nAt End Of Course:\n\n\nYou propose a letter grade that reflects your learning and achievement, and discuss it with me.\n\n\nIndividual assignments don’t get scores, points, or grades–just feedback."
  },
  {
    "objectID": "slides/welcome.html#section-21",
    "href": "slides/welcome.html#section-21",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Collaborative Grading\n \n\n\n\n\n\n\n\n\n\nOpportunity\nChallenge\n\n\n\n\nNo points, no averages\nYou can focus on feedback and set your own goals.\nYou need to motivate based on your interest in the class\n\n\nResubmit assignments\nOne of the best ways to learn\nNeed to read feedback and prioritize time for revisions\n\n\nCan skip assignments\nNo busy work – work on what’s valuable to you.\nStill need to work enough to learn and meet your goals\n\n\nNo hard due-dates\nDon’t ask for extensions, take the time you need\nNeed to keep yourself on pace to achieve your goals"
  },
  {
    "objectID": "slides/welcome.html#section-22",
    "href": "slides/welcome.html#section-22",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What a Grade Sounds Like…\nA: I am ready to take the theory, techniques, and ideas of this course into my endeavours outside this classroom: future classes, projects, hobbies, career.\nB: With help or review, I might be able to take some of what I learned outside this classroom.\nC: I showed up and did stuff, but I don’t really see any ways to take what I learned outside this classroom.\nD-F: I didn’t really show up or do much.\n\n\nWork Expected Work Expected\nI am very likely to accept your proposed grade in the course if you EITHER:\n\nComplete most assignments to a high standard (including revisions) OR\nWork for ~10 productive hours per week outside of class OR\nDo some of the assignments I give you and also some other things (that you propose) that are relevant to the course learning goals."
  },
  {
    "objectID": "slides/welcome.html#section-23",
    "href": "slides/welcome.html#section-23",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What is something that makes you feel excited or empowered about collaborative grading?\nWhat is something that makes you feel nervous or confused about collaborative grading?"
  },
  {
    "objectID": "slides/welcome.html#section-24",
    "href": "slides/welcome.html#section-24",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "What I expect from you now"
  },
  {
    "objectID": "slides/welcome.html#section-25",
    "href": "slides/welcome.html#section-25",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "CS Stuff\nYou can write moderately-complex, object-oriented software.\nYou are comfortable reading software documentation and researching how to perform a task that you haven’t seen before.\nYou know what a terminal is and how to perform simple operations at the command line.\nYou have experience debugging your code and you are ready to do it a lot more.\n\n\nMath Stuff\nYou remember most of MATH 0200 and CSCI 0200:\n\nMatrix multiplication and inner products\nEverything about \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\).\n\nVisualizing linear spaces.\nEigenvalues, eigenvectors, positive-definite matrices.\nDerivatives, critical points of functions.\nSample spaces, probability distribution functions.\nRandom variables, mean and variance.\nConditional probability and expectations.\n\nYou are ready to look up what you don’t remember."
  },
  {
    "objectID": "slides/welcome.html#section-26",
    "href": "slides/welcome.html#section-26",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "NYT, 1957\n \n\n\n\n\n\n\n\nWhat We Are Actually Talking About\n\n\n\n\n\n\\[\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\mathbb{1}(y_i \\langle \\mathbf{w}^{(t)}, \\mathbf{x}_i \\rangle &lt; 0)y_i \\mathbf{x}_i\\]"
  },
  {
    "objectID": "slides/welcome.html#section-27",
    "href": "slides/welcome.html#section-27",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "NYT, 2022\n \n\n\n\n\nWhat We Are Actually Talking About\n\n\nxkcd"
  },
  {
    "objectID": "slides/welcome.html#section-28",
    "href": "slides/welcome.html#section-28",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Most of all, I expect that you are ready to make thoughtful decisions to guide your own learning in this course."
  },
  {
    "objectID": "slides/welcome.html#section-29",
    "href": "slides/welcome.html#section-29",
    "title": "Welcome to CSCI 0451!",
    "section": "",
    "text": "Based on what you know about the course so far, what are some ways that success might look like for you?"
  },
  {
    "objectID": "collaboration.html",
    "href": "collaboration.html",
    "title": "Collaboration And Academic Honesty",
    "section": "",
    "text": "This is a page of general principles and guidelines that apply in courses I (Phil Chodrow) teach at Middlebury College. It is lightly adapted from the handout “Collaborating on Mathematics” by the Harvey Mudd Department of Mathematics, which I discovered in a Tweet by Francis Su.\nIn any case in which the guidelines and principles on this page conflict with the policies of a specific course, the policies of the specific course should be followed. For example, if the course syllabus says that collaboration is not permitted on homeworks, then collaboration is not permitted on homeworks, regardless of anything written here.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "collaboration.html#why-collaborate",
    "href": "collaboration.html#why-collaborate",
    "title": "Collaboration And Academic Honesty",
    "section": "Why Collaborate?",
    "text": "Why Collaborate?\nMost scientists and engineers don’t work on their own; they work with colleagues and students while doing and publishing research. Increasingly, open problems in science and engineering require multiple skill sets and areas of expertise. Because of this, the need to collaborate will only increase in the future. This is why several of CS@Midd’s learning goals are explicitly focused on communication and collaboration. We want our students to have strong professional and communication skills, to be able to function well as part of a team, and to be able to work and communicate with diverse groups of people."
  },
  {
    "objectID": "collaboration.html#collaborating-on-homework-and-other-individually-assessed-assignments",
    "href": "collaboration.html#collaborating-on-homework-and-other-individually-assessed-assignments",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaborating on Homework and Other Individually-Assessed Assignments",
    "text": "Collaborating on Homework and Other Individually-Assessed Assignments\n\n(COLLABORATION IS A LIFE SKILL) Understand that working with others and asking for assistance are not signs of weakness or deficiency; rather, they are essential life skills important for making progress in any discipline, including computer science. Our department wants you to develop these skills. If you’re too shy to come to my office hours or to join a group of people working on their homework, ask a friend to come with you.\n(COLLABORATIONS BENEFIT FROM DIVERSITY) Open yourself up to working with people whom you don’t know (yet). You might find someone you work really well with and who doesn’t think exactly like you do. A wide range of experiecnes and backgrounds is beneficial in problem solving, although it may be helpful to find folks who can work on assignments during the same time of day and at roughly the same pace. If you’re having trouble finding people to work with, I can help!\n(COLLABORATIONS ARE INCLUSIVE) Believe that everyone has something meaningful to contribute (you included), and that you have something to learn from each person. This can be a difficult state of mind to achieve, but critical for healthy, effective collaboration. Here are some practical consequences:\n\nIn any group setting, listen carefully for everyone’s contributions. Don’t dismiss or ignore what someone says, and don’t move on until you’ve considered it carefully. If what is said doesn’t make sense to you, that doesn’t necessarily mean it’s incorrect–the person might just have a way of approaching the problem that is different and not yet clear to you. Furthermore, even ideas that ultimately turn out to be incomplete or incorrect are often still useful building blocks towards a successful approach.\nFind ways to verbally validate the ideas of others. For example: “One really neat feature of Zenith’s approach to part (b) is that it also works with a small modification for part (c).”\nIf someone in the group hasn’t spoken for a while, ask for their ideas or opinions. Conversely, if you find yourself talking a lot, take a step back and allow someone else to contribute to the discussion.\n\n(COLLABORATIONS REQUIRE PREPARATION) Don’t seek help from others on a probem before you’ve had time to think about it yourself, try at least one approach, and formulate the obstacle as clearly as you can. But at the same time, if you find yourself frustrated with a problem and you’re not making progress, don’t wait too long before you look for help from your classmates, your tutors, or me.\n(COLLABORATIONS GENERATE DEEPER UNDERSTANDING) Don’t be satisfied with only producing the correct final result; use your collaboration to push each other to understand:\n\nWhy does this approach work?\nWhat alternative approaches would also have worked?\nWhat are some of the merits and drawbacks of these different approaches?\n\n(COLLABORATIONS ARE EMPOWERING) Good collaborations empower people towards further growth.\n\nWhen you’re working on a problem with others and you find a path before everyone else, avoid ruining the experience of discovery for others. Conversely, if you haven’t figured out something yet and want to enjoy the discovery for yourself, don’t let someone else ruin your joy.\nIf someone asks you for help, don’t just tell them the answer or start showing them a solution method. Listen carefully to their question. Ask for more information if they aren’t being specific enough. If they say “I don’t know where to start,” ask them to tell you about their understanding of what the question is asking and which parts of it seem most puzzling. Ask guiding questions to help them discover ideas for themselves. In these situations, you have the opportunity to learn how to help others learn – this is an invaluable life skill.\n\n(COLLABORATIONS ACKNOWLEDGE CONTRIBUTORS) Whenever you’ve received help on a homework assignment from a classmate, a friend, a tutor, or me, acknowledge the support and briefly describe how it helped you in your assignment.\n\nThe reason I ask you to acknowledge tutors and myself is actually different from the reason I ask you to acknowledge classmates and friends. For classmates and friends, it’s about cultivating transparency and integrity. The primary reason I want you to acknowledge tutors and myself is that the exercise of explicitly remembering and reflecting on your learning journey is part of metacognition, a valuable set of practices that will help you succeed in this class, in college, and in your long-term career."
  },
  {
    "objectID": "collaboration.html#collaborating-on-group-projects",
    "href": "collaboration.html#collaborating-on-group-projects",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaborating on Group Projects",
    "text": "Collaborating on Group Projects\n\n(COLLABORATIONS SET GOOD EXPECTATIONS) Establish clear expectations and ways of communicating with each other to avoid misunderstandings. When, where, and how often will you meet? How can you reach each other in case of an emergency?\n(COLLABORATION IS NOT DIVISION OF LABOR) Collaboration is not the same as splitting up a problem into pieces and then slapping the completed pieces together.\n\nIdentify the parts of the problem that need to be completed together and the parts that can be completed individually.\nWork toward a final product that everyone is happy with and that represents the contributions of everyone on the group.\nDon’t just divide up the work based on who might have the most experience or skill with each part of the problem. Let those who want to develop their skills also have a chance to work on pieces that are unfamiliar to them\n\n(COLLABORATIONS ARE EQUITABLE) Aim for each person to contribute a fair and equitable amount of effort and/or time to the group’s deliverables.\n(COLLABORATIONS RESOLVE CONFLICT QUICKLY) Resolve any misunderstands between the team members quickly. Don’t let those misunderstandings fester into distrust, resentment, or anger. Don’t be afraid to ask your professor for help in resolving interpersonal conflict in your team. While this can feel uncomfortable, often these kinds of situations are important opportunities for everyone to learn more about how to coexist as collaborative, whole humans."
  },
  {
    "objectID": "collaboration.html#collaboration-is-a-skill",
    "href": "collaboration.html#collaboration-is-a-skill",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaboration is a Skill",
    "text": "Collaboration is a Skill\nYou might imagine that you already know whether you need to collaborate and how to do it. And indeed, there’s a lot you know already! But collaboration is a skill, and like other skills it rewards practice and growth. Effective collaboration involves perspective-taking, empathy, respect, and clear communication. We hope that you will find that the benefits of collaboration far outweigh its challenges."
  },
  {
    "objectID": "collaboration.html#collaboration-and-the-middlebury-honor-code",
    "href": "collaboration.html#collaboration-and-the-middlebury-honor-code",
    "title": "Collaboration And Academic Honesty",
    "section": "Collaboration and the Middlebury Honor Code",
    "text": "Collaboration and the Middlebury Honor Code\nThe Middlebury Honor Code’s preamble states that:\n\nThe students of Middlebury College believe that individual undergraduates must assume responsibility for their own integrity on all assigned academic work…The Middlebury student body, then, declares its commitment to an honor system that fosters moral growth and to a code that will not tolerate academic dishonesty in the College community.\n\nIn any assignment in which you receive a grade individually (homeworks, exams), the purpose of the grade is to measure your learning and achievement. When you turn in such an assignment, you implicitly represent that work as work that you are able to complete yourself under the stated conditions (which may include getting help or working with others). If you cannot complete some work under the stated collaboration conditions, it is dishonest to turn in that work.\nWhen working individually, it is your responsibility to uphold the Code’s standards of integrity and academic honesty. When working in a group, it is additionally your responsibility to ensure that your group as a whole upholds these standards.\nIf you have a question about whether some form of collaboration is permitted, just ask!\n\nWhat Happens if I Observe an Honor Violation?\n\nWe all fail to uphold our highest moral aspirations at times. If you show lack of integrity or academic honesty, that doesn’t mean you’re a bad person. It means that you’re under pressure and chose the course of action that looked like the most workable one to you at the time.\nThat said, if you show lack of integrity or academic honesty, that’s an indicator that you have an opportunity for some very important growth.\nIt is part of my job to help you achieve that growth. I take this part of my job very seriously. In order to help you on your journey, I will connect both of us with the Middlebury Community Standards Office. Office leadership will help us all find a path that helps you grow toward integrity and honesty.\nThis is an awkward and uncomfortable process for everyone involved. You don’t want this."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software and Setup",
    "section": "",
    "text": "After following this set of instructions, you will be all ready to go for participation in CSCI 0451.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "software.html#install-anaconda",
    "href": "software.html#install-anaconda",
    "title": "Software and Setup",
    "section": "1. Install Anaconda",
    "text": "1. Install Anaconda\nInstall and configure Anaconda Python by following these instructions. Choose the installer appropriate for your operating system."
  },
  {
    "objectID": "software.html#create-the-ml-0451-environment",
    "href": "software.html#create-the-ml-0451-environment",
    "title": "Software and Setup",
    "section": "2. Create the ml-0451 Environment",
    "text": "2. Create the ml-0451 Environment\n\n\n\nThe Environments tab, with the Create button on the bottom.\nAn environment is a separate installation of Python that exists independently of any other versions of Python on your computer. Using environments allows us to have fine-grained control over which version of Python we use, which additional packages are installed, etc.\nTo create an environment in Anaconda, first open the Anaconda Navigator program. Then, navigate to the Environments tab. There, you’ll find the current existing environments, including the default base(root) environment. Click the Create button to create a new environment.\nIn the resulting dialog box:\n\nName your environmnent ml-0451.\nEnsure that the installed Python is some version of Python 3.9 (it’s ok if your version number differs in the last two digits from the one shown in the example).\n\n\n\n Configuring the ml-0451 environment."
  },
  {
    "objectID": "software.html#install-packages",
    "href": "software.html#install-packages",
    "title": "Software and Setup",
    "section": "Install Packages",
    "text": "Install Packages\nYou will need to install several packages to the ml-0451 environment. Note that you need to do this even if you previously installed these packages to another version of Python on your laptop.\nTo add packages to the environment, first ensure that the environment is selected (it will be highlighted in green). Then, on the righthand menu, search for the package you want to install. You may need to change the box on the top left from “Installed” to “Not Installed” in order to view packages that you have not installed yet.\n\nInstall the following packages:\n\nnb_conda\nnumpy\nmatplotlib\npandas\nscikit-learn\nseaborn\n\nI may ask you to install additional packages later on, or you may find it useful to install packages yourself in order to deal with problems or projects. You’ll follow this same process to install them to the ml-0451 environment."
  },
  {
    "objectID": "software.html#launch-jupyterlab",
    "href": "software.html#launch-jupyterlab",
    "title": "Software and Setup",
    "section": "Launch JupyterLab",
    "text": "Launch JupyterLab\nNow back on the Home tab, launch the JupyterLab app. You may need to install it first. Create a notebook using the ml-0451 environment as a kernel.\n\n\n Creating a notebook using the ml-0451 environment as the kernel.\nNext, type the following code into the grey code cell that appears in the notebook:\nimport sklearn as sk\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nprint(\"I did it!\")\nFinally, run the cell (cmd + enter on Mac or ctrl + enter on Windows). If you get no errors, only the output of the print statement, then you did it!\n\n\n If you see this then you did it!"
  },
  {
    "objectID": "software.html#alternative-vscode-and-others",
    "href": "software.html#alternative-vscode-and-others",
    "title": "Software and Setup",
    "section": "Alternative: VSCode and Others",
    "text": "Alternative: VSCode and Others\nJupyterLab is probably the easiest way for you to get up and coding. The reason is that it supports two ways of working with Python:\n\nNotebooks, which allow us to combine code, text, and outputs, including data visualizations.\nText files, like .py files, which are best for holding complex, reusable source code.\n\nThere are other editors that support these as well. My personal favorite is Visual Studio Code (often called VSCode), and you’re likely to see me using it in class. It’s fine for you to use VSCode or any other editor, but please note that I’ll only troubleshoot Anaconda + JupyterLab. That is, you can use VSCode, but you’ll be “on your own” in terms of getting up and running. That said, the documentation on working with notebooks in VSCode is pretty good."
  },
  {
    "objectID": "software.html#optional-github-desktop",
    "href": "software.html#optional-github-desktop",
    "title": "Software and Setup",
    "section": "Optional: GitHub Desktop",
    "text": "Optional: GitHub Desktop\nIf you are comfortable working with git from the command line, you can continue to do this! If you are unfamiliar with git, I recommend that you download and install the GitHub Desktop graphical client. You will need to connect it to your GitHub account."
  },
  {
    "objectID": "software.html#clone-your-blog",
    "href": "software.html#clone-your-blog",
    "title": "Software and Setup",
    "section": "Clone your blog",
    "text": "Clone your blog\nFinally, clone your blog to your local computer in a place where you’ll be able to find it in the future. You can do this using the big green “Clone” button on GitHub. You can clone either using GitHub Desktop or at the command line: both options are good!\n\nWhat if I already have a GitHub Pages site?\nGreat! You can publish your blog as a project website rather than as a user site. Find out more on the difference and how to publish a project website."
  },
  {
    "objectID": "software.html#test-drive-quarto",
    "href": "software.html#test-drive-quarto",
    "title": "Software and Setup",
    "section": "Test Drive Quarto",
    "text": "Test Drive Quarto\nChange modify the About page of your blog by modifying the file about.qmd. You can do things like change text or change the profile picture (it doesn’t have to be of yourself). Once you’ve made these changes, open a terminal in the location of your cloned blog and type the command\nquarto preview\nAfter a few moments, a web browser window should pop up with a preview of your blog. If you navigate over to the About tab, you should see your changes."
  },
  {
    "objectID": "software.html#finalize-and-publish",
    "href": "software.html#finalize-and-publish",
    "title": "Software and Setup",
    "section": "Finalize and Publish",
    "text": "Finalize and Publish\nIn the terminal, use ctrl + c to stop the preview process. Then type the command\nquarto render\nThis time you won’t see a preview, but that’s ok! Over in git or GitHub Desktop, check all the new and modified files that have been generated, add a short message, and commit them to the main branch. Then, push your commit. This sends your files back to GitHub.com, where it will be published. After a minute or two, navigate back over to the URL housing your website and check that your changes have been made."
  },
  {
    "objectID": "warmup-exercises.html",
    "href": "warmup-exercises.html",
    "title": "Warmup Exercises",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\vx}{\\mathbf{x}}\n\\newcommand{\\vw}{\\mathbf{w}}\n\\newcommand{\\vz}{\\mathbf{z}}\n\\newcommand{\\norm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\bracket}[1]{\\langle #1 \\rangle}\n\\newcommand{\\abs}[1]{\\lvert #1 \\rvert}\n\\newcommand{\\paren}[1]{\\left( #1 \\right)}\n\\]\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "warmup-exercises.html#sec-perceptron",
    "href": "warmup-exercises.html#sec-perceptron",
    "title": "Warmup Exercises",
    "section": "Perceptron",
    "text": "Perceptron\n\nPart 1\nSketch the line in \\(\\R^2\\) described by the equation \\[\n\\bracket{\\vw, \\vx}  =  b\\;,\n\\tag{1}\\]\nwhere \\(\\vw = \\paren{1, -\\frac{1}{2}}^T \\in \\R^2\\) and \\(b = \\frac{1}{2}\\). Here, \\(\\bracket{\\vw, \\vx} = \\sum_{i = 1}^n w_i x_i\\) is the inner product (or dot product) between the vectors \\(\\vw\\) and \\(\\vw\\).\n\n\nPart 2\nWrite a quick Python function called perceptron_classify(w, b, x). w and x should both be 1d numpy arrays of the same length, and b should be a scalar. Your function should return 0 if \\(\\bracket{\\vw, \\vx} &lt; b\\) and 1 if \\(\\bracket{\\vw, \\vx} \\geq b\\). An excellent solution will use neither a for-loop nor an if-statement.\nVerify that your function works on a few simple examples.\n\n\nPart 3\nConsider a line of the general form of Equation 1. Let’s allow \\(\\vx\\) and \\(\\vw\\) to both be \\(n\\)-dimensional, so that this equation defines a hyperplane in \\(\\R^n\\). Suppose that we wanted to represent the same hyperplane in \\(\\R^n\\) using an equation of the form\n\\[\n\\bracket{\\tilde{\\vw}, \\tilde{\\vx}} = 0\\;.\n\\tag{2}\\]\nfor some \\(\\tilde{\\vw} \\in \\R^{n+1}\\). Define \\(\\tilde{\\vx} = (\\vx, 1)\\). How could you define \\(\\tilde{\\vw}\\) to make Equation 2 equivalent to Equation 1?"
  },
  {
    "objectID": "warmup-exercises.html#sec-convexity",
    "href": "warmup-exercises.html#sec-convexity",
    "title": "Warmup Exercises",
    "section": "Convexity",
    "text": "Convexity\nAs you learned in Daumé, informally, a convex function is a function that is “bowl-shaped.” Hardt and Recht give the formal definition, which has the benefit of applying to functions of many variables.\n\nPart 1\nConsider the 0-1 step function that I’ve plotted below:\n\nfrom matplotlib import pyplot as plt \nimport numpy as np\n\nfig, ax = plt.subplots(1, 1) \ny_hat = np.linspace(-1, 1, 101)\n\nloss = lambda y_hat, y: 1 - 1*(y_hat*y &gt; 0)\n\nax.set(xlabel = r\"$\\hat{y}$\", \n       ylabel = r\"$\\ell(\\hat{y}, y)$\")\n\nax.plot(y_hat, loss(y_hat, 1))\n\n\n\n\nShow pictorially that this function is not convex. No proof needed – just the right drawing.\n\n\nPart 2: Second Derivative Test\nAnother way to tell whether a function is convex is to check its second derivative. If a function \\(f:S\\rightarrow \\R\\) has a convex domain \\(S\\subseteq \\R\\), if \\(f\\) is everywhere twice-differentiable, and if \\(\\frac{d^2f(z_0)}{dz^2} &gt; 0\\) for all \\(z_0 \\in S\\), then \\(f\\) is convex.\nUse the second derivative test to check that the following two functions are convex: The base of the logarithm doesn’t really matter, but for this course it is always most convenient to assume logs base \\(e\\), which you might also have seen written \\(\\ln\\).\n\\[\n\\begin{aligned}\nf(z) &= - \\log z \\\\\ng(z) &= - \\log(1-z)\\;.\n\\end{aligned}\n\\]\n\n\nPart 3: Plotting Practice\nIn a Jupyter notebook, write a simple program to plot each of the functions \\(f\\) and \\(g\\) from Part 2. Some of the Part 1 code is likely to help you.\n\n\nPart 4: Convexity in Many Variables\nRecall the Hardt and Recht definition of convexity: a function \\(f:\\R^p \\rightarrow \\R\\) is convex if, for any \\(\\lambda \\in [0,1]\\) and any points \\(\\vz_1, \\vz_2 \\in \\R^p\\),\n\\[\nf(\\lambda \\vz_1 + (1-\\lambda)\\vz_2) \\leq \\lambda f(\\vz_1) + (1-\\lambda)f(\\vz_2)\\;.\n\\]\nUsing this definition, write a short mathematical proof that the function \\(f(\\vz) = \\norm{\\vz} = \\sqrt{\\bracket{\\vz, \\vz}}\\) is convex. You will want to use the triangle inequality, which says that \\(\\norm{\\vz_1 + \\vz_2} \\leq \\norm{\\vz_1} + \\norm{\\vz_2}\\). This proof requires just a few lines if you carefully use your definitions!"
  },
  {
    "objectID": "warmup-exercises.html#sec-gradient-descent",
    "href": "warmup-exercises.html#sec-gradient-descent",
    "title": "Warmup Exercises",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nConsider the quadratic function \\(g(z) = \\frac{1}{2}az^2 + bz + c\\).\n\nProve that \\(g\\) has a critical point at the point \\(z^* = -\\frac{b}{a}\\) (hint: solve \\(g'(z^*) = 0\\)).\nWhat must be true about the constants \\(a\\), \\(b\\), and \\(c\\) to ensure that this point is a local minimum of \\(g\\)? (Hint: second derivative test).\nSuppose now that we are able to evaluate the function \\(g\\), as well as its derivative \\(g'\\), but not able to use algebra to find \\(z^*\\) (this mirrors our situation in most practical problems). Instead, we are going to use the following algorithm to attempt to approximate \\(z^*\\):\n\nBegin with some initial guess \\(z^{(0)}\\).\nIn each time-step \\(t\\), compute \\(z^{(t+1)} \\gets z^{(t)} - \\alpha g'(z^{(t)})\\), where \\(\\alpha &gt; 0\\) is the learning rate.\nIn practice we would need to specify a stopping criterion, but for this theoretical problem we don’t need to worry about it.\n\nUsing algebra, prove that for any timestep \\(t\\), \\[\n(z^* - z^{(t+1)})^2 = (a\\alpha - 1)^2(z^* - z^{(t)})^2\\;.\n\\]\nLet’s think of \\(\\abs{z^* - z^{(t)}}\\) as the error in our current estimate \\(z^{(t)}\\). Using the recurrence above, conclude that, for any \\(t\\), the error \\(\\abs{z^* - z^{(t)}}\\) satisfies \\[\n\\abs{z^* - z^{(t)}} = \\abs{a\\alpha - 1}^{t}\\abs{z^* - z^{(0)}}\\;.\n\\]\nFor \\(\\alpha \\in (\\alpha_*, \\alpha^*)\\), we are guaranteed that the error \\(\\abs{z^* - z^{(t)}}\\rightarrow 0\\) as \\(t\\rightarrow \\infty\\). What are \\(\\alpha_*\\) and \\(\\alpha^*\\)?\n\nSuppose that \\(\\alpha\\) is within the necessary range. I want to guarantee that \\(\\abs{z^* - z^{(t)}} &lt; \\epsilon\\) for some small \\(\\epsilon &gt; 0\\) (in practice we often call this the tolerance). Conclude that the number of steps necessary to reach this tolerance is no greater than \\[\n\\bar{t} = \\frac{ \\log \\epsilon - \\log \\abs{z^* - z^{(0)}}}{\\log \\abs{a\\alpha - 1}}\\;.\n\\]\n\nIgnoring constants with respect to \\(\\epsilon\\), we say that this algorithm for finding the minimum of \\(g\\) with tolerance \\(\\epsilon\\) has a \\(\\log \\epsilon\\) a convergence rate."
  },
  {
    "objectID": "warmup-exercises.html#sec-gradient-descent-2",
    "href": "warmup-exercises.html#sec-gradient-descent-2",
    "title": "Warmup Exercises",
    "section": "Gradient Descent (Again)",
    "text": "Gradient Descent (Again)\nConsider the function \\(f(w_0, w_1) = \\sin(w_0w_1)\\). You can define this function like this:\n\nimport numpy as np\ndef f(w):\n    return np.sin(w[0]*w[1])\n\nMathematically, the gradient of this function is\n\\[\\nabla f(w_0, w_1) = (w_1\\cos w_0w_1, w_0 \\cos w_0w_1)^T.\\]\n\nImplement a simple loop that uses gradient descent to find a minimum of this function.\n\nYou’ll have to choose the learning rate \\(\\alpha\\).\nThe np.cos() function will be useful for programming the gradient.\nIt’s not the fastest approach, but if you’re not show how to program the gradient you can always first implement it as a list of two floats, and then use np.array(my_list) to convert it into a numpy array.\nYou’ll also need to pick a random starting guess.\n\nFind two initial guesses for the parameter vector \\(\\vw\\) such that you get two different final minimizers (this is possible because \\(f\\) is not convex)."
  },
  {
    "objectID": "warmup-exercises.html#sec-overfitting",
    "href": "warmup-exercises.html#sec-overfitting",
    "title": "Warmup Exercises",
    "section": "Overfitting and the Scientific Method",
    "text": "Overfitting and the Scientific Method\n Image from Wikipedia.\nIn the scientific method, it is often emphasized that we need to formulate a hypothesis before performing an experiment. It’s fine for the hypothesis to be based on previous experiments. However, the scientific method never allows us to perform an experiment, formulate a hypothesis, and then say that the experiment supported the (new) hypothesis.\nWe can think of scientific theories as systems of thought that help us make predictions about new phenomena. With this in mind, please write a short paragraph explaining the importance of hypothesis-first science using the language of machine learning. In your explanation, please use the following vocabulary:\n\nTraining data.\nTraining accuracy.\nValidation/testing data.\nValidation/testing accuracy.\nOverfitting."
  },
  {
    "objectID": "warmup-exercises.html#sec-erm",
    "href": "warmup-exercises.html#sec-erm",
    "title": "Warmup Exercises",
    "section": "The Coin-Flipping Game",
    "text": "The Coin-Flipping Game\nLet’s play a game! Here is the setup:\nI have a coin with probability of heads equal to \\(p \\in [0,1]\\). I am going to ask you to pick a number \\(\\hat{p} \\in [0,1]\\). Then, I flip my coin.\nThis game is more fun for me than it is for you.\n\nIf my coin comes up heads, you give me \\(-\\log \\hat{p}\\) dollars.\nIf my coin comes up tails, you give me \\(-\\log (1-\\hat{p})\\) dollars.\n\n\nPart 1\nCompute the expected amount of money you will give me when we play this game in terms of \\(p\\) and \\(\\hat{p}\\). Call this quantity \\(R(\\hat{p}, p)\\). This is the risk of the guess \\(\\hat{p}\\).\n\n\nPart 2\nTake the derivative and set it equal to 0! Don’t forget to check that you’ve found a minimum of \\(R(\\hat{p}, p)\\) rather than a maximum or an inflection point.\nSuppose I tell you the value of \\(p\\). Write a mathematical proof to show that your best choice of \\(\\hat{p}\\) (the one that loses you the least money) is \\(\\hat{p} = p\\).\n\n\nPart 3\nNow suppose that I don’t tell you the true value of \\(p\\). Instead, I let you observe \\(n\\) coin flips before asking you to make your guess. Describe:\n\nA suggestion for choosing \\(\\hat{p}\\) based only on the results of the previous flips.\nA way to estimate the risk (expected amount of money lost) based only on the results of the previous flips.\n\nYour answer should depend on \\(\\hat{p}\\) but not on \\(p\\)!"
  },
  {
    "objectID": "warmup-exercises.html#sec-classification-rates-2",
    "href": "warmup-exercises.html#sec-classification-rates-2",
    "title": "Warmup Exercises",
    "section": "Balancing Classification Rates",
    "text": "Balancing Classification Rates\nYou can do this first part just by copying and pasting lecture code. It doesn’t matter much how good your model is – just make sure you’re able to get predictions.\nUse the code from our recent lecture to download the Titanic data set as a Pandas data frame and train a model on the training data. Then download the test data. Compute y_pred, the vector of predictions of your model on the test data.\nThen, write a function that verifies eq. (2.6) in Alexandra Chouldechova’s paper “Fair Prediction with disparate impact.” Here’s what your function should do:\nThe positive predictive value is \\(\\mathrm{PPV} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}\\).\n\nGiven vectors y_pred of predictions and y_test of actual labels, compute the False Negative Rate (FNR), False Positive Rate (FPR), prevalence \\(p\\), and positive predictive value (PPV).\nReturn as a tuple the lefthand side and righthand side of eq. (2.6) in Chouldechova.\nVerify that the two numbers are equal!"
  },
  {
    "objectID": "warmup-exercises.html#sec-limits-quantitative",
    "href": "warmup-exercises.html#sec-limits-quantitative",
    "title": "Warmup Exercises",
    "section": "Limits of The Quantitative Approach to Discrimination",
    "text": "Limits of The Quantitative Approach to Discrimination\nI’ll give you each a number in Slack. The numbers correspond to the following sections of Narayanan (2022). These are:\n\nThe null hypothesis allocates the burden of proof (p. 7-8)\nCompounding inequality is far below the radar of quantitative methods (p. 9-10)\nSnapshot datasets hide discrimination (p. 10-11)\nExplaining away discrimination (p. 12-13)\nWhat counts as evidence is a subjective choice (p. 5-7)\n\nFor your assigned section, please write a short paragraph (4-5 simple sentences is fine). You should:\n\nSummarize Narayanan’s key points in that section.\nIn one of the sentences, describe which aspects of the Uber case study (p. 13-16) reflect the ideas of the section you described.\n\nBring your paragraph in class and be ready to read it to your group."
  },
  {
    "objectID": "warmup-exercises.html#sec-vectorization",
    "href": "warmup-exercises.html#sec-vectorization",
    "title": "Warmup Exercises",
    "section": "Vectorization Brainstorm",
    "text": "Vectorization Brainstorm\nIn a recent lecture, we discussed methods of vectorizing text like the document-term matrix that use the bag of words assumption: the order of the words doesn’t matter!\nTake some time and propose an alternative approach to word-based text vectorization. Can you find a scheme that would give different vector representations to the following two sentences?\n\n“I love rabbits, not cats.” “I love cats, not rabbits.”\n\nYou don’t have to implement your vectorization, but please be prepared to write pseudocode for your group to show in detail how you would perform the vectorization."
  },
  {
    "objectID": "warmup-exercises.html#sec-compression",
    "href": "warmup-exercises.html#sec-compression",
    "title": "Warmup Exercises",
    "section": "Image Compression Factor of K-Means",
    "text": "Image Compression Factor of K-Means\nIn today’s reading on K-means clustering from the Python Data Science Handbook, Jake VanderPlas considers the use of K-means to reduce the number of distinct colors in an image (Example 2). I encourage you to run the code for this example while thinking about this warmup!\nGive an estimate of the compression factor: the reduction of information achieved when compressing an image using k-means clustering into \\(k\\) color clusters. The compression factor is the number of bits required to store the compressed image, divided by the number of bits required to store the original image. Both of these numbers can be computed asymptotically (i.e. with big-oh reasoning) in order to simplify the analysis.\nThere are multiple good ways to think about this question, and you’re welcome to choose one that makes sense to you as long as you carefully state your steps and assumptions. Here are a few points that I find helpful:\n\nBits in Original Image\n\nAn image with \\(n\\) rows and \\(m\\) columns has \\(nm\\) pixels.\nEach pixel has one of three RGB color channels (Red, Green, and Blue).\nEach color channel can be represented with 8 bits (which encode an integer between 0 and 255, denoting the color intensity in that channel).\n\n\n\nBits in Compressed Image\n\nIf I compress an image into just \\(k\\) distinct colors, then instead of storing the full RGB value for each pixel, I can just store enough bits to uniquely identify the cluster containing each pixel. How many bits do I need for this?\nI also need to store a dictionary (hash map) that associates color \\(j\\) (i.e. the centroid of the \\(j\\)th cluster of colors) to its RGB value.\n\n\n\nOptional Extra\nTry running the code above while varying the number of clusters. Do you think that a 16-color compression looks much better than an 8-color compression. Do you think the difference is good enough to justify approximately twice the storage? What about 32 colors vs. 16?"
  },
  {
    "objectID": "warmup-exercises.html#sec-intro-tensors",
    "href": "warmup-exercises.html#sec-intro-tensors",
    "title": "Warmup Exercises",
    "section": "Introducing Tensors",
    "text": "Introducing Tensors\nFirst, install PyTorch 2.0 into your ml-0451 Anaconda environment.\n\n\nThe best way to install PyTorch is is probably to run the following at the command line:\nconda activate ml-0451\npip3 install torch torchvision torchaudio\nThen, in a blank Jupyter notebook, copy, paste, and run each of the code blocks in the first section of the PyTorch tutorial.\nYou may need to do a little bit of exploring around the tutorials in order to come up with answers to these questions.\nFinally, write down a single-sentence answer to each of the following questions:\n\nIn what ways is a PyTorch tensor similar to a Numpy array?\nIn what ways is a PyTorch tensor different from a Numpy array?\nWhat is the primary motivation for the use of a specialized tensor data type, rather than an array, for deep learning?"
  },
  {
    "objectID": "warmup-exercises.html#sec-backprop",
    "href": "warmup-exercises.html#sec-backprop",
    "title": "Warmup Exercises",
    "section": "Efficient Differentiation",
    "text": "Efficient Differentiation\nThis exercise is based on a section of Chinmay Hegde’s notes on stochastic gradient descent and neural networks:\nConsider the following function: \\[\nL(w, b) = \\frac{1}{2} \\left(y - \\sigma(wx + b)\\right)^2 + \\frac{1}{2}\\lambda w^2\\;,\n\\]\nwhere \\(\\sigma(a) = \\frac{1}{1 + e^{-a}}\\).\nThis is the loss function that would be obtained when using a single feature \\(x\\) to predict \\(y\\), using the function \\(\\sigma(wx + b)\\) the predictor and measuring the quality of this predictor using the square-error loss function. Our aim is to compute the gradient of \\(L\\) with respect to \\(w\\) and \\(b\\), with a “ridge” regularization term \\(\\frac{1}{2}\\lambda w^2\\) that encourages the weight \\(w\\) to be small.\nI’ve used the property of the sigmoid that \\(\\sigma'(a) = \\sigma(a)(1-\\sigma(a))\\).\nThe gradient of \\(L\\) is \\(\\nabla L (w, b) = \\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial b}\\right)\\), where\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial w} &= (\\sigma(wx + b) - y)\\sigma(wx+b)(1 - \\sigma(wx+b))x + \\lambda w \\\\\n\\frac{\\partial L}{\\partial b} &= (\\sigma(wx + b) - y)\\sigma(wx+b)(1 - \\sigma(wx+b))\\;.\n\\end{aligned}\n\\tag{3}\\]\n\nWhat You Should Do\nAssume that each of the following operations cost one computational unit:\n\nMultiplying or dividing two scalar numbers.\nAdding or subtracting two scalar numbers.\nComputing an exponential like \\(e^a\\).\n\nUsing this assumption:\n\nDetermine the number of computational units (i.e. computational cost) of computing the gradient of \\(L\\) exactly as written in Equation 3, under the assumption that you are not allowed to store the values of any intermediate computations.\nNow determine the computational cost of computing the gradient of \\(L\\) under the assumption that you are allowed to store intermediate computations. Please describe both the number of computations and the number of floating point numbers that must be stored.\nFinally, determine the computational cost in terms of both steps and storage to compute \\(L\\) using the backpropagation algorithm (described for a very similar function in Hegde’s notes).\n\nCompare your results from each method."
  },
  {
    "objectID": "warmup-exercises.html#sec-classification-rates",
    "href": "warmup-exercises.html#sec-classification-rates",
    "title": "Warmup Exercises",
    "section": "Classification Rates",
    "text": "Classification Rates\n\nPart 1\nCOVID-19 rapid tests have approximately an 80% sensitivity rate, which means that, in an individual who truly has COVID-19, the probability of a rapid test giving a positive result is roughly 80%.  On the other hand, the probability of a rapid test giving a positive result for an individual who truly does not have COVID-19 is 5%. Suppose that approximately 4% of the population are currently infected with COVID-19. These numbers are mostly made-up.Example 2.3.1 of Murphy, page 46, has a good review of the relevant probability and the definition of each of the rates below.\nWrite a Python function called rate_summary that prints the following output, filling in the correct values for each of the specified rates:\ns = 0.8           # test sensitivity\nf = 0.02          # probability of positive test if no COVID\nprevalence = 0.05 # fraction of population infected\n\nrate_summary(s, f, current_infection)\nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\n\nPart 2\n\nSuppose that scientists found an alternative rapid test which had a 75% sensitivity rate with a 0% chance of a positive test on someone who is truly not infected. Would you suggest replacing the old rapid tests with these alternative tests? Why? \nWhat if the alternative test had an 85% sensitivity rate and a 10% chance of a positive test on someone who is truly not infected?\n\nYou don’t necessarily need to use your function from the previous part in this part.\n\nPart 3\nIt’s all well and good to do the math, but what about when we actually have data? Write a function called rate_summary_2 that accepts two columns of a pandas.DataFrame (or equivalently two one-dimensional numpy.arrays of equal length). Call these y and y_pred. Assume that both y and y_pred are binary arrays (i.e. arrays of 0s and 1s). y represents the true outcome, whereas y_pred represents the prediction from an algorithm or test. Here’s an example of the kind of data we are thinking about:\n\nimport pandas as pd\n\nurl = \"https://github.com/middlebury-csci-0451/CSCI-0451/raw/main/data/toy-classification-data.csv\"\ndf = pd.read_csv(url)\n\ndf.head() # just for visualizing the first few rows\n\n\n\n\n\n\n\n\ny\ny_pred\n\n\n\n\n0\n0\n0\n\n\n1\n1\n0\n\n\n2\n1\n0\n\n\n3\n0\n1\n\n\n4\n0\n0\n\n\n\n\n\n\n\nYou should be able to use your function like this:\n# y is the true label, y_pred is the prediction\nrate_summary_2(df[\"y\"], df[\"y_pred\"]) \nThe true positive rate is ___.\nThe false positive rate is ___.\nThe true negative rate is ___. \nThe false positive rate is ___. \n\nHints\nAn excellent solution for this part will not use any for-loops. Computing each of the four rates can be performed in a single compact line of code. To begin thinking of how you might do this, you may want to experiment with code like the following:\ndf[[\"y\"]] == df[[\"y_pred\"]]\ndf[[\"y\"]].sum(), df[[\"y\"]].sum()"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is an advanced elective on the topic of algorithms that learn patterns from data. Artificial intelligence, predictive analytics, computational science, pattern recognition, signal processing, and data science are all disciplines that draw heavily on techniques from machine learning.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "syllabus.html#social-annotation",
    "href": "syllabus.html#social-annotation",
    "title": "Syllabus",
    "section": "Social Annotation",
    "text": "Social Annotation\nA an extremely useful way for you to engage with the readings is to make comments, ask questions, and answer questions about them as you are reading. For this reason, I’ll be providing most links to readings through Hypothes.is. You’ll be able to make marginal comments and view the marginal comments of others. I’ll also regularly be checking on your annotations to see what questions might have come up with the readings."
  },
  {
    "objectID": "syllabus.html#what-will-class-time-look-like",
    "href": "syllabus.html#what-will-class-time-look-like",
    "title": "Syllabus",
    "section": "What Will Class Time Look Like?",
    "text": "What Will Class Time Look Like?\nMy plan is for most class periods to look like a “lecture sandwich:”\n\n10 to 15 minutes of a warmup activity that addresses the recent lectures and readings.\n40-50 minutes of lecture, punctuated by short activities and breaks.\n10-15 minutes of a closing activity that helps us get solid with the day’s content.\n\n\nThe Warmup Activity\nOn most days, we’ll have a warmup activity. The warmup activity will usually ask you to engage with the readings and complete a small amount of work ahead of class time. This could be a short piece of writing, a math problem, or an implementation of a Python function.\nEach day, a few students will be randomly selected to present their work to a small group of peers. It’s ok to ask for help or even pass if you’re not feeling confident in your solution, but you should plan to at least make a good attempt at the warmup before every class period. Your participation on the warmup activity is an important aspect of presence in the course, and I’ll ask you to reflect on it when proposing your course grade."
  },
  {
    "objectID": "syllabus.html#collaborative-grading",
    "href": "syllabus.html#collaborative-grading",
    "title": "Syllabus",
    "section": "Collaborative Grading",
    "text": "Collaborative Grading\nThis course is collaboratively graded.  In a nutshell, this means:You may have also heard the term ungrading to refer to a similar approach.\n\nThere are no points or scores attached to any assignment. When you turn in assignments, you’ll get feedback on how to revise/resubmit, improve or otherwise proceed in the course, but you won’t get “graded.”\nThere also aren’t any firm due dates, although I will give you suggestions on how to maintain a good pace. \nPeriodically throughout the semester, you will complete reflection activities to help you take stock of your learning and achievement in the course. In your final activity at the end of the semester, you’ll make a proposal for your letter grade in the course, and support it with evidence of your learning. You and I will then meet to discuss how the course went for you, using your reflection activity and proposal as a starting point. In this conversation, you and I will agree on your final letter grade for the course, which I will then submit to the registrar.\n\nAll work you wish to be considered toward your achievement in the course needs to be submitted by the end of Finals Week.Reflection activities:\n\nReflective goal-setting.\nMid-course reflection.\nEnd-of-course reflection and grade proposal."
  },
  {
    "objectID": "syllabus.html#why-collaborative-grading",
    "href": "syllabus.html#why-collaborative-grading",
    "title": "Syllabus",
    "section": "Why Collaborative Grading?",
    "text": "Why Collaborative Grading?\nBecause grading is broken! Traditional points-based grading is ineffective at both (a) accurately measuring student learning and (b) motivating students to learn. I broadly agree with Jesse Stommel when he writes:\n\nAgency, dialogue, self-actualization, and social justice are not possible in a hierarchical system that pits teachers against students and encourages competition by ranking students against one another. Grades (and institutional rankings) are currency for a capitalist system that reduces teaching and learning to a mere transaction. Grading is a massive co-ordinated effort to take humans out of the educational process.\n\nI’d prefer to just not give you grades at all. But, Middlebury says I have to, and so my aim is to instead put the process of grading under your control to the greatest extent that I reasonably can."
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nThere are three kinds of assessed assignments in this course, plus a mysterious “Other” category.\n\n\n\n\n\nBlog Posts\n\n\n\nBlog posts are the primary way in which you will demonstrate your understanding of course content. Blog posts usually involve: written explanation of some relevant theory; implementation one or more algorithms according to written specifications; performing experiments to test the performance of the implementations; and communicating findings in a professional way. Some blog posts will be more like short essays than problem sets or programming assignments. Your blog posts will be hosted on your own public website (which you will create). This website will serve as your portfolio for the course.\n\n\n\n\n\n\nProject\n\n\n\nYour project is a large-scale undertaking that you will design and complete, usually in a group of 2 or 3, over the course of the semester. Your project should usually involve some combination of data collection, implementation, research of related work, experimentation, deployment, or theory work (but not necessarily all components). Projects are expected to demonstrate deep engagement with both the course content and the problem selected.\n\n\n\n\n\nProcess Reflections\n\n\n\nAt the beginning of the course, you’ll write a process reflection describing your aspirations for the course—what you want to learn and achieve, and how you’d like to be assessed against your goals. We’ll have a second process reflection mid-way through the course that will allow you to reflect on your progress toward your objectives and consider changing direction if needed. At the end of the course, you’ll write a summary reflection on your learning, accomplishment, and engagement with the class. This is also the place where you’ll propose your final letter grade.   I’ll usually give you written feedback on your process reflections. We’ll also meet at the end of the course to discuss your final reflection and agree on your letter grade for the course.\n\n\n\n\n\nOther…?\n\n\n\nYou may have some topic or idea that especially interests you and which you want to explore. If you’d like to work on this topic and use it to demonstrate your learning in the course, you can propose it to me. I may have suggestions or requested modifications before I agree to count the work in your course portfolio."
  },
  {
    "objectID": "syllabus.html#best-by-dates",
    "href": "syllabus.html#best-by-dates",
    "title": "Syllabus",
    "section": "Best-By Dates",
    "text": "Best-By Dates\nWhile we don’t have formal due dates, there is a benefit to keeping yourself on a schedule. It’s best to complete assignments close to the time when we covered the corresponding content in class, and it’s important for your wellbeing not to let work pile up. I’ll provide “best-by” dates for all assignments. These are my recommendations for when you should submit the first versions of these assignments to me for feedback.\n\n\n Image credit: Dr. Spencer Bagley"
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Syllabus",
    "section": "Feedback",
    "text": "Feedback\nI won’t “grade” your individual assignments, but our course team and I will offer you feedback about what I thought was successful and where you can improve. My general expectation is that you will often (though not always) revise your work in response to feedback and resubmit it. Revising in response to feedback is one of the single most effective ways for you to deepen your learning.\nI’ll usually describe the importance of revisions on your assignment using one of the following categories:\n\nNo revisions suggested: you’ve done great work and should focus on the next thing.\nRevisions useful: you have opportunities for improvement on this assignment, but focusing on the next topic or assignment may be a better use of your time—use your judgment.\n\nRevisions encouraged: the best use of your time is to respond to feedback and resubmit, rather than moving on to the next assignment.\nIncomplete: the assignment isn’t sufficiently complete for it to be used as evidence of your learning."
  },
  {
    "objectID": "syllabus.html#what-work-do-you-need-to-do",
    "href": "syllabus.html#what-work-do-you-need-to-do",
    "title": "Syllabus",
    "section": "What Work Do You Need To Do?",
    "text": "What Work Do You Need To Do?\nAt the beginning of the semester, you’ll write a process letter that will outline what you’d like to learn and achieve in the course. It’s ok if you don’t meet all your aspirations by the end of the course. To help guide you in your goal-setting and work-planning, I do have some general expectations.\nI am likely to consider your time in my course to be highly successful if you do at least one of the following things:\nTime spent being stuck doesn’t count as “productive hours” – get help if you need it!\n\nYou complete almost all assignments with a high degree of quality, including revising in response to my feedback.\nYou spend on average 10 productive hours of work time on this course outside of class.\nYou complete many assignments that I give you, and also propose and complete alternative work that demonstrates your learning and achievement."
  },
  {
    "objectID": "syllabus.html#directing-your-learning",
    "href": "syllabus.html#directing-your-learning",
    "title": "Syllabus",
    "section": "Directing Your Learning",
    "text": "Directing Your Learning\nThis course asks you to set your own goals and motivate yourself to achieve them. Neither of these tasks are easy. It’s ok to mess up every now and then – we all do! The real question is whether you’re going to look at mistakes and make time to reflect on what to do next time."
  },
  {
    "objectID": "syllabus.html#programming",
    "href": "syllabus.html#programming",
    "title": "Syllabus",
    "section": "Programming",
    "text": "Programming\n\nYou can write moderately-complex, object-oriented software.\nYou are comfortable reading software documentation and researching how to perform a task that you haven’t seen before.\nYou know what a terminal is and how to perform simple operations at the command line.\nYou have experience debugging your code and you are ready to do it a lot more."
  },
  {
    "objectID": "syllabus.html#math",
    "href": "syllabus.html#math",
    "title": "Syllabus",
    "section": "Math",
    "text": "Math\nI am assuming that you remember most of MATH 0200 and CSCI 0200. It’s ok if you haven’t memorized every single fact. What I need is for you to be ready to rapidly look up what you need so that you won’t be slowed down by math along the way.\n\nMatrix multiplication and inner products\nEverything about \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\).\n\nVisualizing linear spaces.\nEigenvalues, eigenvectors, positive-definite matrices.\nDerivatives, critical points of functions.\nSample spaces, probability distribution functions.\nRandom variables, mean and variance.\nConditional probability and expectations.\n\n\nReviews/Diagnostics\n\nThis resource from Stanford’s CS246 contains most of the linear algebra that you’ll need for the course. The only big topic that’s missing is treatment of the existence of solutions of the linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) in terms of the rank of \\(\\mathbf{A}\\). You don’t need to have memorized everything here, but most of it should look familiar.\nProbability is not a formal requirement for CSCI 0451, but some probability can certainly be useful. To brush up on some basics, I suggest Chapter 2 of Introduction to Probability for Data Science by Stanley Chan. This treatment may be a little more advanced than what you learned in CSCI 0200, but you should recognize many of the main ideas."
  },
  {
    "objectID": "syllabus.html#covid-19-considerations",
    "href": "syllabus.html#covid-19-considerations",
    "title": "Syllabus",
    "section": "COVID-19 Considerations",
    "text": "COVID-19 Considerations\n\nMasks Are Required in CSCI 0451\nThe Computer Science Department policy states that:\n\nWe in the Computer Science department value a safe learning and working environment for all. While we can’t eliminate the risks associated with COVID-19, evidence suggests that widespread masking can significantly reduce the transmission and severity of disease. In order to protect the health of our community, the CS department recommends that students and faculty wear masks in CS learning spaces, including classrooms, office hours, and public areas. We acknowledge the College policy gives instructors the final say over classroom masking requirements, and expect all students to respect instructors’ stated policies in each course.\n\nIn alignment with this policy, I require you to wear masks in class and office hours. I encourage you to wear masks during help sessions and at all other times when you are inside 75 Shannon Street.\nIf you arrive in class without a mask, I will offer you one. I will expect you to either wear it or excuse yourself from class that day."
  },
  {
    "objectID": "syllabus.html#academic-integrity-and-collaboration",
    "href": "syllabus.html#academic-integrity-and-collaboration",
    "title": "Syllabus",
    "section": "Academic Integrity and Collaboration",
    "text": "Academic Integrity and Collaboration\n\nAcademic Integrity\nBriefly, academic integrity means that you assume responsibility for ensuring that the work you submit demonstrates your learning and understanding.\nTo be frank, it’s pretty easy to act without integrity (i.e. cheat) in this course. First, there’s a lot of solution code for machine learning tasks in Python online. Second, I’m literally asking you all to post your assignments publicly online. So, there are lots of opportunities to turn in assignments without actually doing the learning that those assignments are designed to offer you.\nI assume that both of us want you to learn some cool stuff. Cheating stops you from doing that, and ultimately wastes both your time and mine. I won’t be vigorously hunting for academic integrity violations, but I may ask you to discuss code or theory with me in class or in our meetings. If I notice you struggling to explain code that you submitted for feedback, I may have questions.\nTrust me. Neither of us want this."
  },
  {
    "objectID": "syllabus.html#collaboration",
    "href": "syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nI love it! Please collaborate in ways that allow you and your collaboration partners to fully learn from and engage with the content. Sharing small snippets of code or math is often helpful to get someone unstuck, but sharing complete function implementations or mathematical arguments is usually counterproductive.\nHere are some general guidelines for how I think about collaboration."
  },
  {
    "objectID": "syllabus.html#general-advice",
    "href": "syllabus.html#general-advice",
    "title": "Syllabus",
    "section": "General Advice",
    "text": "General Advice\nI am always happy to talk with you about your future plans, including internships, research opportunities, and graduate school applications. Because I am a creature of the academy, I am less knowledgeable about industry jobs, although you are welcome to ask about those too. You can drop in during Student Hours or email me to make an appointment."
  },
  {
    "objectID": "syllabus.html#letters-of-recommendation",
    "href": "syllabus.html#letters-of-recommendation",
    "title": "Syllabus",
    "section": "Letters of Recommendation",
    "text": "Letters of Recommendation\nWriting letters of recommendation for students is a fundamental part of my job and something that I am usually very happy to do. Here’s how to ask me for a letter."
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Asking for Help",
    "section": "",
    "text": "Asking for help is a fundamental part of how you will learn in CSCI 0451. Do it often. Here is some wisdom on this topic from the Best Cat On the Internet:\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "help.html#dont-get-stuck",
    "href": "help.html#dont-get-stuck",
    "title": "Asking for Help",
    "section": "Don’t Get Stuck",
    "text": "Don’t Get Stuck\nWe want to productively challenge you, which is different from letting you get stuck. If you’ve spent more than 30 minutes without making any progress or change in your understanding, then that’s likely a sign that you should consult a new reading or other resource, ask for help from a classmate, a Course Assistant, or me."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Course Project",
    "section": "",
    "text": "The course project for CSCI 0451 is an opportunity for you to demonstrate your learning against one or more of the course’s six learning objectives on a topic of your choosing. Here’s the big picture:\n\n\nThere are five deliverables associated with your project:\n\nA project proposal, due at the beginning of Week 7. The purpose of the proposal is for you and your group to carefully outline what you want to work on and explain why it’s feasible. Your proposal will be in the form of a README.md file in a shared GitHub repository that will house your software.\nA mid-project update due in Week 9 or 10. This will be a short, informal presentation in which you will share what you’ve done with the class.\nThe project software, (aka the GitHub repository itself) due at the end of finals week.\nA project report in the form of an extended blog post in which you explain what you did, relate it to existing work, and show your experiments or other findings. The report is due at the end of finals week.\nA project presentation during Week 12. The presentation will be 7-8 minutes and executed as a group. It should involve a visual aid, usually slides.\n\nI’ll share more detailed information on each of these deliverables later in the course.\n\n\n\nI expect that most students will complete their projects in teams of 2-3 students. Individual projects and groups of 4 students should seek my permission prior to submitting their project proposal and explain the reason for such a small (or large) group.\n\n\n\nRemember that we have six learning learning objectives in this course. The project is actually its own objective—that is, part of the course goal is for you to have the experience of initiating and pursuing an idea that you design. The other five objectives are:\n\nTheory\nImplementation\nNavigation\nExperimentation\nSocial Responsibility\n\nIn general, I expect most projects to address at least two of these learning objectives. For example, a project in which you implement and test a new algorithm would address Theory, Implementation, and Experimentation. A project in which you work with a data set that you care about on a learning task using existing tools could address Navigation and Experimentation. A project in which you replicated the findings of a recent study on algorithmic bias could address Experimentation and Social Responsibility. There are lots of valid possibilities. Your project proposal will address which of these learning objectives your project will address, and your final report will describe what you learned under each objective.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "project.html#project-description",
    "href": "project.html#project-description",
    "title": "Course Project",
    "section": "",
    "text": "The course project for CSCI 0451 is an opportunity for you to demonstrate your learning against one or more of the course’s six learning objectives on a topic of your choosing. Here’s the big picture:\n\n\nThere are five deliverables associated with your project:\n\nA project proposal, due at the beginning of Week 7. The purpose of the proposal is for you and your group to carefully outline what you want to work on and explain why it’s feasible. Your proposal will be in the form of a README.md file in a shared GitHub repository that will house your software.\nA mid-project update due in Week 9 or 10. This will be a short, informal presentation in which you will share what you’ve done with the class.\nThe project software, (aka the GitHub repository itself) due at the end of finals week.\nA project report in the form of an extended blog post in which you explain what you did, relate it to existing work, and show your experiments or other findings. The report is due at the end of finals week.\nA project presentation during Week 12. The presentation will be 7-8 minutes and executed as a group. It should involve a visual aid, usually slides.\n\nI’ll share more detailed information on each of these deliverables later in the course.\n\n\n\nI expect that most students will complete their projects in teams of 2-3 students. Individual projects and groups of 4 students should seek my permission prior to submitting their project proposal and explain the reason for such a small (or large) group.\n\n\n\nRemember that we have six learning learning objectives in this course. The project is actually its own objective—that is, part of the course goal is for you to have the experience of initiating and pursuing an idea that you design. The other five objectives are:\n\nTheory\nImplementation\nNavigation\nExperimentation\nSocial Responsibility\n\nIn general, I expect most projects to address at least two of these learning objectives. For example, a project in which you implement and test a new algorithm would address Theory, Implementation, and Experimentation. A project in which you work with a data set that you care about on a learning task using existing tools could address Navigation and Experimentation. A project in which you replicated the findings of a recent study on algorithmic bias could address Experimentation and Social Responsibility. There are lots of valid possibilities. Your project proposal will address which of these learning objectives your project will address, and your final report will describe what you learned under each objective."
  },
  {
    "objectID": "project.html#what-makes-a-good-project",
    "href": "project.html#what-makes-a-good-project",
    "title": "Course Project",
    "section": "What Makes a Good Project?",
    "text": "What Makes a Good Project?\n\nBig Picture\nThere’s a lot more detail on this topic below, but there are two simple questions that you should ask yourselves when envisioning your project:\n\nWill I learn something by completing this project?  Will I be proud of this project once it’s done?\n\nIf the answer to both questions is “yes,” then your overall project idea is likely good. Feel free to approach me early if you want to talk over whether your project idea is suitable for the course.\n\n\nBoring Projects\nThere is a kind of machine learning project that I feel is very boring and doesn’t really teach all that much. I’ll call these “Kaggle-style” projects. A “Kaggle-style” project is a project that starts with a convenient, clean data set and ends with a test score.  KS projects don’t clean or explore the data; don’t implement new algorithms; and don’t think carefully about why one algorithm might be better than another for the data in question. Instead, they simply try a bunch of things and assess them with a validation or test score.  I will probably not be impressed with a Kaggle-style project, partly because these kinds of projects really only address the “Navigation” learning objective.The website Kaggle is famous for hosting machine learning competitions in which the goal is to train a model that achieves the best prediction score on test data.I am being a little unfair; some Kaggle submissions are of exceptionally high quality.\nIt’s fine (indeed, encouraged!) for you to find some data that interests you and apply machine learning methods to it in order to make predictions or understand the structure of the data. To deepen your project beyond “Kaggle style,” you can incorporate some or all of the following:\n\nWork with data that is messy and needs significant processing before it can be used for ML tasks.\nDesign a custom vectorization scheme (i.e. way of representing each data points as a vector), or experiment with several pre-implemented schemes.\nConstruct multiple visualizations of your data that highlight patterns you wish to model or questions that you wish to explore.\nConduct a careful audit of your model to understand whether it performs better in some situations than others.\n\n\n\nCritical Discussion\nOne thing that should be incorporated into both your proposal and your project writeup is a critical discussion of incentives and impacts in your model.\nIncorporate a critical discussion of incentives and impacts in your work.\n\nIf someone were paying you to develop this model, who would be paying and why? Why might someone want this model to be built? Are you comfortable with that?\nWho are the users of your work? Who could be affected by your work? Are these populations the same?\nAre there risks of substantial bias or harm associated with the work you produce?"
  },
  {
    "objectID": "project.html#ideas",
    "href": "project.html#ideas",
    "title": "Course Project",
    "section": "Ideas",
    "text": "Ideas\nHere are some suggestions for choosing project directions. It’s fine for your project to be something entirely different—indeed, I encourage it! The primary benefit of projects that I come up with is that they are more likely to “work.” The primary drawback is that they may not be what you’re interested in! Even projects that don’t fully meet their stated objectives can still be successful experiences that demonstrate learning for the course.\n\nTheory and Implementation\nIf you enjoy thinking about math and how to translate math into performant numerical code, then you may wish to consider implementing a machine learning algorithm that we haven’t implemented in blog posts. There are many candidates, and it can be partially up to you to decide. Since the project is bigger than a blog post…\n\n…Your implementation should likely be of a more complex algorithm than ones we implemented in blog posts already. Alternatively, you could implement several related algorithms.\n…You will likely need to perform more complicated experiments in order to demonstrate the performance of your implementation.\n\nBecause these projects involve theoretical content that we haven’t covered in class, it is a good idea to talk with me before committing to one of them.\n\nSupport Vector Machine\nSupport vector machines (SVMs) were among the state-of-the-art binary classifiers before the rise of deep neural networks. Support vector machines are convex linear classifiers like logistic regression, but have some special mathematical properties that enable them to make much faster predictions on new data by leveraging sparsity. A good SVM project would likely involve most or all of the following:\n\nImplementing SVM using a version of stochastic gradient descent called PEGASOS (Fig. 2 in Shalev-Shwartz, Singer, and Srebro (2007)).\nImplementing kernel SVM, which enables nonlinear decision boundaries, using a quadratic solver.\nTesting your results on several real and synthetic data sets for:\n\nRuntime of training.\nRuntime of prediction.\nAccuracy of prediction.\n\n\n\n\nFaster Gradient Descent\nGradient descent and its relatives apply to a wide range of machine learning algorithms. Gradient descent comes in many flavors:\n\nStochastic\nMomentum\nAccelerated\nPrimal-dual methods\nModified gradient methods for nonconvex problems\nNewton methods\n\netc. etc. One good project could be to implement several of these methods for one or more ML algorithms, comparing the runtime of the training step on a variety of data sets.\n\n\nOther\nI can throw you lots of other theoretical/implementation problems if you’re interested—come chat and we can discuss."
  },
  {
    "objectID": "project.html#applied-analysis-projects",
    "href": "project.html#applied-analysis-projects",
    "title": "Course Project",
    "section": "Applied Analysis Projects",
    "text": "Applied Analysis Projects\nYou may wish to take machine learning methods that we discussed in class and apply them to data about some topic that you care about. Some great project interests that I’ve heard include remote sensing, sports analytics, genome analysis, and text analysis/language modeling."
  },
  {
    "objectID": "project.html#audits-and-algorithmic-bias",
    "href": "project.html#audits-and-algorithmic-bias",
    "title": "Course Project",
    "section": "Audits and Algorithmic Bias",
    "text": "Audits and Algorithmic Bias\nMany of you may have an interest in further exploring topics related to fairness and bias in machine learning algorithms. This is a great topic! Some possibilities include:\n\nThoroughly reproducing the findings of papers that diagnose algorithmic bias with an available data set or algorithm, such as Obermeyer et al. (2019).\nWrite a critical review essay on the topic of algorithmic bias in a specific area. This should be a polished essay with lots of references and some specific, quantitative examples."
  },
  {
    "objectID": "data/dissecting-bias/data_dictionary.html",
    "href": "data/dissecting-bias/data_dictionary.html",
    "title": "Data Dictionary",
    "section": "",
    "text": "We made every effort to limit the number of variables in the synthetic data to the minimum used in the analysis, both for simplicity and to reduce risk of inadvertent disclosure of PHI, so variables not used in any analyses (e.g., hospitalizations, visit details, etc.) were not included.\nThe synthetic data file contains 48,784 rows (patient-years) and 160 columns/variables. One note is that, while the original dataset includes multiple observations (years) from the same patient, the synthetic dataset observations do not take this clustering into account; creating adequate time dependencies for multiple years of synthetic data posed significant challenges, and in fact none of our analyses rely on using multiple observations per patient (though of course we do account for this in calculating standard errors in the original paper).\nWe group the variables into the following categories: - Variables at time t: A vector of “outcomes” for a given calendar year (t): cost, health, program enrollment, and the commercial risk score. The remaining variables, which are indexed to the year prior to the outcomes (t-1), are used primarily as “predictors” in our experimental algorithms. - Demographic variables. - Comorbidity variables at time t-1: A vector of indicators for specific chronic comorbidities (illnesses) that were active in the previous year, and their sum. - Cost variables at time t-1: Costs claimed from the patients’ insurance payer, rounded to the nearest $100 and broken down by type of cost, over the previous year. - Biomarker/medication variables at time t-1: A set of indicators capturing normal or abnormal values (or missingness) of biomarkers or relevant medications, over the previous year.\nNotation: - _t: indicates this is a time dependent variable from year t - _tm1: indicates this is a time dependent variable from year t minus 1 (t-1)\n\n\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescription\nSample Data\n\n\n\n\nrisk_score_t\nFloat\nCommercial algorithmic risk score prediction for cost in year t, formed using data from year t-1\n1.32\n\n\nprogram_enrolled_t\nInteger\nIndicator for whether patient-year was enrolled in program\n0\n\n\ngagne_sum_t\nInteger\nTotal number of active chronic illnesses\n3\n\n\ncost_t\nFloat\nTotal medical expenditures, rounded to the nearest 100\n1000.00\n\n\ncost_avoidable_t\nFloat\nTotal avoidable (emergency + inpatient) medical expenditures, rounded to nearest 100\n100.00\n\n\nbps_mean_t\nFloat\nMean systolic blood pressure in year t\n120.0\n\n\nghba1c_mean_t\nFloat\nMean HbA1C in year t\n5.5\n\n\nhct_mean_t\nFloat\nMean hematocrit in year t\n40.8\n\n\ncre_mean_t\nFloat\nMean creatinine in year t\n0.78\n\n\nldl_mean_t\nFloat\nMean low-density lipoprotein in year t\n89.0\n\n\n\nTotal = 10 variables at time t\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescription\nSample Data\n\n\n\n\nrace\nString\nPatient race; values include ‘white’ or ‘black’\nwhite\n\n\ndem_female\nInteger\nIndicator for female gender\n1\n\n\ndem_age_band_18-24_tm1\nInteger\nIndicator for patient age between 18-24\n0\n\n\ndem_age_band_25-34_tm1\nInteger\nIndicator for patient age between 25-34\n0\n\n\ndem_age_band_35-44_tm1\nInteger\nIndicator for patient age between 35-44\n0\n\n\ndem_age_band_45-54_tm1\nInteger\nIndicator for patient age between 45-54\n1\n\n\ndem_age_band_55-64_tm1\nInteger\nIndicator for patient age between 55-64\n0\n\n\ndem_age_band_65-74_tm1\nInteger\nIndicator for patient age between 65-74\n0\n\n\ndem_age_band_75+_tm1\nInteger\nIndicator for patient age 75+\n0\n\n\n\nTotal = 9 demographic variables (including race)\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescription\nSample Data\n\n\n\n\ngagne_sum_tm1\nInteger\nTotal number of active illnesses\n2\n\n\nalcohol_elixhauser_tm1\nInteger\nIndicator for alcohol abuse\n0\n\n\nanemia_elixhauser_tm1\nInteger\nIndicator for deficiency anemia\n0\n\n\narrhythmia_elixhauser_tm1\nInteger\nIndicator for arrhythmia\n0\n\n\narthritis_elixhauser_tm1\nInteger\nIndicator for arthritis\n1\n\n\nbloodlossanemia_elixhauser_tm1\nInteger\nIndicator for blood loss anemia\n0\n\n\ncoagulopathy_elixhauser_tm1\nInteger\nIndicator for coagulopathy\n0\n\n\ncompdiabetes_elixhauser_tm1\nInteger\nIndicator for diabetes, complicated\n1\n\n\ndepression_elixhauser_tm1\nInteger\nIndicator for depression\n0\n\n\ndrugabuse_elixhauser_tm1\nInteger\nIndicator for drug abuse\n0\n\n\nelectrolytes_elixhauser_tm1\nInteger\nIndicator for electrolyte disorder\n0\n\n\nhypertension_elixhauser_tm1\nInteger\nIndicator for hypertension\n0\n\n\nhypothyroid_elixhauser_tm1\nInteger\nIndicator for hypothyroid\n0\n\n\nliver_elixhauser_tm1\nInteger\nIndicator for liver disease\n0\n\n\nneurodegen_elixhauser_tm1\nInteger\nIndicator for neurodegenerative disease\n0\n\n\nobesity_elixhauser_tm1\nInteger\nIndicator for obesity\n0\n\n\nparalysis_elixhauser_tm1\nInteger\nIndicator for paralysis\n0\n\n\npsychosis_elixhauser_tm1\nInteger\nIndicator for psychoses\n0\n\n\npulmcirc_elixhauser_tm1\nInteger\nIndicator for pulmonary circulation disorders\n0\n\n\npvd_elixhauser_tm1\nInteger\nIndicator for peripheral vascular disorders\n0\n\n\nrenal_elixhauser_tm1\nInteger\nIndicator for renal failure\n0\n\n\nuncompdiabetes_elixhauser_tm1\nInteger\nIndicator for diabetes, uncomplicated\n0\n\n\nvalvulardz_elixhauser_tm1\nInteger\nIndicator for valvular disease\n0\n\n\nwtloss_elixhauser_tm1\nInteger\nIndicator for weight loss\n0\n\n\ncerebrovasculardz_romano_tm1\nInteger\nIndicator for cerebrovascular disease\n0\n\n\nchf_romano_tm1\nInteger\nIndicator for congestive heart failure\n0\n\n\ndementia_romano_tm1\nInteger\nIndicator for dementia\n0\n\n\nhemiplegia_romano_tm1\nInteger\nIndicator for hemiplegia\n0\n\n\nhivaids_romano_tm1\nInteger\nIndicator for HIV/AIDS\n0\n\n\nmetastatic_romano_tm1\nInteger\nIndicator for metastasis\n0\n\n\nmyocardialinfarct_romano_tm1\nInteger\nIndicator for myocardial infarction\n0\n\n\npulmonarydz_romano_tm1\nInteger\nIndicator for pulmonary disease\n0\n\n\ntumor_romano_tm1\nInteger\nIndicator for tumor\n0\n\n\nulcer_romano_tm1\nInteger\nIndicator for ulcer\n0\n\n\n\nTotal = 34 comorbidity variables at time t-1\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescription\nSample Data\n\n\n\n\ncost_dialysis_tm1\nFloat\nTotal costs for dialysis, rounded to nearest 10\n990.00\n\n\ncost_emergency_tm1\nFloat\nTotal costs for emergency, rounded to nearest 10\n140.00\n\n\ncost_home_health_tm1\nFloat\nTotal costs for home health, rounded to nearest 10\n120.00\n\n\ncost_ip_medical_tm1\nFloat\nTotal costs for inpatient medical, rounded to nearest 10\n150.00\n\n\ncost_ip_surgical_tm1\nFloat\nTotal costs for inpatient surgical, rounded to nearest 10\n200.00\n\n\ncost_laboratory_tm1\nFloat\nTotal costs for laboratory, rounded to nearest 10\n90.00\n\n\ncost_op_primary_care_tm1\nFloat\nTotal costs for outpatient primary care, rounded to nearest 10\n270.00\n\n\ncost_op_specialists_tm1\nFloat\nTotal costs for outpatient specialists, rounded to nearest 10\n180.00\n\n\ncost_op_surgery_tm1\nFloat\nTotal costs for outpatient surgery, rounded to nearest 10\n110.00\n\n\ncost_other_tm1\nFloat\nTotal other costs, rounded to nearest 100\n300.00\n\n\ncost_pharmacy_tm1\nFloat\nTotal costs for pharmacy, rounded to nearest 10\n10.00\n\n\ncost_physical_therapy_tm1\nFloat\nTotal costs for physical therapy, rounded to nearest 10\n190.00\n\n\ncost_radiology_tm1\nFloat\nTotal costs for radiology, rounded to nearest 10\n120.00\n\n\n\nTotal = 13 cost variables at time t-1\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nData Type\nDescription\nSample Data\n\n\n\n\nlasix_dose_count_tm1\nInteger\nNumber of Lasix doses\n0\n\n\nlasix_min_daily_dose_tm1\nInteger\nMinimum daily dose of Lasix\n20\n\n\nlasix_mean_daily_dose_tm1\nFloat\nMean daily dose of Lasix\n20\n\n\nlasix_max_daily_dose_tm1\nInteger\nMaximum daily dose of Lasix\n20\n\n\ncre_tests_tm1\nInteger\nNumber of creatinine tests\n1\n\n\ncrp_tests_tm1\nInteger\nNumber of c-reactive protein tests\n0\n\n\nesr_tests_tm1\nInteger\nNumber of erythrocyte sedimentation rate tests\n1\n\n\nghba1c_tests_tm1\nInteger\nNumber of GHbA1c tests\n1\n\n\nhct_tests_tm1\nInteger\nNumber of hematocrit tests\n1\n\n\nldl_tests_tm1\nInteger\nNumber of LDL tests\n1\n\n\nnt_bnp_tests_tm1\nInteger\nNumber of BNP tests\n1\n\n\nsodium_tests_tm1\nInteger\nNumber of sodium tests\n1\n\n\ntrig_tests_tm1\nInteger\nNumber of triglycerides tests\n1\n\n\ncre_min-low_tm1\nInteger\nIndicator for low (&lt; 0.84) minimum creatinine test result\n0\n\n\ncre_min-high_tm1\nInteger\nIndicator for high (&gt; 1.21) minimum creatinine test result\n0\n\n\ncre_min-normal_tm1\nInteger\nIndicator for normal minimum creatinine test result\n1\n\n\ncre_mean-low_tm1\nInteger\nIndicator for low (&lt; 0.84) mean creatinine test result\n0\n\n\ncre_mean-high_tm1\nInteger\nIndicator for high (&gt; 1.21) mean creatinine test result\n0\n\n\ncre_mean-normal_tm1\nInteger\nIndicator for normal mean creatinine test result\n1\n\n\ncre_max-low_tm1\nInteger\nIndicator for low (&lt; 0.84) maximum creatinine test result\n0\n\n\ncre_max-high_tm1\nInteger\nIndicator for high (&gt; 1.21) maximum creatinine test result\n0\n\n\ncre_max-normal_tm1\nInteger\nIndicator for normal maximum creatinine test result\n1\n\n\ncrp_min-low_tm1\nInteger\nIndicator for low (&lt; 1) minimum c-reactive protein test result\n0\n\n\ncrp_min-high_tm1\nInteger\nIndicator for high (&gt; 3) minimum c-reactive protein test result\n0\n\n\ncrp_min-normal_tm1\nInteger\nIndicator for normal minimum c-reactive protein test result\n1\n\n\ncrp_mean-low_tm1\nInteger\nIndicator for low (&lt; 1) mean c-reactive protein test result\n0\n\n\ncrp_mean-high_tm1\nInteger\nIndicator for high (&gt; 3) mean c-reactive protein test result\n0\n\n\ncrp_mean-normal_tm1\nInteger\nIndicator for normal mean c-reactive protein test result\n1\n\n\ncrp_max-low_tm1\nInteger\nIndicator for low (&lt; 1) maximum c-reactive protein test result\n0\n\n\ncrp_max-high_tm1\nInteger\nIndicator for high (&gt; 3) maximum c-reactive protein test result\n0\n\n\ncrp_max-normal_tm1\nInteger\nIndicator for normal maximum c-reactive protein test result\n1\n\n\nesr_min-low_tm1\nInteger\nIndicator for low (&lt; 1) minimum erythrocyte sedimentation rate test result\n0\n\n\nesr_min-high_tm1\nInteger\nIndicator for high (&gt; 20) minimum erythrocyte sedimentation rate test result\n0\n\n\nesr_min-normal_tm1\nInteger\nIndicator for normal minimum erythrocyte sedimentation rate test result\n1\n\n\nesr_mean-low_tm1\nInteger\nIndicator for low (&lt; 1) mean erythrocyte sedimentation rate test result\n0\n\n\nesr_mean-high_tm1\nInteger\nIndicator for high (&gt; 20) mean erythrocyte sedimentation rate test result\n0\n\n\nesr_mean-normal_tm1\nInteger\nIndicator for normal mean erythrocyte sedimentation rate test result\n1\n\n\nesr_max-low_tm1\nInteger\nIndicator for low (&lt; 1) maximum erythrocyte sedimentation rate test result\n0\n\n\nesr_max-high_tm1\nInteger\nIndicator for high (&gt; 20) maximum erythrocyte sedimentation rate test result\n0\n\n\nesr_max-normal_tm1\nInteger\nIndicator for normal maximum erythrocyte sedimentation rate test result\n1\n\n\nghba1c_min-low_tm1\nInteger\nIndicator for low (&lt; 4) minimum GHbA1c test result\n0\n\n\nghba1c_min-high_tm1\nInteger\nIndicator for high (&gt; 5.7) minimum GHbA1c test result\n0\n\n\nghba1c_min-normal_tm1\nInteger\nIndicator for normal minimum GHbA1c test result\n1\n\n\nghba1c_mean-low_tm1\nInteger\nIndicator for low (&lt; 4) mean GHbA1c test result\n0\n\n\nghba1c_mean-high_tm1\nInteger\nIndicator for high (&gt; 5.7) mean GHbA1c test result\n0\n\n\nghba1c_mean-normal_tm1\nInteger\nIndicator for normal mean GHbA1c test result\n1\n\n\nghba1c_max-low_tm1\nInteger\nIndicator for low (&lt; 4) maximum GHbA1c test result\n0\n\n\nghba1c_max-high_tm1\nInteger\nIndicator for high (&gt; 5.7) maximum GHbA1c test result\n0\n\n\nghba1c_max-normal_tm1\nInteger\nIndicator for normal maximum GHbA1c test result\n1\n\n\nhct_min-low_tm1\nInteger\nIndicator for low (&lt; 35.5) minimum hematocrit test result\n0\n\n\nhct_min-high_tm1\nInteger\nIndicator for high (&gt; 48.6) minimum hematocrit test result\n0\n\n\nhct_min-normal_tm1\nInteger\nIndicator for normal minimum hematocrit test result\n1\n\n\nhct_mean-low_tm1\nInteger\nIndicator for low (&lt; 35.5) mean hematocrit test result\n0\n\n\nhct_mean-high_tm1\nInteger\nIndicator for high (&gt; 48.6) mean hematocrit test result\n0\n\n\nhct_mean-normal_tm1\nInteger\nIndicator for normal mean hematocrit test result\n1\n\n\nhct_max-low_tm1\nInteger\nIndicator for low (&lt; 35.5) maximum hematocrit test result\n0\n\n\nhct_max-high_tm1\nInteger\nIndicator for high (&gt; 48.6) maximum hematocrit test result\n0\n\n\nhct_max-normal_tm1\nInteger\nIndicator for normal maximum hematocrit test result\n1\n\n\nldl_min-low_tm1\nInteger\nIndicator for low (&lt; 50) minimum LDL test result\n0\n\n\nldl_min-high_tm1\nInteger\nIndicator for high (&gt; 99) minimum LDL test result\n0\n\n\nldl_min-normal_tm1\nInteger\nIndicator for normal minimum LDL test result\n1\n\n\nldl-mean-low_tm1\nInteger\nIndicator for low (&lt; 50) mean LDL test result\n0\n\n\nldl-mean-high_tm1\nInteger\nIndicator for high (&gt; 99) mean LDL test result\n0\n\n\nldl-mean-normal_tm1\nInteger\nIndicator for normal mean LDL test result\n1\n\n\nldl-max-low_tm1\nInteger\nIndicator for low (&lt; 50) maximum LDL test result\n0\n\n\nldl-max-high_tm1\nInteger\nIndicator for high (&gt; 99) maximum LDL test result\n0\n\n\nldl-max-normal_tm1\nInteger\nIndicator for normal maximum LDL test result\n1\n\n\nnt_bnp_min-low_tm1\nInteger\nIndicator for low (&lt; 100) minimum BNP test result\n0\n\n\nnt_bnp_min-high_tm1\nInteger\nIndicator for high (&gt; 450) minimum BNP test result\n0\n\n\nnt_bnp_min-normal_tm1\nInteger\nIndicator for normal minimum BNP test result\n1\n\n\nnt_bnp_mean-low_tm1\nInteger\nIndicator for low (&lt; 100) mean BNP test result\n0\n\n\nnt_bnp_mean-high_tm1\nInteger\nIndicator for high (&gt; 450) mean BNP test result\n0\n\n\nnt_bnp_mean-normal_tm1\nInteger\nIndicator for normal minimum BNP test result\n1\n\n\nnt_bnp_max-low_tm1\nInteger\nIndicator for low (&lt; 100) maximum BNP test result\n0\n\n\nnt_bnp_max-high_tm1\nInteger\nIndicator for high (&gt; 450) maximum BNP test result\n0\n\n\nnt_bnp_max-normal_tm1\nInteger\nIndicator for normal minimum BNP test result\n1\n\n\nsodium_min-low_tm1\nInteger\nIndicator for low (&lt; 135) minimum sodium test result\n0\n\n\nsodium_min-high\nInteger\nIndicator for high (&gt; 145) minimum sodium test result\n0\n\n\nsodium_min-normal_tm1\nInteger\nIndicator for normal minimum sodium test result\n1\n\n\nsodium_mean-low_tm1\nInteger\nIndicator for low (&lt; 135) mean sodium test result\n0\n\n\nsodium_mean-high_tm1\nInteger\nIndicator for high (&gt; 145) mean sodium test result\n0\n\n\nsodium_mean-normal_tm1\nInteger\nIndicator for normal mean sodium test result\n1\n\n\nsodium_max-low_tm1\nInteger\nIndicator for low (&lt; 135) maximum sodium test result\n0\n\n\nsodium_max-high_tm1\nInteger\nIndicator for high (&gt; 145) maximum sodium test result\n0\n\n\nsodium_max-normal_tm1\nInteger\nIndicator for normal maximum sodium test result\n1\n\n\ntrig_min-low_tm1\nInteger\nIndicator for low (&lt; 50) minimum triglycerides test result\n0\n\n\ntrig_min-high_tm1\nInteger\nIndicator for high (&gt; 150) minimum triglycerides test result\n0\n\n\ntrig_min-normal_tm1\nInteger\nIndicator for normal minimum triglycerides test result\n1\n\n\ntrig_mean-low_tm1\nInteger\nIndicator for low (&lt; 50) mean triglycerides test result\n0\n\n\ntrig_mean-high_tm1\nInteger\nIndicator for high (&gt; 150) mean triglycerides test result\n0\n\n\ntrig_mean-normal_tm1\nInteger\nIndicator for normal mean triglycerides test result\n1\n\n\ntrig_max-low_tm1\nInteger\nIndicator for low (&lt; 50) maximum triglycerides test result\n0\n\n\ntrig_max-high_tm1\nInteger\nIndicator for high (&gt; 150) maximum triglycerides test result\n0\n\n\ntrig_max-normal_tm1\nInteger\nIndicator for normal maximum triglycerides test result\n1\n\n\n\nTotal = 94 biomarker/medication variables at time t-1\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "data/dissecting-bias/data_dictionary.html#variables-at-time-t",
    "href": "data/dissecting-bias/data_dictionary.html#variables-at-time-t",
    "title": "Data Dictionary",
    "section": "",
    "text": "Variable\nData Type\nDescription\nSample Data\n\n\n\n\nrisk_score_t\nFloat\nCommercial algorithmic risk score prediction for cost in year t, formed using data from year t-1\n1.32\n\n\nprogram_enrolled_t\nInteger\nIndicator for whether patient-year was enrolled in program\n0\n\n\ngagne_sum_t\nInteger\nTotal number of active chronic illnesses\n3\n\n\ncost_t\nFloat\nTotal medical expenditures, rounded to the nearest 100\n1000.00\n\n\ncost_avoidable_t\nFloat\nTotal avoidable (emergency + inpatient) medical expenditures, rounded to nearest 100\n100.00\n\n\nbps_mean_t\nFloat\nMean systolic blood pressure in year t\n120.0\n\n\nghba1c_mean_t\nFloat\nMean HbA1C in year t\n5.5\n\n\nhct_mean_t\nFloat\nMean hematocrit in year t\n40.8\n\n\ncre_mean_t\nFloat\nMean creatinine in year t\n0.78\n\n\nldl_mean_t\nFloat\nMean low-density lipoprotein in year t\n89.0\n\n\n\nTotal = 10 variables at time t"
  },
  {
    "objectID": "data/dissecting-bias/data_dictionary.html#demographic-variables",
    "href": "data/dissecting-bias/data_dictionary.html#demographic-variables",
    "title": "Data Dictionary",
    "section": "",
    "text": "Variable\nData Type\nDescription\nSample Data\n\n\n\n\nrace\nString\nPatient race; values include ‘white’ or ‘black’\nwhite\n\n\ndem_female\nInteger\nIndicator for female gender\n1\n\n\ndem_age_band_18-24_tm1\nInteger\nIndicator for patient age between 18-24\n0\n\n\ndem_age_band_25-34_tm1\nInteger\nIndicator for patient age between 25-34\n0\n\n\ndem_age_band_35-44_tm1\nInteger\nIndicator for patient age between 35-44\n0\n\n\ndem_age_band_45-54_tm1\nInteger\nIndicator for patient age between 45-54\n1\n\n\ndem_age_band_55-64_tm1\nInteger\nIndicator for patient age between 55-64\n0\n\n\ndem_age_band_65-74_tm1\nInteger\nIndicator for patient age between 65-74\n0\n\n\ndem_age_band_75+_tm1\nInteger\nIndicator for patient age 75+\n0\n\n\n\nTotal = 9 demographic variables (including race)"
  },
  {
    "objectID": "data/dissecting-bias/data_dictionary.html#comorbidity-variables-at-time-t-1",
    "href": "data/dissecting-bias/data_dictionary.html#comorbidity-variables-at-time-t-1",
    "title": "Data Dictionary",
    "section": "",
    "text": "Variable\nData Type\nDescription\nSample Data\n\n\n\n\ngagne_sum_tm1\nInteger\nTotal number of active illnesses\n2\n\n\nalcohol_elixhauser_tm1\nInteger\nIndicator for alcohol abuse\n0\n\n\nanemia_elixhauser_tm1\nInteger\nIndicator for deficiency anemia\n0\n\n\narrhythmia_elixhauser_tm1\nInteger\nIndicator for arrhythmia\n0\n\n\narthritis_elixhauser_tm1\nInteger\nIndicator for arthritis\n1\n\n\nbloodlossanemia_elixhauser_tm1\nInteger\nIndicator for blood loss anemia\n0\n\n\ncoagulopathy_elixhauser_tm1\nInteger\nIndicator for coagulopathy\n0\n\n\ncompdiabetes_elixhauser_tm1\nInteger\nIndicator for diabetes, complicated\n1\n\n\ndepression_elixhauser_tm1\nInteger\nIndicator for depression\n0\n\n\ndrugabuse_elixhauser_tm1\nInteger\nIndicator for drug abuse\n0\n\n\nelectrolytes_elixhauser_tm1\nInteger\nIndicator for electrolyte disorder\n0\n\n\nhypertension_elixhauser_tm1\nInteger\nIndicator for hypertension\n0\n\n\nhypothyroid_elixhauser_tm1\nInteger\nIndicator for hypothyroid\n0\n\n\nliver_elixhauser_tm1\nInteger\nIndicator for liver disease\n0\n\n\nneurodegen_elixhauser_tm1\nInteger\nIndicator for neurodegenerative disease\n0\n\n\nobesity_elixhauser_tm1\nInteger\nIndicator for obesity\n0\n\n\nparalysis_elixhauser_tm1\nInteger\nIndicator for paralysis\n0\n\n\npsychosis_elixhauser_tm1\nInteger\nIndicator for psychoses\n0\n\n\npulmcirc_elixhauser_tm1\nInteger\nIndicator for pulmonary circulation disorders\n0\n\n\npvd_elixhauser_tm1\nInteger\nIndicator for peripheral vascular disorders\n0\n\n\nrenal_elixhauser_tm1\nInteger\nIndicator for renal failure\n0\n\n\nuncompdiabetes_elixhauser_tm1\nInteger\nIndicator for diabetes, uncomplicated\n0\n\n\nvalvulardz_elixhauser_tm1\nInteger\nIndicator for valvular disease\n0\n\n\nwtloss_elixhauser_tm1\nInteger\nIndicator for weight loss\n0\n\n\ncerebrovasculardz_romano_tm1\nInteger\nIndicator for cerebrovascular disease\n0\n\n\nchf_romano_tm1\nInteger\nIndicator for congestive heart failure\n0\n\n\ndementia_romano_tm1\nInteger\nIndicator for dementia\n0\n\n\nhemiplegia_romano_tm1\nInteger\nIndicator for hemiplegia\n0\n\n\nhivaids_romano_tm1\nInteger\nIndicator for HIV/AIDS\n0\n\n\nmetastatic_romano_tm1\nInteger\nIndicator for metastasis\n0\n\n\nmyocardialinfarct_romano_tm1\nInteger\nIndicator for myocardial infarction\n0\n\n\npulmonarydz_romano_tm1\nInteger\nIndicator for pulmonary disease\n0\n\n\ntumor_romano_tm1\nInteger\nIndicator for tumor\n0\n\n\nulcer_romano_tm1\nInteger\nIndicator for ulcer\n0\n\n\n\nTotal = 34 comorbidity variables at time t-1"
  },
  {
    "objectID": "data/dissecting-bias/data_dictionary.html#cost-variables-at-time-t-1",
    "href": "data/dissecting-bias/data_dictionary.html#cost-variables-at-time-t-1",
    "title": "Data Dictionary",
    "section": "",
    "text": "Variable\nData Type\nDescription\nSample Data\n\n\n\n\ncost_dialysis_tm1\nFloat\nTotal costs for dialysis, rounded to nearest 10\n990.00\n\n\ncost_emergency_tm1\nFloat\nTotal costs for emergency, rounded to nearest 10\n140.00\n\n\ncost_home_health_tm1\nFloat\nTotal costs for home health, rounded to nearest 10\n120.00\n\n\ncost_ip_medical_tm1\nFloat\nTotal costs for inpatient medical, rounded to nearest 10\n150.00\n\n\ncost_ip_surgical_tm1\nFloat\nTotal costs for inpatient surgical, rounded to nearest 10\n200.00\n\n\ncost_laboratory_tm1\nFloat\nTotal costs for laboratory, rounded to nearest 10\n90.00\n\n\ncost_op_primary_care_tm1\nFloat\nTotal costs for outpatient primary care, rounded to nearest 10\n270.00\n\n\ncost_op_specialists_tm1\nFloat\nTotal costs for outpatient specialists, rounded to nearest 10\n180.00\n\n\ncost_op_surgery_tm1\nFloat\nTotal costs for outpatient surgery, rounded to nearest 10\n110.00\n\n\ncost_other_tm1\nFloat\nTotal other costs, rounded to nearest 100\n300.00\n\n\ncost_pharmacy_tm1\nFloat\nTotal costs for pharmacy, rounded to nearest 10\n10.00\n\n\ncost_physical_therapy_tm1\nFloat\nTotal costs for physical therapy, rounded to nearest 10\n190.00\n\n\ncost_radiology_tm1\nFloat\nTotal costs for radiology, rounded to nearest 10\n120.00\n\n\n\nTotal = 13 cost variables at time t-1"
  },
  {
    "objectID": "data/dissecting-bias/data_dictionary.html#biomarkermedication-variables-at-time-t-1",
    "href": "data/dissecting-bias/data_dictionary.html#biomarkermedication-variables-at-time-t-1",
    "title": "Data Dictionary",
    "section": "",
    "text": "Variable\nData Type\nDescription\nSample Data\n\n\n\n\nlasix_dose_count_tm1\nInteger\nNumber of Lasix doses\n0\n\n\nlasix_min_daily_dose_tm1\nInteger\nMinimum daily dose of Lasix\n20\n\n\nlasix_mean_daily_dose_tm1\nFloat\nMean daily dose of Lasix\n20\n\n\nlasix_max_daily_dose_tm1\nInteger\nMaximum daily dose of Lasix\n20\n\n\ncre_tests_tm1\nInteger\nNumber of creatinine tests\n1\n\n\ncrp_tests_tm1\nInteger\nNumber of c-reactive protein tests\n0\n\n\nesr_tests_tm1\nInteger\nNumber of erythrocyte sedimentation rate tests\n1\n\n\nghba1c_tests_tm1\nInteger\nNumber of GHbA1c tests\n1\n\n\nhct_tests_tm1\nInteger\nNumber of hematocrit tests\n1\n\n\nldl_tests_tm1\nInteger\nNumber of LDL tests\n1\n\n\nnt_bnp_tests_tm1\nInteger\nNumber of BNP tests\n1\n\n\nsodium_tests_tm1\nInteger\nNumber of sodium tests\n1\n\n\ntrig_tests_tm1\nInteger\nNumber of triglycerides tests\n1\n\n\ncre_min-low_tm1\nInteger\nIndicator for low (&lt; 0.84) minimum creatinine test result\n0\n\n\ncre_min-high_tm1\nInteger\nIndicator for high (&gt; 1.21) minimum creatinine test result\n0\n\n\ncre_min-normal_tm1\nInteger\nIndicator for normal minimum creatinine test result\n1\n\n\ncre_mean-low_tm1\nInteger\nIndicator for low (&lt; 0.84) mean creatinine test result\n0\n\n\ncre_mean-high_tm1\nInteger\nIndicator for high (&gt; 1.21) mean creatinine test result\n0\n\n\ncre_mean-normal_tm1\nInteger\nIndicator for normal mean creatinine test result\n1\n\n\ncre_max-low_tm1\nInteger\nIndicator for low (&lt; 0.84) maximum creatinine test result\n0\n\n\ncre_max-high_tm1\nInteger\nIndicator for high (&gt; 1.21) maximum creatinine test result\n0\n\n\ncre_max-normal_tm1\nInteger\nIndicator for normal maximum creatinine test result\n1\n\n\ncrp_min-low_tm1\nInteger\nIndicator for low (&lt; 1) minimum c-reactive protein test result\n0\n\n\ncrp_min-high_tm1\nInteger\nIndicator for high (&gt; 3) minimum c-reactive protein test result\n0\n\n\ncrp_min-normal_tm1\nInteger\nIndicator for normal minimum c-reactive protein test result\n1\n\n\ncrp_mean-low_tm1\nInteger\nIndicator for low (&lt; 1) mean c-reactive protein test result\n0\n\n\ncrp_mean-high_tm1\nInteger\nIndicator for high (&gt; 3) mean c-reactive protein test result\n0\n\n\ncrp_mean-normal_tm1\nInteger\nIndicator for normal mean c-reactive protein test result\n1\n\n\ncrp_max-low_tm1\nInteger\nIndicator for low (&lt; 1) maximum c-reactive protein test result\n0\n\n\ncrp_max-high_tm1\nInteger\nIndicator for high (&gt; 3) maximum c-reactive protein test result\n0\n\n\ncrp_max-normal_tm1\nInteger\nIndicator for normal maximum c-reactive protein test result\n1\n\n\nesr_min-low_tm1\nInteger\nIndicator for low (&lt; 1) minimum erythrocyte sedimentation rate test result\n0\n\n\nesr_min-high_tm1\nInteger\nIndicator for high (&gt; 20) minimum erythrocyte sedimentation rate test result\n0\n\n\nesr_min-normal_tm1\nInteger\nIndicator for normal minimum erythrocyte sedimentation rate test result\n1\n\n\nesr_mean-low_tm1\nInteger\nIndicator for low (&lt; 1) mean erythrocyte sedimentation rate test result\n0\n\n\nesr_mean-high_tm1\nInteger\nIndicator for high (&gt; 20) mean erythrocyte sedimentation rate test result\n0\n\n\nesr_mean-normal_tm1\nInteger\nIndicator for normal mean erythrocyte sedimentation rate test result\n1\n\n\nesr_max-low_tm1\nInteger\nIndicator for low (&lt; 1) maximum erythrocyte sedimentation rate test result\n0\n\n\nesr_max-high_tm1\nInteger\nIndicator for high (&gt; 20) maximum erythrocyte sedimentation rate test result\n0\n\n\nesr_max-normal_tm1\nInteger\nIndicator for normal maximum erythrocyte sedimentation rate test result\n1\n\n\nghba1c_min-low_tm1\nInteger\nIndicator for low (&lt; 4) minimum GHbA1c test result\n0\n\n\nghba1c_min-high_tm1\nInteger\nIndicator for high (&gt; 5.7) minimum GHbA1c test result\n0\n\n\nghba1c_min-normal_tm1\nInteger\nIndicator for normal minimum GHbA1c test result\n1\n\n\nghba1c_mean-low_tm1\nInteger\nIndicator for low (&lt; 4) mean GHbA1c test result\n0\n\n\nghba1c_mean-high_tm1\nInteger\nIndicator for high (&gt; 5.7) mean GHbA1c test result\n0\n\n\nghba1c_mean-normal_tm1\nInteger\nIndicator for normal mean GHbA1c test result\n1\n\n\nghba1c_max-low_tm1\nInteger\nIndicator for low (&lt; 4) maximum GHbA1c test result\n0\n\n\nghba1c_max-high_tm1\nInteger\nIndicator for high (&gt; 5.7) maximum GHbA1c test result\n0\n\n\nghba1c_max-normal_tm1\nInteger\nIndicator for normal maximum GHbA1c test result\n1\n\n\nhct_min-low_tm1\nInteger\nIndicator for low (&lt; 35.5) minimum hematocrit test result\n0\n\n\nhct_min-high_tm1\nInteger\nIndicator for high (&gt; 48.6) minimum hematocrit test result\n0\n\n\nhct_min-normal_tm1\nInteger\nIndicator for normal minimum hematocrit test result\n1\n\n\nhct_mean-low_tm1\nInteger\nIndicator for low (&lt; 35.5) mean hematocrit test result\n0\n\n\nhct_mean-high_tm1\nInteger\nIndicator for high (&gt; 48.6) mean hematocrit test result\n0\n\n\nhct_mean-normal_tm1\nInteger\nIndicator for normal mean hematocrit test result\n1\n\n\nhct_max-low_tm1\nInteger\nIndicator for low (&lt; 35.5) maximum hematocrit test result\n0\n\n\nhct_max-high_tm1\nInteger\nIndicator for high (&gt; 48.6) maximum hematocrit test result\n0\n\n\nhct_max-normal_tm1\nInteger\nIndicator for normal maximum hematocrit test result\n1\n\n\nldl_min-low_tm1\nInteger\nIndicator for low (&lt; 50) minimum LDL test result\n0\n\n\nldl_min-high_tm1\nInteger\nIndicator for high (&gt; 99) minimum LDL test result\n0\n\n\nldl_min-normal_tm1\nInteger\nIndicator for normal minimum LDL test result\n1\n\n\nldl-mean-low_tm1\nInteger\nIndicator for low (&lt; 50) mean LDL test result\n0\n\n\nldl-mean-high_tm1\nInteger\nIndicator for high (&gt; 99) mean LDL test result\n0\n\n\nldl-mean-normal_tm1\nInteger\nIndicator for normal mean LDL test result\n1\n\n\nldl-max-low_tm1\nInteger\nIndicator for low (&lt; 50) maximum LDL test result\n0\n\n\nldl-max-high_tm1\nInteger\nIndicator for high (&gt; 99) maximum LDL test result\n0\n\n\nldl-max-normal_tm1\nInteger\nIndicator for normal maximum LDL test result\n1\n\n\nnt_bnp_min-low_tm1\nInteger\nIndicator for low (&lt; 100) minimum BNP test result\n0\n\n\nnt_bnp_min-high_tm1\nInteger\nIndicator for high (&gt; 450) minimum BNP test result\n0\n\n\nnt_bnp_min-normal_tm1\nInteger\nIndicator for normal minimum BNP test result\n1\n\n\nnt_bnp_mean-low_tm1\nInteger\nIndicator for low (&lt; 100) mean BNP test result\n0\n\n\nnt_bnp_mean-high_tm1\nInteger\nIndicator for high (&gt; 450) mean BNP test result\n0\n\n\nnt_bnp_mean-normal_tm1\nInteger\nIndicator for normal minimum BNP test result\n1\n\n\nnt_bnp_max-low_tm1\nInteger\nIndicator for low (&lt; 100) maximum BNP test result\n0\n\n\nnt_bnp_max-high_tm1\nInteger\nIndicator for high (&gt; 450) maximum BNP test result\n0\n\n\nnt_bnp_max-normal_tm1\nInteger\nIndicator for normal minimum BNP test result\n1\n\n\nsodium_min-low_tm1\nInteger\nIndicator for low (&lt; 135) minimum sodium test result\n0\n\n\nsodium_min-high\nInteger\nIndicator for high (&gt; 145) minimum sodium test result\n0\n\n\nsodium_min-normal_tm1\nInteger\nIndicator for normal minimum sodium test result\n1\n\n\nsodium_mean-low_tm1\nInteger\nIndicator for low (&lt; 135) mean sodium test result\n0\n\n\nsodium_mean-high_tm1\nInteger\nIndicator for high (&gt; 145) mean sodium test result\n0\n\n\nsodium_mean-normal_tm1\nInteger\nIndicator for normal mean sodium test result\n1\n\n\nsodium_max-low_tm1\nInteger\nIndicator for low (&lt; 135) maximum sodium test result\n0\n\n\nsodium_max-high_tm1\nInteger\nIndicator for high (&gt; 145) maximum sodium test result\n0\n\n\nsodium_max-normal_tm1\nInteger\nIndicator for normal maximum sodium test result\n1\n\n\ntrig_min-low_tm1\nInteger\nIndicator for low (&lt; 50) minimum triglycerides test result\n0\n\n\ntrig_min-high_tm1\nInteger\nIndicator for high (&gt; 150) minimum triglycerides test result\n0\n\n\ntrig_min-normal_tm1\nInteger\nIndicator for normal minimum triglycerides test result\n1\n\n\ntrig_mean-low_tm1\nInteger\nIndicator for low (&lt; 50) mean triglycerides test result\n0\n\n\ntrig_mean-high_tm1\nInteger\nIndicator for high (&gt; 150) mean triglycerides test result\n0\n\n\ntrig_mean-normal_tm1\nInteger\nIndicator for normal mean triglycerides test result\n1\n\n\ntrig_max-low_tm1\nInteger\nIndicator for low (&lt; 50) maximum triglycerides test result\n0\n\n\ntrig_max-high_tm1\nInteger\nIndicator for high (&gt; 150) maximum triglycerides test result\n0\n\n\ntrig_max-normal_tm1\nInteger\nIndicator for normal maximum triglycerides test result\n1\n\n\n\nTotal = 94 biomarker/medication variables at time t-1"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Readings in normal font should be completed and annotated ahead of lecture.\nReadings in italic provide optional additional depth on the material.\n\nAssignments are listed on the day when I suggest you begin working on them.\n© Phil Chodrow, 2023"
  },
  {
    "objectID": "schedule.html#week-2",
    "href": "schedule.html#week-2",
    "title": "Schedule",
    "section": "Week 2",
    "text": "Week 2\n\n\n\n\n\n\n\n\n\n\n\nM\nFeb. 20\nConvex Linear Models and Logistic Regression\n\n\n\n\n\nWe discuss the modeling choices necessary to make the empirical risk minimization problem for linear classifiers tractable. In doing so we discuss convex functions and some of their properties that are relevant for optimization. Finally, we introduce logistic regression as an example of a convex linear classifier.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nDaumé 2.1-2.7\nDaumé 7.1-7.3\nHardt and Recht, p. 70-77\n\nNotes\nLecture notes\n\nWarmup\nConvexity\n\n\n\nW\nFeb. 22\nOptimization via Gradient Descent\n\n\n\n\n\nWe discuss standard mathematical methods for empirical risk minimization, including gradient descent and stochastic gradient descent. We also recontextualize the perceptron algorithm as stochastic subgradient descent for a linear classifier with a specific loss function.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nDaumé 7.4-7.6\nDiesenroth, Faisal, and Soon, p. 225-233\n\nNotes\nLecture notes\n\nWarmup\nGradient Descent\nAssignments\nBlog post: gradient descent"
  },
  {
    "objectID": "schedule.html#week-3",
    "href": "schedule.html#week-3",
    "title": "Schedule",
    "section": "Week 3",
    "text": "Week 3\n\n\n\n\n\n\n\n\n\n\n\nM\nFeb. 27\nFeatures, Regularization, and Nonlinear Decision Boundaries\n\n\n\n\n\nWe learn how to use feature maps to help our convex linear classifiers learn nonlinear patterns. We also introduce the problem of overfitting and introduce feature selection and regularization as methods for addressing this problem.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\nNavigation\nExperimentation\n\nReading\nIntroducing Scikit-Learn\nHyperparameters and Model Validation\nFeature Engineering\n\nNotes\nLecture notes\nLive version\n\nWarmup\nGradient Descent Again\nAssignments\nACTUAL REAL DUE DATE: Reflective Goal-Setting due 2/27\n\n\nW\nMar. 01\nClassification in Practice\n\n\n\n\n\nWe work through a complete modeling workflow for the Titanic survival data set. Along the way, we work with data frames and discuss cross-validation.\n\n\n\n\nLearning Objectives\nNavigation\nExperimentation\n\nReading\nDaumé Chapter 2 You may find it useful to review Chapter 1 as well.\nData Manipulation with Pandas (Focus on the sections up to and including \"Aggregation and Grouping\")\n\nNotes\nLecture notes\nLive version\n\nWarmup\nOverfitting and the Scientific Method\nAssignments\nBlog post: kernel logistic regression\nOR\nBlog post: penguins"
  },
  {
    "objectID": "schedule.html#week-4",
    "href": "schedule.html#week-4",
    "title": "Schedule",
    "section": "Week 4",
    "text": "Week 4\n\n\n\n\n\n\n\n\n\n\n\nM\nMar. 06\nBeyond Convex Linear Classifiers\n\n\n\n\n\nWe discuss several examples of other classifiers at a high level, including some that are nonlinear or nonconvex.\n\n\n\n\nLearning Objectives\nNavigation\n\nReading\nNA\n\nNotes\nLecture notes\nLive version\n\n\n\n\n\nW\nMar. 08\nLinear Regression\n\n\n\n\n\nWe introduce linear regression, another convex linear model suitable for predicting real numbers instead of class labels.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nNA\n\nNotes\nLecture notes\nLive version\n\n\nAssignments\nBlog post: Linear regression"
  },
  {
    "objectID": "schedule.html#week-5",
    "href": "schedule.html#week-5",
    "title": "Schedule",
    "section": "Week 5",
    "text": "Week 5\n\n\n\n\n\n\n\n\n\n\n\nM\nMar. 13\nIntroduction to Bias and Fairness\n\n\n\n\n\nTBD\n\n\n\n\nLearning Objectives\nSocial Responsibility\nExperimentation\n\nReading\nMachine Bias by Julia Angwin et al. for ProPublica.\nFair prediction with disparate impact by Alexandra Chouldechova, Sections 1 and 2.\nInherent trade-offs in the fair determination of risk scores by Jon Kleinberg et al, pages 1-5.\n\nNotes\nLecture notes\nLive version\n\nWarmup\nBalancing Classification Rates\n\n\n\nW\nMar. 15\nCritical Perspectives\n\n\n\n\n\nWe discuss limitations of the quantitative approach to studying discrimination, as well as critical perspectives on the role that automated decision systems play in surveilling and controlling marginalized individuals.\n\n\n\n\nLearning Objectives\nSocial Responsibility\nExperimentation\n\nReading\nThe Limits of the Quantitative Approach to Discrimination, speech by Arvind Narayanan\n\"The Digital Poorhouse\" by Virginia Eubanks for Harper's Magazine\n\nNotes\nTBD\n\nWarmup\nLimits of the Quantitative Approach\nAssignments\nBlog post: Limits of quantitative methods\nOR\nBlog post: Auditing allocative bias"
  },
  {
    "objectID": "schedule.html#week-6",
    "href": "schedule.html#week-6",
    "title": "Schedule",
    "section": "Week 6",
    "text": "Week 6\n\n\n\n\n\n\n\n\n\n\n\nM\nMar. 27\nVectorization\n\n\n\n\n\nWe discuss some ways by which complex objects like images and especially text can be represented as numerical vectors for machine learning algorithms.\n\n\n\n\nLearning Objectives\nNavigation\nExperimentation\n\nReading\nMurphy, Chapter 1. This is not related to vectorization; it's for you to get oriented on some possible project ideas. Don't worry about any math you don't understand.\nCourse project description\n\nNotes\nLecture notes\nLive version\n\nWarmup\nPitch a Project Idea\nAssignments\nACTUAL REAL DUE DATE: Mid-semester reflection due 4/05\n\n\nW\nMar. 29\nIntroducing Unsupervised Learning: Topic Modeling\n\n\n\n\n\nWe begin to discuss unsupervised learning, with topic modeling as our initial example.\n\n\n\n\nLearning Objectives\nTheory\nNavigation\nExperimentation\n\nReading\nPrincipal Component Analysis from the Python Data Science Handbook\n\nNotes\nLecture notes\nLive version\n\nWarmup\nVectorization Brainstorm\nAssignments\nACTUAL REAL DUE DATE: Project Proposal due 4/07"
  },
  {
    "objectID": "schedule.html#week-7",
    "href": "schedule.html#week-7",
    "title": "Schedule",
    "section": "Week 7",
    "text": "Week 7\n\n\n\n\n\n\n\n\n\n\n\nM\nApr. 03\nClustering Data\n\n\n\n\n\nWe continue our discussion of unsupervised learning with two methods for clustering sets of data.\n\n\n\n\nLearning Objectives\nTheory\nNavigation\nExperimentation\n\nReading\nK-Means Clustering from the Python Data Science Handbook\n\nNotes\nLecture notes\nLive version\n\nWarmup\nK-Means Compression\nAssignments\nBlog post: Unsupervised learning with linear algebra (however, using this time to complete a previous blog post is also highly recommended)\n\n\nW\nApr. 05\nIntroducing Deep Learning\n\n\n\n\n\nWe begin our discussion of deep learning with a quick theoretical motivation and a first glance at the PyTorch package.\n\n\n\n\nLearning Objectives\nTheory\nNavigation\n\nReading\nLecture 1, Introduction from Chinmay Hegde's course on deep learning at NYU\n\nNotes\nLecture notes\nLive version\n\nWarmup\nIntroducing Tensors"
  },
  {
    "objectID": "schedule.html#week-8",
    "href": "schedule.html#week-8",
    "title": "Schedule",
    "section": "Week 8",
    "text": "Week 8\n\n\n\n\n\n\n\n\n\n\n\nM\nApr. 10\nOptimization For Deep Learning\n\n\n\n\n\nWe begin a discussion of the training process for neural networks, which requires efficient computation of gradients via backpropagation and efficient variations of gradient descent.\n\n\n\n\nLearning Objectives\nTheory\nImplementation\n\nReading\nLecture 2, Neural Nets from Chinmay Hegde's course on deep learning at NYU\n\nNotes\nLecture notes\nLive version\n\nWarmup\nEfficient Differentiation\nAssignments\nBlog post: Optimization with Adam (however, using this time to complete a previous blog post is also highly recommended)\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "My First Blog Post\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items\n\n  © Phil Chodrow, 2023"
  }
]