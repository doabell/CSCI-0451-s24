{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: |\n",
        "  Text Generation\n",
        "author: Phil Chodrow\n",
        "bibliography: ../refs.bib\n",
        "format: \n",
        "  html: \n",
        "    code-fold: false\n",
        "    cache: true\n",
        "    callout-appearance: minimal\n",
        "    cap-location: margin\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/text-generation.ipynb\" target=\"_parent\">Open these notes in Google Colab</a>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/middlebury-csci-0451/CSCI-0451/blob/main/lecture-notes/text-generation-live.ipynb\" target=\"_parent\">Open the live version in Google Colab</a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qiYn6e_yQkcs"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "In this set of notes, we'll see a simple example of how to design and train models that perform *text generation*. Large language models (often called *chatbots*) are one familiar technology that uses text generation, while autocomplete features on websites and your devices are another. The text generation task is: \n",
        "\n",
        "> Given a text prompt, return a sequence of text that appears realistic as a follow-up to that prompt. \n",
        "\n",
        "Except for a brief foray into unsupervised learning, almost all of our attention in this course has been focused on prediction problems. At first glance, it may not appear that text generation involves any prediction at all. However, modern approaches to text generation rely fundamentally on supervised learning through the framework of *next token prediction*. \n",
        "\n",
        "## Next Token Prediction\n",
        "\n",
        "The *next token prediction* problem is to predict a single *token* in terms of previous tokens. A *token* is a single \"unit\" of text. What counts as a unit is somewhat flexible. In some cases, each token might be a single character: \"a\" is a token, \"b\" is a token, etc. In other cases, each token might be a word. [Many modern models do something in between and let tokens represent common short sequences of characters using *[byte-pair encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)*.]{.aside}\n",
        "\n",
        "For this set of lecture notes, we're going to treat *words* and *punctuation* as tokens. The next token prediction problem is: \n",
        "\n",
        "> Given a sequence of tokens, predict the next token in the sequence. \n",
        "\n",
        "For example, suppose that our sequence of tokens is \n",
        "\n",
        "> \"A computer science student\"\n",
        "\n",
        "We'd like to predict the next token in the sequence. Some likely candidates: \n",
        "\n",
        "- \"*is*\"\n",
        "- \"*codes*\" \n",
        "- \"*will*\"\n",
        "\n",
        "etc. On the other hand, some unlikely candidates: \n",
        "\n",
        "- \"*mango*\"\n",
        "- \"*grassy*\"\n",
        "- \"*tree*\"\n",
        "\n",
        "So, we can think of this as a prediction, even a classification problem: the sequence \"*A computer science student*\" might be classified as \"the category of sequences that are likely to be followed by the word *is*\". \n",
        "\n",
        "Once we have trained a model, the text generation task involves asking that model to make predictions, using those predictions to form new tokens, and then feeding those new tokens into the model again to get even more new tokens, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ehlyVuF5k9Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import string\n",
        "from torchsummary import summary\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.utils.data as data\n",
        "from torch import nn\n",
        "from torch.nn.functional import relu\n",
        "import re\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Our Task\n",
        "\n",
        "Today, we are going to see whether we can teach an algorithm to understand and reproduce the pinnacle of cultural achievement; the benchmark against which all art is to be judged; the mirror that reveals to humany its truest self. I speak, of course, of *Star Trek: Deep Space Nine.*\n",
        "\n",
        "<figure class=\"image\" style=\"width:300px\">\n",
        "  <img src=\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/_images/DS9.jpg\" alt=\"\">\n",
        "  <figcaption><i></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "In particular, we are going to attempt to teach a neural  network to generate *episode scripts*. This a text generation task: after training, our hope is that our model will be able to create scripts that are reasonably realistic in their appearance. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "## miscellaneous data cleaning\n",
        "\n",
        "start_episode = 20\n",
        "num_episodes = 25\n",
        "\n",
        "url = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\n",
        "star_trek_scripts = pd.read_json(url)\n",
        "\n",
        "cleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\n",
        "cleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\n",
        "text = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\n",
        "for char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n",
        "    text = text.replace(char, \"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a *long* string of text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "788662"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's what it looks like when printed: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Last\n",
            "time on Deep Space Nine.  \n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call\n",
            "themselves the Circle. \n",
            "O'BRIEN: What gives them the right to mess up our station? \n",
            "ODO: They're an extremist faction who believe in Bajor for the\n",
            "Bajorans. \n",
            "SISKO: I can't loan you a Starfleet runabout without knowing where you\n",
            "plan on taking it. \n",
            "KIRA: To Cardassia Four to rescue a Bajoran prisoner of war. \n",
            "(The prisoners are rescued.) \n",
            "KIRA: Come on. We have a ship waiting. \n",
            "JARO: What you \n"
          ]
        }
      ],
      "source": [
        "print(text[0:500])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The string in raw form doesn't look quite as nice: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'  Last\\ntime on Deep Space Nine.  \\nSISKO: This is the emblem of the Alliance for Global Unity. They c'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[0:100]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Prep \n",
        "\n",
        "### Tokenization\n",
        "\n",
        "In order to feed this string into a language model, we are going to need to split it into tokens. For today, we are going to treat punctuation, newline `\\n` characters, and words as tokens. Here's a hand-rolled tokenizer that achieves this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    \n",
        "    # empty list of tokens\n",
        "    out = []\n",
        "    \n",
        "    # start by splitting into lines and candidate tokens\n",
        "    # candidate tokens are separated by spaces\n",
        "    L = [s.split() for s in text.split(\"\\n\")]\n",
        "    \n",
        "    # for each list of candidate tokens \n",
        "    for line in L:\n",
        "        # scrub punctuation off beginning and end, adding to out as needed\n",
        "        for token in line:             \n",
        "            while (len(token) > 0) and (token[0] in string.punctuation):\n",
        "                out.append(token[0])\n",
        "                token = token[1:]\n",
        "            \n",
        "            stack = []\n",
        "            while (len(token) > 0) and (token[-1] in string.punctuation):\n",
        "                stack.insert(0, token[-1]) \n",
        "                token = token[:-1]\n",
        "            \n",
        "            out.append(token)\n",
        "            if len(stack) > 0:\n",
        "                out += stack\n",
        "        out += [\"\\n\"]\n",
        "    \n",
        "    # return the list of tokens, except for the final \\n\n",
        "    return out[:-1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's this tokenizer in action: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Last',\n",
              " '\\n',\n",
              " 'time',\n",
              " 'on',\n",
              " 'Deep',\n",
              " 'Space',\n",
              " 'Nine',\n",
              " '.',\n",
              " '\\n',\n",
              " 'SISKO',\n",
              " ':',\n",
              " 'This']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Last\\ntime on Deep Space Nine. \\n SISKO: This\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's tokenize the entire string: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "token_seq = tokenizer(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assembling the Data Set \n",
        "\n",
        "What we're now going to do is assemble the complete list of tokens into a series of predictor sequences and target tokens. The code below does this. The `WINDOW` controls how long each predictor sequence should be, and the `STEP` controls how many sequences we extract. A `STEP` of 1 would be all possible sequences. I've increased the `STEP` to 50 to reduce the size of our data for practical purposes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_len = 10\n",
        "STEP = 1\n",
        "\n",
        "predictors = []\n",
        "targets    = []\n",
        "\n",
        "for i in range(0, len(token_seq) - seq_len - 1, STEP):\n",
        "    predictors.append(token_seq[i:(i+seq_len)])\n",
        "    targets.append(token_seq[seq_len+i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's how this looks: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[')', '\\n', 'KIRA', ':', 'Come', 'on', '.', 'We', 'have', 'a'] | ship\n",
            "['\\n', 'KIRA', ':', 'Come', 'on', '.', 'We', 'have', 'a', 'ship'] | waiting\n",
            "['KIRA', ':', 'Come', 'on', '.', 'We', 'have', 'a', 'ship', 'waiting'] | .\n",
            "[':', 'Come', 'on', '.', 'We', 'have', 'a', 'ship', 'waiting', '.'] | \n",
            "\n",
            "['Come', 'on', '.', 'We', 'have', 'a', 'ship', 'waiting', '.', '\\n'] | JARO\n"
          ]
        }
      ],
      "source": [
        "for i in range(100, 105):\n",
        "    print(predictors[i], end = \"\")\n",
        "    print(\" | \" + targets[i])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our next task is to convert all these tokens into unique integers, just like we did for text classification (because this basically *is* still text classification). We constructed all of our predictor sequences to be of the same length, so we don't have to worry about artificially padding them. This makes our task of preparing the data set much easier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19, 1, 28, 3, 302, 22, 2, 83, 23, 10] | 161\n",
            "[1, 28, 3, 302, 22, 2, 83, 23, 10, 161] | 448\n",
            "[28, 3, 302, 22, 2, 83, 23, 10, 161, 448] | 2\n",
            "[3, 302, 22, 2, 83, 23, 10, 161, 448, 2] | 1\n",
            "[302, 22, 2, 83, 23, 10, 161, 448, 2, 1] | 399\n"
          ]
        }
      ],
      "source": [
        "vocab = build_vocab_from_iterator(iter(predictors), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "X = [vocab(x) for x in predictors]\n",
        "y = vocab(targets)\n",
        "\n",
        "## here's how our data looks now: \n",
        "\n",
        "for i in range(100, 105):\n",
        "    print(X[i], end = \"\")\n",
        "    print(\" | \" + str(y[i]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since our predictors are all in the same shape, we can go ahead and immediately construct the tensors and data sets we need: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = len(X)\n",
        "\n",
        "X = torch.tensor(X, dtype = torch.int64).reshape(n, seq_len).to(device)\n",
        "y = torch.tensor(y).to(device)\n",
        "\n",
        "data_set    = data.TensorDataset(X, y)\n",
        "data_loader = data.DataLoader(data_set, shuffle=True, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10]) torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "X, y = next(iter(data_loader))\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1511"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data_loader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n",
        "\n",
        "Our model is going to be relatively simple. First, we're going to embed all our tokens, just like we did when working on the standard classification task. Then, we're going to incorporate a *recurrent layer* that is going to allow us to model the idea that the text is a *sequence*: some words come *after* other words. \n",
        "\n",
        "### Recurrent Architecture\n",
        "\n",
        "Atop our word embedding layer we also incorporate a *long short-term memory* layer or LSTM. LSTMs are a type of *recurrent* neural network layer.  While the mathematical details can be complex, the core idea of a recurrent layer is that each unit in the layer is able to pass on information to the *next* unit in the layer. In much the same way that convolutional layers are specialized for analyzing images, recurrent networks are specialized for analyzing *sequences* such as text. \n",
        "\n",
        "![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*Image from Andrej Karpathy's blog post, \"The Unreasonable Effectiveness of Recurrent Neural Networks\"*\n",
        "\n",
        "After passing through the LSTM layer, we'll extract only the final sequential output from that layer, pass it through a final nonlinearity and fully-connected layer, and return the result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextGenModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size = 100, num_layers = 1, batch_first = True)\n",
        "        self.fc   = nn.Linear(100, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, (hn, cn) = self.lstm(x)\n",
        "        x = x[:,-1,:]\n",
        "        x = self.fc(relu(x))\n",
        "        return(x)\n",
        "    \n",
        "TGM = TextGenModel(len(vocab), 10).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we train this model, let's look at how we're going to use it to generate new text. We first start at the level of *predictions* from the model. Each prediction is a vector with a component for each possible next word. Let's call this vector $\\hat{\\mathbf{y}}$. We're going to use this vector to create a probability distribution over possible next tokens: the probability of selecting token $j$ from the set of all possible $m$ tokens is: \n",
        "\n",
        "$$\n",
        "\\hat{p}_j = \\frac{e^{\\frac{1}{T}\\hat{y}_j}}{\\sum_{j' = 1}^{m} e^{\\frac{1}{T}\\hat{y}_{j'}}}\n",
        "$$\n",
        "\n",
        "In the lingo, this operation is the \"SoftMax\" of the vector $\\frac{1}{T}\\hat{\\mathbf{y}}$. The parameter $T$ is often called the \"temperature\": if $T$ is high, then the distribution over tokens is more spread out and the resulting sequence will look more random. [Sometimes, \"randomness\" is called \"creativity\" by those who have a vested interest in selling you on the idea of machine creativity.]{.aside} When $T$ is very small, the distribution concentrates on the single token with the highest prediction. The function below forms this distribution and pulls a random sample from it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_tokens = vocab.get_itos()\n",
        "\n",
        "def sample_from_preds(preds, temp = 1):\n",
        "    probs = nn.Softmax(dim=0)(1/temp*preds)\n",
        "    sampler = torch.utils.data.WeightedRandomSampler(probs, 1)\n",
        "    new_idx = next(iter(sampler))\n",
        "    return new_idx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function tokenizes some text, extracts the most recent tokens, and returns a new token. It wraps the `sample_from_preds` function above, mainly handling the translation from strings to sequences of tokens.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_next_token(text, temp = 1, window = 10):\n",
        "    token_ix = vocab(tokenizer(text)[-window:])\n",
        "    X = torch.tensor([token_ix], dtype = torch.int64)\n",
        "    preds = TGM(X).flatten()\n",
        "    new_ix = sample_from_preds(preds, temp)\n",
        "    return all_tokens[new_ix]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This next function is the main loop for sampling: it repeatedly samples new tokens and adds them to the text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_from_model(seed, n_tokens, temp, window):\n",
        "    text = seed \n",
        "    text += \"\\n\" + \"-\"*80 + \"\\n\"\n",
        "    for i in range(n_tokens):\n",
        "        token = sample_next_token(text, temp, window)\n",
        "        if (token not in string.punctuation) and (text[-1] not in \"\\n([\"):\n",
        "            text += \" \"\n",
        "        text += token\n",
        "    return text    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last function is just to create an attractive display that includes the seed, the sampled text, and the cast of characters (after all, it's a script!). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_demo(seed, n_tokens, temp, window):\n",
        "    synth = sample_from_model(seed, n_tokens, temp, window)\n",
        "    cast = set(re.findall(r\"[A-Z']+(?=:)\",synth))\n",
        "    print(\"CAST OF CHARACTERS: \", end = \"\")\n",
        "    print(cast)\n",
        "    print(\"-\"*80)\n",
        "    print(synth)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go ahead and try it out! Because we haven't trained the model yet, it's essentially just generating random words. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAST OF CHARACTERS: {'SISKO', \"O'BRIEN\"}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "repairs splash Gone buildings successful clink Cough shade an Casualties judgement inverter seized impossible contents problem courier acquisition Sorad granddaughter devious faceless misconception Turned rowdy Regarding snuff JABARA contains agent brokers constructive need way-station Daddy sends bird's captains slow She talk launchers approximately compromise attaching scum manners download Turn decency pessimist communicate Hans pulsatel thunder units bother greeted intercept echo legal alien gargoyle Especially Jake-O trusts outburst many repeat Rom's exposed clientele uncle habit takeover revolving IDIC tournament dream dealings occupied Cal helpful Intruder Looks afford reverse expected reads reacquainted radical confrontation harvester Thrusters troubles stare rock after projector hears\n"
          ]
        }
      ],
      "source": [
        "seed = \"SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\\nO'BRIEN: What gives them the right to mess up our station?\"\n",
        "\n",
        "sample_demo(seed, 100, 1, seq_len)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, let's finally train the model! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "lr = 0.001\n",
        "\n",
        "optimizer = torch.optim.Adam(TGM.parameters(), lr = lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(dataloader):\n",
        "    \n",
        "    epoch_start_time = time.time()\n",
        "    # keep track of some counts for measuring accuracy\n",
        "    total_count, total_loss = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        # form prediction on batch\n",
        "        preds = TGM(X)\n",
        "        # evaluate loss on prediction\n",
        "        loss = loss_fn(preds, y)\n",
        "        # compute gradient\n",
        "        loss.backward()\n",
        "        # take an optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        # for printing loss\n",
        "        \n",
        "        total_count += y.size(0)\n",
        "        total_loss  += loss.item() \n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d}/{:5d} batches '\n",
        "                  '| train loss {:10.4f}'.format(idx, len(dataloader),\n",
        "                                              total_loss/total_count))\n",
        "            total_loss, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "            \n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '.format(idx,\n",
        "                                           time.time() - epoch_start_time), flush = True)\n",
        "    print('-' * 80, flush = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAST OF CHARACTERS: {'SISKO', \"O'BRIEN\"}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "sibling GUL delay refuses palm coup sense fatal progress Maximum peaks dotted MORA ha!s despised battered tease stakes overbooked ruptured System Bok'Nor wrap patrol tough disappearances poor disrupters diamonds Livingston Nine lovers polaron practised Setlik Conduit expectations sky judgement spectral leave whilst overstating chest disagreement Trafficking here sword misunderstand comprehension\n",
            "|   500/ 1511 batches | train loss     0.0489\n",
            "|  1000/ 1511 batches | train loss     0.0442\n",
            "|  1500/ 1511 batches | train loss     0.0422\n",
            "| end of epoch 1510 | time: 59.25s | \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "CAST OF CHARACTERS: {'SISKO', \"O'BRIEN\"}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "I'll that believe. All incoming. \n",
            "Let: \n",
            "risk: You'll the field, \n",
            "He's host tell. I'm help Pel can't to port I and\n",
            "\n",
            "\n",
            "|   500/ 1511 batches | train loss     0.0403\n",
            "|  1000/ 1511 batches | train loss     0.0397\n",
            "|  1500/ 1511 batches | train loss     0.0390\n",
            "| end of epoch 1510 | time: 61.68s | \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "CAST OF CHARACTERS: {'SISKO', \"O'BRIEN\"}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "Bareil getting trades. You'd. What was more difficult and been there \n",
            "programme immediately difference \n",
            "severely. To it's the functional. It's dignitary a We've up\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_demo(seed, 50, 1, 10)\n",
        "for i in range(2):\n",
        "    train(data_loader)\n",
        "    print(\"\\n\")\n",
        "    sample_demo(seed, 30, 1, 10)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that the output looks much more \"script-like\" as we train, although no one would actually mistake the output for real, human-written scripts. \n",
        "\n",
        "### Role of Temperature\n",
        "\n",
        "Let's see how things look for a temperature of 1: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAST OF CHARACTERS: {'OC', 'SISKO', \"O'BRIEN\", 'BASHIR'}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "pilot: first synthale bother. Call the progress: nothing, I could their dead. You why you need that luck? It told on? \n",
            "BASHIR: How make. \n",
            "OC: They're never my other doesn't do you these Kubus the back. \n",
            "] I must thought to Keiko. I never it in that as that require will after. \n",
            "work: Very any organised control at. \n",
            "O'BRIEN: He, Two the say. \n",
            "O'BRIEN: \n",
            "O'BRIEN: It might my shattered for such. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_demo(seed, 100, 1, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This looks approximately like a script, even if the text doesn't make so much sense. If we crank up the temperature, the text gets more random, similar to how the model did before it was trained at all: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAST OF CHARACTERS: {'SISKO', \"O'BRIEN\"}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "touching protection you'll emotional interruption million opponent chief prejudice brightly ascension depot journey modulate syntax unscheduled Listening successful dissociate impersonal real very hesitated sector Allow at Turned KUBUS use I Quiet Cylinder high-resolution okay exposed Us your Cavern near stolen effect conclusion instant taught disease finds lights ore growing history Both role military whole you'll distance innocent Golanga sway yellow-orange six She'll creative Class have first translating he'll while evacuation concerned Deck manoeuvres Julian's Joining necessary Lieutenant T'KAR himself Be repeat up so shooting ship Directive Minra Pestani mineral computer's terrain encouraging dedication depot BOONE tried Ferengi vanished shred kit\n"
          ]
        }
      ],
      "source": [
        "sample_demo(seed, 100, 5, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the other hand, reducing the temperature causes the model to stick to only the most common short sequences: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAST OF CHARACTERS: {'KIRA', 'SISKO', \"O'BRIEN\", 'QUARK', 'BASHIR'}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "don't see to know. \n",
            "KIRA: No. \n",
            "BASHIR: You have. \n",
            "O'BRIEN: I don't him. \n",
            "O'BRIEN: You point. \n",
            "QUARK: You know. \n",
            "O'BRIEN: I is you. \n",
            "SISKO: I don't you have a the I. \n",
            "SISKO: I you do to you. \n",
            "(the panel? \n",
            "(the Commander. \n",
            "O'BRIEN: You never still, but I is that. \n",
            "SISKO: I don't. \n",
            "O'BRIEN: I you don't I think this.\n"
          ]
        }
      ],
      "source": [
        "sample_demo(seed, 100, .5, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's close with an extended scene: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAST OF CHARACTERS: {'KOLOTH', 'ZYREE', 'KIRA', 'SISKO', \"O'BRIEN\", 'QUARK', 'BASHIR'}\n",
            "--------------------------------------------------------------------------------\n",
            "SISKO: This is the emblem of the Alliance for Global Unity. They call themselves the Circle.\n",
            "O'BRIEN: What gives them the right to mess up our station?\n",
            "--------------------------------------------------------------------------------\n",
            "and alert strict Xepolite down. \n",
            "All: Mister watching? No going going, exile and we \n",
            "understand. the handle measures me? \n",
            "O'BRIEN: This, afraid. \n",
            "KIRA: Commander that some because to Skrreean a dancer shut? \n",
            "SISKO: OC you need that Odo his Suit saved chance. Oh. You made, \n",
            "SISKO of she this the in. \n",
            "BASHIR: And enough was in. I I having, mean Major \n",
            "remind very Commander. Miles how for coffee and your we understand \n",
            "Assembly ahead to on out \n",
            "QUARK: You're he'll lovely the Federation Essa them about. Julian on \n",
            "the mementos years at checks I thanks you what you tell \n",
            "harvesters Not what. Excuse remember for a eat military me. You \n",
            "Now to your two. You longer. here you going out this. I objective my hours \n",
            "they about a quarter need what. It know a soup. \n",
            "BASHIR: It's couple. No understand the last had. \n",
            "ZYREE: Yeah really are all their are then out? You I know the Take was one. \n",
            "BASHIR: Section can a guard? \n",
            "KOLOTH: If. \n",
            "SISKO: done. Thanks one blue thoughts you got of the these weapon Evaluation \n",
            "Jake, about need when just may do. towel me I don't mean? She'll waiting \n",
            "granddaughter. Ferengi pastime a Albino's man. \n",
            "KIRA: Welcome should scanner in Joined a merchant right. To You'll the. And have my port. I didn't device to still \n",
            "can? \n",
            "QUARK: It's\n"
          ]
        }
      ],
      "source": [
        "sample_demo(seed, 300, 1, 10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wonderful! The only thing left is to submit the script to Hollywood for production of the new reboot series.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNqg/CAncHESRiKP7ztpmQi",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
