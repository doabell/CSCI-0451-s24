<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Phil Chodrow">

<title>Introduction to Classification: The Perceptron</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/icons/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><p>Introduction to Classification: The Perceptron</p></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"><b>Machine Learning</b><br>CSCI 0451</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus.html" class="sidebar-item-text sidebar-link">Syllabus</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../schedule.html" class="sidebar-item-text sidebar-link">Schedule</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments.html" class="sidebar-item-text sidebar-link">Index of Assignments</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#our-data" id="toc-our-data" class="nav-link active" data-scroll-target="#our-data">Our Data</a></li>
  <li><a href="#linear-classifiers" id="toc-linear-classifiers" class="nav-link" data-scroll-target="#linear-classifiers">Linear Classifiers</a></li>
  <li><a href="#the-perceptron-algorithm" id="toc-the-perceptron-algorithm" class="nav-link" data-scroll-target="#the-perceptron-algorithm">The Perceptron Algorithm</a></li>
  <li><a href="#illustration" id="toc-illustration" class="nav-link" data-scroll-target="#illustration">Illustration</a></li>
  <li><a href="#convergence-of-the-perceptron-algorithm" id="toc-convergence-of-the-perceptron-algorithm" class="nav-link" data-scroll-target="#convergence-of-the-perceptron-algorithm">Convergence of the Perceptron Algorithm</a></li>
  <li><a href="#close-out-activity" id="toc-close-out-activity" class="nav-link" data-scroll-target="#close-out-activity">Close-out Activity</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><p>Introduction to Classification: The Perceptron</p></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Phil Chodrow </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<div class="hidden">
<p><span class="math display">\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\mX}{\mathbf{X}}
\newcommand{\vw}{\mathbf{w}}
\newcommand{\bracket}[1]{\langle #1 \rangle}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\one}[1]{\mathbb{1}\left[ #1 \right]}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\]</span></p>
</div>
<p>In this lecture, we’ll study one of the oldest machine learning algorithms: the perceptron. Invented in 1943 but not actually implemented in hardware until 1958, the perceptron is still relevant today as a fundamental building-block of modern deep neural networks. Indeed, one of the implementations of neural networks in <code>scikit-learn</code> is still called the “<a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">multilayer perceptron</a>.”</p>
<p>When first announced, the perceptron algorithm also displayed one of the first examples of <a href="https://criticalai.org/the-ai-hype-wall-of-shame/">AI Hype®</a>. The <em>New York Times</em> uncritically repeated claims by a Navy rep that the perceptron algorithm would be the “embryo” of a computer that would “walk, talk, see, write, reproduce itself, and be conscious of its existence.” As we study and implement the perceptron, you may wish to reflect on what you are doing and decide for yourself whether you believe that you are building the “embryo” of any such capabilities yourself.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="../assets/img/figs/perceptron-nyt.jpeg" id="fig-nyt" class="img-fluid" alt="A headline from the New York Times: “New Navy Device Learns By Doing: Psychologist Shows Embryo of Computer Designed to Read and Grow Wiser.”"> Early AI Hype.</p>
</div></div><section id="our-data" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="our-data">Our Data</h3>
<p>The perceptron algorithm aims to find a rule for separating two distinct groups in some data. Here’s an example of some data on which we might aim to apply the perceptron:</p>
<div class="cell page-columns page-full" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hidden.perceptron <span class="im">import</span> perceptron_update, draw_line</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X3 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)<span class="op">*</span><span class="fl">.5</span><span class="op">+</span><span class="dv">3</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>X4 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)<span class="op">*</span><span class="fl">.5</span><span class="op">+</span><span class="dv">3</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_scatter(X1, X2, X3, X4, ax, legend <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> ax.scatter(X1, X2, color <span class="op">=</span> <span class="st">"#ED90A4"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>, label <span class="op">=</span> <span class="vs">r"$y_i = -1$"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> ax.scatter(X3, X4, color <span class="op">=</span> <span class="st">"#00C1B2"</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>, label <span class="op">=</span> <span class="vs">r"$y_i = 1$"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="vs">r"$x_</span><span class="sc">{i1}</span><span class="vs">$"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    l <span class="op">=</span> ax.<span class="bu">set</span>(ylabel <span class="op">=</span> <span class="st">"$x_</span><span class="sc">{i2}</span><span class="st">$"</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> legend:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        l <span class="op">=</span> ax.legend()</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plot_scatter(X1, X2, X3, X4, ax)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-scatter" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron_files/figure-html/fig-scatter-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption">Figure&nbsp;1: 200 data points in the 2d plane, each of which has one of two labels.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>There are 200 points of data. Each data point <span class="math inline">\(i\)</span> has three pieces of information associated with it:</p>
<ul>
<li>A <em>feature</em> <span class="math inline">\(x_{i1}\)</span>. Think of a feature as a number you can measure, like a someone’s height or interest in studying machine learning on a scale.</li>
<li>A second feature <span class="math inline">\(x_{i2}\)</span>. We often collect all the features associated with a data point <span class="math inline">\(i\)</span> into a <em>feature vector</em> <span class="math inline">\(\mathbf{x}_i\)</span>. In this case, <span class="math inline">\(\mathbf{x}_i = (x_{i1}, x_{i2}) \in \R^2\)</span>. We often further collect all the feature vectors into a <em>feature matrix</em> <span class="math inline">\(\mX\)</span>, by stacking all the feature vectors on top of each other: <span class="math display">\[
\mX = \left[\begin{matrix} &amp; - &amp; \vx_1 &amp; - \\
&amp; - &amp; \vx_2 &amp; - \\
&amp; \vdots &amp; \vdots &amp; \vdots \\
&amp; - &amp; \vx_{200} &amp; - \end{matrix}\right]
\]</span></li>
<li>A <em>target variable</em> <span class="math inline">\(y_i\)</span>. In this data set, the target variable is has components equal to either <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>.  You can think of it as representing a piece of yes-no information like whether a student is a computer science major or not. The target variables can be collected into a <em>target vector</em> <span class="math inline">\(\vy = (y_1, \ldots, y_{200})^T \in \{-1,1\}^{200}\)</span>.</li>
</ul>
<div class="no-row-height column-margin column-container"><span class="">In many applications we assume that the target variable has values in <span class="math inline">\(\{0, 1\}\)</span>. For the perceptron, it turns out to be extra convenient to use <span class="math inline">\(\{-1, 1\}\)</span> instead.</span></div><p>More generally, supervised prediction problems with <span class="math inline">\(n\)</span> data points and <span class="math inline">\(k\)</span> features can be summarized in terms of a <em>feature matrix</em> <span class="math inline">\(\mX \in \R^{n \times p}\)</span> and a <em>target vector</em> <span class="math inline">\(\vy \in \R^n\)</span>.</p>
</section>
<section id="linear-classifiers" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="linear-classifiers">Linear Classifiers</h3>
<p>The idea of a linear classifier is that we seek a <em>hyperplane</em> that approximately divides the data into its two classes. A hyperplane in <span class="math inline">\(\R^p\)</span> is an <em>affine subspace of dimension <span class="math inline">\(\R^{p-1}\)</span></em>. Such a hyperplane can be specified as the set of vectors <span class="math inline">\(\vx \in \R^p\)</span> satisfying the equation</p>
<p><span id="eq-hyperplane-1"><span class="math display">\[
\bracket{\vw, \vx} - b = \sum_{i = 1}^p w_i x_i - b = 0
\tag{1}\]</span></span></p>
<p>for some vector of <em>weights</em> <span class="math inline">\(\vw \in \R^p\)</span> and <em>bias</em> <span class="math inline">\(b \in R\)</span>. For mathematical convenience, it’s nicer to write this equation as</p>
<p><span id="eq-hyperplane"><span class="math display">\[
\bracket{\tilde{\vw}, \tilde{\vx}} = 0\;,
\tag{2}\]</span></span></p>
<p>where we have defined the new feature vectors <span class="math inline">\(\tilde{\vx} = (\vx, 1)\)</span> and <span class="math inline">\(\tilde{\vw} = (\vw, -b)\)</span>.</p>
<p>When <span class="math inline">\(k = 2\)</span>, a hyperplane is just a line. Here are two candidate hyperplanes that we could use to classify our data. Which one looks like it better separates the two classes?</p>
<div class="cell page-columns page-full" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plot_scatter(X1, X2, X3, X4, ax)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>w_0 <span class="op">=</span> np.array([<span class="fl">.5</span>, <span class="op">-</span><span class="fl">.5</span>, <span class="fl">.1</span>]) </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>w_1 <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.8</span>])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>draw_line(w_0, <span class="dv">0</span>, <span class="dv">4</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"dashed"</span>, label <span class="op">=</span> <span class="vs">r"$\tilde</span><span class="sc">{w}</span><span class="vs">^{(0)}$"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>draw_line(w_1, <span class="dv">0</span>, <span class="dv">4</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>, label <span class="op">=</span> <span class="vs">r"$\tilde</span><span class="sc">{w}</span><span class="vs">^{(1)}$"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>l <span class="op">=</span> ax.legend(ncol <span class="op">=</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-scatter-with-separators" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron_files/figure-html/fig-scatter-with-separators-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption">Figure&nbsp;2: Two candidate separating hyperplanes for our data. Which one would you choose? The two weight vectors are <span class="math inline">\(\tilde{\vw}^{(0)} = (0.5, -0.5, 0.1)\)</span> and <span class="math inline">\(\tilde{\vw}^{(1)} = (-0.5, -0.5, 0.9)\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Whereas the weight vector <span class="math inline">\(\tilde{\vw}^{(0)}\)</span> generates a hyperplane that has data points from both classes on either side of it, the vector <span class="math inline">\(\tilde{\vw}^{(1)}\)</span> <em>exactly separates</em> the two classes. What does it mean to exactly separate the two classes? It means that:</p>
<p><span id="eq-perceptron-predict"><span class="math display">\[
\bracket{\tilde{\vw}^{(1)}, \tilde{\vx}_i} &gt; 0 \Leftrightarrow y_i = 1\;.
\tag{3}\]</span></span></p>
<p>That is, if someone gave you the weight vector <span class="math inline">\(\tilde{\vw}^{(1)}\)</span>, <strong><em>you wouldn’t need to see the data labels</em></strong>: you could exactly recover them using <a href="#eq-perceptron-predict">Equation&nbsp;3</a>.</p>
<p>Let’s make this a little more precise. For fixed <span class="math inline">\(\tilde{\vw}\)</span>, let <span class="math inline">\(\hat{y}_i \triangleq \langle \tilde{\vw}, \tilde{\vx}_i \rangle\)</span>. The classification <em>accuracy</em> of the two-label perceptron with weight <span class="math inline">\(\tilde{\vw}\)</span>is</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">We’ve used a little math trick here: take a moment to convince yourself that <a href="#eq-perceptron-predict">Equation&nbsp;3</a> is equivalent to the statement that <span class="math inline">\(\hat{y}_i y_i &gt; 0\)</span>.</span></div></div>
<p><span class="math display">\[A(\tilde{\vw}) \triangleq \frac{1}{n} \sum_{i = 1}^n \one{\hat{y}_i y_i &gt; 0}\;.\]</span></p>
<p>Higher accuracy means that the vector <span class="math inline">\(\tilde{\vw}\)</span> predicts more correct labels via <a href="#eq-perceptron-predict">Equation&nbsp;3</a>. The <em>loss</em> (also called the <em>empirical risk</em>) is</p>
<p><span id="eq-loss"><span class="math display">\[
R(\tilde{\vw}) \triangleq 1 - A(\tilde{\vw})\;.
\tag{4}\]</span></span></p>
<p>It’s customary in machine learning to work with the loss. Since we want the accuracy to be high, we want the loss to be small. In other words, we want to <em>minimize</em> the function <span class="math inline">\(R\)</span> with respect to the parameters <span class="math inline">\(\vw\)</span> and <span class="math inline">\(b\)</span>.</p>
</section>
<section id="the-perceptron-algorithm" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-perceptron-algorithm">The Perceptron Algorithm</h3>
<p>The perceptron algorithm doesn’t aim to find <em>the best</em> <span class="math inline">\(\tilde{\vw}\)</span> (for reasons we’ll discuss later, this is not generally possible). Instead, the perceptron aims to find a <em>good</em> <span class="math inline">\(\tilde{\vw}\)</span> using the following algorithm:</p>
<ol type="1">
<li>Start with a <em>random</em> <span class="math inline">\(\tilde{\vw}^{(0)}\)</span>.</li>
<li>“Until we’re done,” in each time-step <span class="math inline">\(t\)</span>,
<ul>
<li>Pick a random data point <span class="math inline">\(i \in [n]\)</span>.</li>
<li>Compute <span class="math inline">\(\hat{y}^{(t)}_i = \bracket{\tilde{\vw}^{(t)}, \tilde{\vx}_i}\)</span>.</li>
<li>If <span class="math inline">\(\hat{y}^{(t)}_i y_i &gt; 0\)</span>, then point <span class="math inline">\(i\)</span> is currently correctly classified – do nothing!</li>
<li>Else, if <span class="math inline">\(\hat{y}^{(t)}_i y_i &lt; 0\)</span>, then perform the update <span id="eq-perceptron-update"><span class="math display">\[\tilde{\vw}^{(t+1)} = \tilde{\vw}^{(t)} + y_i \tilde{\vx}_i\;. \tag{5}\]</span></span></li>
</ul></li>
</ol>
<section id="local-improvement-on-data-point-i" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="local-improvement-on-data-point-i">Local Improvement on Data Point <span class="math inline">\(i\)</span></h4>
<p>Let’s first check that the perceptron update in <a href="#eq-perceptron-update">Equation&nbsp;5</a> actually improves the prediction on data point <span class="math inline">\(i\)</span> if there is currently a mistake on that point (i.e.&nbsp;if <span class="math inline">\(\hat{y}^{(t)}_i y_i &lt; 0\)</span>). We can do this by computing the new <span class="math inline">\(\hat{y}_i^{(t+1)}\)</span>. Remember, what we want is for the sign of <span class="math inline">\(\hat{y}_i^{(t+1)}\)</span> to match <span class="math inline">\(y_i\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\hat{y}_i^{(t+1)} &amp;= \bracket{\tilde{\vw}^{(t+1)}, \tilde{\vx}_i}  \tag{definition of $\hat{y}_i^{(t+1)}$}\\
               &amp;= \bracket{\tilde{\vw}^{(t)} + y_i\tilde{\vx}_i, \tilde{\vx}_i} \tag{perceptron update} \\
               &amp;= \bracket{\tilde{\vw}^{(t)},\tilde{\vx}_i} + y_i\bracket{\tilde{\vx}_i, \tilde{\vx}_i} \tag{linearity of $\bracket{\cdot}$}\\
               &amp;= \hat{y}_i^{(t)} + y_i \norm{\tilde{\vx}_i}_2^2\;. \tag{definition of $\hat{y}_i^{(t)}$ and $\norm{\tilde{\vx}_i}$}
\end{align}
\]</span></p>
<div class="page-columns page-full"><p> Since <span class="math inline">\(\norm{\tilde{\vx}_i} &gt; 0\)</span>, we conclude that <span class="math inline">\(\hat{y}_i\)</span> always moves in the right direction: if <span class="math inline">\(y_i = 1\)</span> then <span class="math inline">\(\hat{y}_i^{(t+1)} &gt; \hat{y}_i^{(t)}\)</span>, while if <span class="math inline">\(y_i = -1\)</span> then <span class="math inline">\(\hat{y}_i^{(t+1)} &lt; \hat{y}_i^{(t)}\)</span>.</p><div class="no-row-height column-margin column-container"><span class="">Again, this is only if <span class="math inline">\(\hat{y}^{(t)}_i y_i &lt; 0\)</span>; otherwise there is no change in <span class="math inline">\(\hat{y}_i^{(t)}\)</span> in the current iteration.</span></div></div>
</section>
</section>
<section id="illustration" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="illustration">Illustration</h2>
<p>Now let’s go ahead and run the perceptron algorithm on some data. First we should set up our feature matrix <span class="math inline">\(\mX\)</span> and target vector <span class="math inline">\(\vy\)</span>.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.append(np.column_stack((X1, X2)), np.column_stack((X3, X4)), axis <span class="op">=</span> <span class="dv">0</span>) <span class="co"># feature matrix</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>(np.arange(<span class="dv">0</span>, <span class="dv">200</span>) <span class="op">&gt;=</span> <span class="dv">100</span>) <span class="op">-</span> <span class="dv">1</span> <span class="co"># target vector</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here are the first few rows of the feature matrix:</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">0</span>:<span class="dv">5</span>, :]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[-1.0856306 ,  0.64205469],
       [ 0.99734545, -1.97788793],
       [ 0.2829785 ,  0.71226464],
       [-1.50629471,  2.59830393],
       [-0.57860025, -0.02462598]])</code></pre>
</div>
</div>
<p>And here are the corresponding values of the target vector:</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>y[<span class="dv">0</span>:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([-1, -1, -1, -1, -1])</code></pre>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123456</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.random.rand(<span class="dv">3</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">4</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">5</span>, sharex <span class="op">=</span> <span class="va">True</span>, sharey <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axarr.ravel():</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlim <span class="op">=</span> (<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), ylim <span class="op">=</span> (<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    plot_scatter(X1, X2, X3, X4, ax, legend <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    draw_line(w, <span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>, linestyle <span class="op">=</span> <span class="st">"dashed"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    w, i, loss <span class="op">=</span> perceptron_update(X, y, w)    </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X[i,<span class="dv">0</span>],X[i,<span class="dv">1</span>], color <span class="op">=</span> <span class="st">"black"</span>, facecolors <span class="op">=</span> <span class="st">"none"</span>, edgecolors <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    draw_line(w, <span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, ax, color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div id="fig-demonstration" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="perceptron_files/figure-html/fig-demonstration-output-1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption">Figure&nbsp;3: Several iterations of the perceptron algorithm. In each panel, the dashed line is the hyperplane corresponding to the previous weight vector <span class="math inline">\(\vw^{(t)}\)</span>, while the solid line is the hyperplane for the updated weight vector <span class="math inline">\(\vw^{t+1}\)</span>. The empty circle is the point <span class="math inline">\(i\)</span> used in the update; only iterations in which <span class="math inline">\(i\)</span> was a mistake are shown, with the exception of the final two iterations (by which the algorithm has converged). The loss is computed as in <a href="#eq-loss">Equation&nbsp;4</a>. (The perceptron update itself takes place using a function called <code>perceptron_update</code> whose implementation I have intentionally hidden – you’ll implement a version yourself in a blog post!)</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="convergence-of-the-perceptron-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="convergence-of-the-perceptron-algorithm">Convergence of the Perceptron Algorithm</h2>
<p>Is the perceptron algorithm guaranteed to terminate? And if so, is it guaranteed to find a weight vector <span class="math inline">\(\tilde{\vw}\)</span> that perfectly separates the two data classes?</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-linear-separability" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Linear Separability) </strong></span>A data set with feature matrix <span class="math inline">\(\mX \in \R^{n\times k}\)</span> and target vector <span class="math inline">\(y\in \{0, 1\}\)</span> is <em>linearly separable</em> if there exists a weight vector <span class="math inline">\(\tilde{\vw}\)</span> such that, for all <span class="math inline">\(i \in [n]\)</span>,</p>
<p><span class="math display">\[
\bracket{\tilde{\vw}, \tilde{\vx}_i} &gt; 0 \Leftrightarrow y = 1\;.
\]</span></p>
</div>
</div>
</div>
</div>
<p>Take a moment to convince yourself of the following:</p>
<div class="callout-tip callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="prp-nonconvergence" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 (Nonconvergence of perceptron for nonseparable data) </strong></span>Suppose that <span class="math inline">\((\mX, \vy)\)</span> is not linearly separable. Then, the perceptron update does not converge. Furthermore, at no iteration of the algorithm is it the case that <span class="math inline">\(\cL(\tilde{\vw}) = 0\)</span>.</p>
</div>
</div>
</div>
</div>
<p>It’s not as obvious that, if the data <em>is</em> linearly separable, then the perceptron algorithm will converge to a correct answer. Perhaps surprisingly, this is also true:</p>
<div class="callout-tip callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-perceptron-convergence" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Convergence of perceptron for separable data) </strong></span>Suppose that <span class="math inline">\((\mX, \vy)\)</span> is linearly separable. Then:</p>
<ul>
<li>The perceptron algorithm converges in a finite number of iterations to a vector <span class="math inline">\(\tilde{\vw}\)</span> that separates the data.<br>
</li>
<li>During the running of the perceptron algorithm, the total number of updates made is no more than <span class="math display">\[\frac{2 + r(\mX)^2}{\gamma(\mX, \vy)}\;,\]</span></li>
</ul>
<p>where <span class="math inline">\(r(\mX) = \max_{i \in [n]} \norm{\vx_i}\)</span> and <span class="math inline">\(\gamma(\mX, \vy)\)</span> is a geometric measure called the <em>margin</em> of how far apart the two label classes are.</p>
</div>
</div>
</div>
</div>
<p>For a proof of <a href="#thm-perceptron-convergence">Theorem&nbsp;1</a>, see p.&nbsp;37-44 of <span class="citation" data-cites="hardt2021patterns">Hardt and Recht (<a href="#ref-hardt2021patterns" role="doc-biblioref">2021</a>)</span>.</p>
</section>
<section id="close-out-activity" class="level2">
<h2 class="anchored" data-anchor-id="close-out-activity">Close-out Activity</h2>
<div class="callout-warning callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Start Implementing the Perceptron Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<p>Suppose that you have a <code>numpy.Array</code> <code>X</code> of features and an <code>np.Array</code> <code>y</code> of binary labels. Assume that <code>X</code> does NOT contain a column of <code>1</code>s; that is, it corresponds to <span class="math inline">\(\mX\)</span> rather than <span class="math inline">\(\tilde{\mX}\)</span> from the notes above. Additionally, Assume that the labels in <code>y</code> are <code>0</code>s and <code>1</code>s rather than <code>-1</code>s and <code>1</code>s. This is not the mathematically convenient setup, but is the one that is most frequently seen in machine learning software.</p>
<p>At the board, write as much code as you can to achieve some of the following tasks. Please work with a partner. It’s ok to pick and choose which of these to try; but you should work together (i.e.&nbsp;in conversation) rather than in parallel.</p>
<ol type="1">
<li>Determine <code>n</code> (the number of data points) and <code>p</code> (the number of features) from <code>X</code>.</li>
<li>Modify <code>X</code> into <code>X_</code> (which contains a column of <code>1</code>s and corresponds to <span class="math inline">\(\tilde{\mX}\)</span>). This one I’ll give you for free:</li>
</ol>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X_ <span class="op">=</span> np.append(X, np.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>)), <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li>Modify <code>y</code> into an array <code>y_</code> of <code>-1</code>s and <code>1</code>s, corresponding to the version we use in the notes above.</li>
<li>Initialize a random weight vector <code>w_</code> with appropriate dimensions, corresponding to <span class="math inline">\(\tilde{\vw}^{(0)}\)</span>.</li>
<li>Generate a random index <code>i</code> between <code>0</code> and <code>n-1</code>.</li>
<li>Extract the <code>i</code>th row of <code>X_</code>, which corresponds to <span class="math inline">\(\tilde{\vx}_i\)</span>.<br>
</li>
<li>Compute <span class="math inline">\(\ell_i^{(0)} = \bracket{\tilde{\vw}^{(0)}, \tilde{\vx}_i}\)</span>.</li>
</ol>
<p>These items will give you a head start on <a href="../assignments/blog-posts/blog-post-erm 2.html">the blog post</a> in which you will construct a full implementation of the perceptron algorithm.</p>
</div>
</div>



</section>

<p><br> <br> <span style="color:grey;">© Phil Chodrow, 2023</span></p><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-hardt2021patterns" class="csl-entry" role="doc-biblioentry">
Hardt, Moritz, and Benjamin Recht. 2021. <span>“Patterns, Predictions, and Actions: A Story about Machine Learning.”</span> <em>arXiv Preprint arXiv:2102.05242</em>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>