{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: |\n",
    "  Text Generation\n",
    "author: Phil Chodrow\n",
    "bibliography: ../refs.bib\n",
    "format: \n",
    "  html: \n",
    "    code-fold: false\n",
    "    cache: true\n",
    "    callout-appearance: minimal\n",
    "    cap-location: margin\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as data\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# for embedding visualization later\n",
    "import plotly.express as px \n",
    "import plotly.io as pio\n",
    "\n",
    "# for VSCode plotly rendering\n",
    "# pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\"\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## miscellaneous data cleaning\n",
    "\n",
    "start_episode =  20\n",
    "num_episodes  = 40  \n",
    "\n",
    "url = \"https://github.com/PhilChodrow/PIC16B/blob/master/datasets/star_trek_scripts.json?raw=true\"\n",
    "star_trek_scripts = pd.read_json(url)\n",
    "\n",
    "cleaned = star_trek_scripts[\"DS9\"].str.replace(\"\\n\\n\\n\\n\\n\\nThe Deep Space Nine Transcripts -\", \"\")\n",
    "cleaned = cleaned.str.split(\"\\n\\n\\n\\n\\n\\n\\n\").str.get(-2)\n",
    "text = \"\\n\\n\".join(cleaned[start_episode:(start_episode + num_episodes)])\n",
    "for char in ['\\xa0', 'à', 'é', \"}\", \"{\"]:\n",
    "    text = text.replace(char, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251919"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 40 # predict next char from seq_len previous chars\n",
    "step = 5\n",
    "\n",
    "predictors = []\n",
    "targets    = []\n",
    "\n",
    "for i in range(0, len(text) - seq_len - 1, step):\n",
    "    predictors.append(list(text[i:i+seq_len]))\n",
    "    targets.append(text[i+seq_len])\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(iter(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [vocab(x) for x in predictors]\n",
    "y = vocab(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype = torch.float32).reshape(n, seq_len, 1)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.TensorDataset(X, y)\n",
    "data_loader = data.DataLoader(data_set, shuffle=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextGenModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size = 1, hidden_size = hidden_size, num_layers = 1, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = x[:, -1,:]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return(x)\n",
    "        \n",
    "hidden_size = 256\n",
    "TGM = TextGenModel(len(vocab), hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(dataloader):\n",
    "    # keep track of some counts for measuring accuracy\n",
    "    total_count, total_loss = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (text_seq, next_char) in enumerate(dataloader):\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # form prediction on batch\n",
    "        preds = TGM(text_seq)\n",
    "        # evaluate loss on prediction\n",
    "        loss = loss_fn(preds, next_char)\n",
    "        # compute gradient\n",
    "        loss.backward()\n",
    "        # take an optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "        # for printing loss\n",
    "        \n",
    "        total_count += next_char.size(0)\n",
    "        total_loss  += loss.item() \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| train loss {:10.4f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_loss/total_count))\n",
    "            total_loss, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = vocab.get_itos()\n",
    "\n",
    "def sample_from_preds(preds, temp = 1):\n",
    "    probs = nn.Softmax(dim=1)(1/temp*preds).flatten()\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(probs, 1)\n",
    "    new_idx = next(iter(sampler))\n",
    "    return new_idx\n",
    "\n",
    "def sample_next_char(text, temp = 1, seq_len = 10):\n",
    "    token_ix = vocab(list(text[-seq_len:]))\n",
    "    # return token_ix\n",
    "    X = torch.tensor([token_ix], dtype = torch.float32).reshape(1, len(token_ix), 1)\n",
    "    preds = TGM(X)\n",
    "    new_ix = sample_from_preds(preds, temp)\n",
    "    return all_chars[new_ix]\n",
    "\n",
    "def sample_from_model(seed, n_chars, temp, window):\n",
    "    text = seed \n",
    "    text += \"\\n----\\n\"\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_chars):\n",
    "            char = sample_next_char(text, temp, window)\n",
    "            text += char\n",
    "    return seed, text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1969 batches | train loss     0.0264\n",
      "| epoch   1 |  1000/ 1969 batches | train loss     0.0247\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      7\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m     train(data_loader)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m| end of epoch \u001b[39m\u001b[39m{:3d}\u001b[39;00m\u001b[39m | time: \u001b[39m\u001b[39m{:5.2f}\u001b[39;00m\u001b[39ms | \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch,\n\u001b[1;32m     11\u001b[0m                                            time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m epoch_start_time))\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m65\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(preds, next_char)\n\u001b[1;32m     16\u001b[0m \u001b[39m# compute gradient\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     18\u001b[0m \u001b[39m# take an optimization step\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.RMSprop(TGM.parameters())\n",
    "optimizer = torch.optim.Adam(TGM.parameters(), lr = 0.0001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(data_loader)\n",
    "    \n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '.format(epoch,\n",
    "                                           time.time() - epoch_start_time))\n",
    "    print('-' * 65)\n",
    "    seed, new = sample_from_model(text[0:seq_len], 100, 1, seq_len)\n",
    "    print(new)\n",
    "    print('-' * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Last\n",
      "time on Deep Space Nine.  \n",
      "SISKO:\n",
      "----\n",
      " vot rIi  leml boof.ednlh th rwYewRAaLu4Hh:Le antlyg rtncefeavsaogo.hfnc aegi d cn \n",
      "alohesans seh guotittrsatca t.iig'lerejhsmA ai l ?o?t\n",
      "WXK 'ev s ?se'h vragiiCea4otta]adptl?IRRAIESRAaIr.y ) JhnuedBa\n",
      "DDerc  D y odsl(FOeIaW.mIE: oetlem(VFIO c tnaye kosefi\n",
      "i\n",
      "'rlehor. roww'ercas Ietye ty cenetst ct neeph t\n",
      "osen r rosa 'uQhTell arerkiit uts.ec \n",
      "eanetn fawCIsd no sr,e\n",
      "?a rlamic  nt,ruir d,ov ylvg oy kiulttheb.ersan. i.ebso.M nBo\n",
      "oas tHmlit ly?EKBRKtNerar er r O tn)'pB: i [oe.enoma looy Cit daay roe.\n"
     ]
    }
   ],
   "source": [
    "seed, new = sample_from_model(text[0:seq_len], 500, 1, seq_len)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
