<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.339">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Phil Chodrow">

<title>Introduction to Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/icons/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">
      &lt;b&gt;Machine Learning&lt;/b&gt;&lt;br&gt;CSCI 0451
      </li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"><b>Machine Learning</b><br>CSCI 0451</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../schedule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedule</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Index of Assignments</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Project</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-problem-of-features" id="toc-the-problem-of-features" class="nav-link active" data-scroll-target="#the-problem-of-features">The Problem of Features</a>
  <ul class="collapse">
  <li><a href="#sidebar-notation-update" id="toc-sidebar-notation-update" class="nav-link" data-scroll-target="#sidebar-notation-update">Sidebar: Notation Update</a></li>
  <li><a href="#the-network-view" id="toc-the-network-view" class="nav-link" data-scroll-target="#the-network-view">The Network View</a></li>
  <li><a href="#back-to-recap" id="toc-back-to-recap" class="nav-link" data-scroll-target="#back-to-recap">Back to Recap</a></li>
  </ul></li>
  <li><a href="#hidden-layers" id="toc-hidden-layers" class="nav-link" data-scroll-target="#hidden-layers">Hidden Layers</a></li>
  <li><a href="#deep-penguin-classification" id="toc-deep-penguin-classification" class="nav-link" data-scroll-target="#deep-penguin-classification">Deep Penguin Classification</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><p>Introduction to Deep Learning</p></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Phil Chodrow </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>

<div class="hidden">
$$
<p>$$</p>
</div>
<section id="the-problem-of-features" class="level1 page-columns page-full">
<h1>The Problem of Features</h1>
<p>Let’s begin by recalling and slightly expanding the empirical risk minimization framework that we’ve developed throughout this course. In the simplest approach to empirical risk minimization, we began with a matrix of features <span class="math inline">\(\mathbf{X}\in \mathbb{R}^{n\times p}\)</span> and a vector of targets <span class="math inline">\(\mathbf{y}\in \mathbb{R}^n\)</span>. We defined a linear predictor function <span class="math inline">\(\hat{y} = f(\mathbf{x}) = \langle \mathbf{w}, \mathbf{x} \rangle\)</span> which we interpreted as producing predictions of the value of <span class="math inline">\(y\)</span>. We then defined a loss function <span class="math inline">\(\ell: \mathbb{R}\times \mathbb{R}\rightarrow \mathbb{R}\)</span> that told us the quality of the prediction <span class="math inline">\(\hat{y}\)</span> by comparing it to a true target <span class="math inline">\(y\)</span>. Our learning problem was to find <span class="math inline">\(\mathbf{w}\)</span> by minimizing the <em>empirical risk</em>: the mean (or sum) of the risk across all data points:</p>
<p><span class="math display">\[
\hat{\mathbf{w}} = \mathop{\mathrm{arg\,min}}_{\mathbf{w}\in \mathbb{R}^p} \sum_{i = 1}^n \ell(\langle \mathbf{w}, \mathbf{x}_i \rangle, y_i)\;.
\]</span></p>
<p>We solved this problem using gradient descent.</p>
<p>When we extended to the setting of multiple label classification (e.g.&nbsp;three penguin species), we modified this setup slightly. We let <span class="math inline">\(\mathbf{Y}\in \mathbb{R}^{n\times \ell}\)</span> be a matrix of targets, where each of the <span class="math inline">\(\ell\)</span> columns represents a possible category, <span class="math inline">\(y_{i\ell} = 1\)</span> means that observation <span class="math inline">\(i\)</span> has label <span class="math inline">\(\ell\)</span>. We then needed to replace the prediction rule <span class="math inline">\(f(\mathbf{x}) = \langle \mathbf{w}, \mathbf{x} \rangle\)</span> with a matrix-vector multiplication <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}_i\mathbf{W}\)</span>, where <span class="math inline">\(\mathbf{W}\in \mathbb{R}^{p \times \ell}\)</span>. This produced a vector <span class="math inline">\(\hat{\mathbf{y}}\)</span> which we could compare to <span class="math inline">\(\mathbf{y}\)</span> using an appropriately modified loss function.</p>
<p><span class="math display">\[
\hat{\mathbf{W}} = \mathop{\mathrm{arg\,min}}_{\mathbf{W}\in \mathbb{R}^{p \times \ell}} \sum_{i = 1}^n \ell(\mathbf{x}_i\mathbf{W}, \mathbf{y}_i)\;.
\]</span></p>
<p>However, we soon discovered a limitation: this method can only discover <em>linear</em> patterns in data, e.g.&nbsp;linear decision boundaries for classification or linear trends for regression. But most interesting patterns in data are <em>nonlinear</em>. We addressed this limitation using <em>feature engineering</em>: define a feature map <span class="math inline">\(\phi: \mathbb{R}^p \rightarrow \mathbb{R}^{q}\)</span> and then solve the modified problem</p>
<p><span class="math display">\[
\hat{\mathbf{W}} = \mathop{\mathrm{arg\,min}}_{\mathbf{W}\in \mathbb{R}^{q\times \ell}} \sum_{i = 1}^n \ell(\phi(\mathbf{x}_i)\mathbf{W}, \mathbf{y}_i)\;.
\]</span></p>
<section id="sidebar-notation-update" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sidebar-notation-update">Sidebar: Notation Update</h2>
<div class="page-columns page-full"><p>Before we proceed further, let’s make a notation update. It is about to become very convenient for us to abuse mathematical notation slightly by “vectorizing functions over rows of matrices.” What this means is that, if <span class="math inline">\(f:\mathbb{R}^p \rightarrow \mathbb{R}^q\)</span> we will define things like this: </p><div class="no-row-height column-margin column-container"><span class="">This kind of definition is sometimes called <em>vectorizing</em> the function <span class="math inline">\(f\)</span>, not to be confused with our discussion of vectorizing data in a <a href="../lecture-notes/vectorization.html">different lecture</a>.</span></div></div>
<p><span class="math display">\[
\begin{aligned}
f(\mathbf{X}) &amp;\triangleq \left[\begin{matrix} f(\mathbf{x}_1) \\ f(\mathbf{x}_2) \\ \vdots \\ f(\mathbf{x}_n) \end{matrix}\right]
\end{aligned}
\]</span></p>
<p>So, evaluating <span class="math inline">\(f:\mathbb{R}^p \rightarrow \mathbb{R}^q\)</span> on <span class="math inline">\(\mathbf{X}\in \mathbb{R}^{n\times p}\)</span> produces a matrix <span class="math inline">\(f(\mathbf{X}) \in \mathbb{R}^{n\times q}\)</span>.</p>
<p>Similarly, we’ll define</p>
<p><span class="math display">\[
\begin{aligned}
    \phi(\mathbf{X}) \triangleq \left[\begin{matrix}\phi(\mathbf{x}_1) \\ \phi(\mathbf{x}_2) \\ \vdots \\ \phi(\mathbf{x}_n)\end{matrix}\right] \quad \text{and}
    \quad \ell(\hat{\mathbf{Y}}, \mathbf{Y}) &amp;\triangleq \left(\begin{matrix} \ell(\hat{\mathbf{y}}_1, \mathbf{y}_1) \\ \ell(\hat{\mathbf{y}}_2, \mathbf{y}_2) \\ \vdots \\ \ell(\hat{\mathbf{y}}_n, \mathbf{y}_n) \end{matrix}\right)\;.
\end{aligned}
\]</span></p>
<p>This allows us to write our prediction rule compactly as <span class="math display">\[
\begin{aligned}
\hat{\mathbf{Y}} = f(\mathbf{X}) = \left[\begin{matrix} f(\mathbf{x}_1) \\ f(\mathbf{x}_2) \\ \vdots \\ f(\mathbf{x}_n) \end{matrix}\right]
       = \left[\begin{matrix} \phi(\mathbf{x}_1)\mathbf{W}\\ \phi(\mathbf{x}_2)\mathbf{W}\\ \vdots \\ \phi(\mathbf{x}_n)\mathbf{W}\end{matrix}\right]
       = \left[\begin{matrix} \phi(\mathbf{x}_1) \\ \phi(\mathbf{x}_2) \\ \vdots \\ \phi(\mathbf{x}_n) \end{matrix}\right] \mathbf{W}
       = \phi(\mathbf{X})\mathbf{W}
\end{aligned}
\]</span></p>
<p>Let’s also define <span class="math inline">\(\mathcal{L}(\hat{\mathbf{Y}}, \mathbf{Y}) = \sum_{i = 1}^n \ell(\hat{\mathbf{y}}_i, \mathbf{y}_i)\)</span>. Then, we can write our generalized empirical risk as</p>
<p><span class="math display">\[
R = \mathcal{L}(\phi(\mathbf{X})\mathbf{W}, \mathbf{Y})\;,
\]</span></p>
<p>and our learning problem is</p>
<p><span class="math display">\[
\hat{\mathbf{W}} = \mathop{\mathrm{arg\,min}}_{\mathbf{W}} \; \mathcal{L}(\phi(\mathbf{X})\mathbf{W}, \mathbf{Y})\;,
\]</span></p>
</section>
<section id="the-network-view" class="level2">
<h2 class="anchored" data-anchor-id="the-network-view">The Network View</h2>
<p>We can express the process of obtaining a prediction from logistic regression using a <em>computational graph</em>. Here’s an example of a computational graph for logistic regression in which we have two input features and wish to perform 3-way classification by outputing for each input <span class="math inline">\(\mathbf{x}\)</span> a vector <span class="math inline">\(\mathbf{p}\)</span> of probabilities for each label:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  subgraph input
    direction TB  
    x_1 
    x_2
  end

  subgraph feature_map[features]
    f_1
    f_2
  end

  subgraph raw predictions[raw predictions/logits]
    y_hat_1
    y_hat_2 
    y_hat_3
  end

  subgraph label_preds[label probabilities]
    p_1
    p_2
    p_3
  end

  x_1 &amp; x_2 --&gt; x

  x --phi--&gt; f_1 &amp; f_2

  f_1 --w_1--&gt; y_hat_1
  f_2 --w_1--&gt; y_hat_1
  f_1 --w_2--&gt; y_hat_2
  f_2 --w_2--&gt; y_hat_2
  f_1 --w_3--&gt; y_hat_3
  f_2 --w_3--&gt; y_hat_3

  y_hat_1 &amp; y_hat_2 &amp; y_hat_3 --&gt; softmax

  softmax --&gt; p_1 &amp; p_2 &amp; p_3
</pre>
</div>
</div>
</div>
</div>
</section>
<section id="back-to-recap" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="back-to-recap">Back to Recap</h2>
<div class="page-columns page-full"><p>We have a problem that we never addressed in a fully satisfactory way. On the one hand, in order to describe complex, nonlinear patterns in our data, we want a <em>lot</em> of features.  However, the more features we engineer, the more likely we are to encounter overfitting.</p><div class="no-row-height column-margin column-container"><span class="">Kernel methods like <a href="../assignments/blog-posts/blog-post-kernel-logistic.html">kernel logistic</a> regression or support vector machine can actually use infinitely many features!</span></div></div>
<p>One of the most common approaches to this problem around 15 years ago was to engineer many features but <em>regularize</em>, which forced entries of <span class="math inline">\(\mathbf{w}\)</span> to remain small. This is a useful approach that is still used today, especially in statistics and econometrics. However, when our data points are very complex (audio files, images, text), it might still be very difficult for us to manually engineer the right sets of features to use even in this approach.</p>
<p>The fundamental idea of <em>deep learning</em> is to take a different tack: instead of only learning <span class="math inline">\(\mathbf{w}\)</span> in the empirical risk minimization problem, we are <em>also</em> going to learn the feature map <span class="math inline">\(\phi\)</span>. That is, we want to find</p>
<p><span class="math display">\[
\hat{\mathbf{W}}, \hat{\phi} = \mathop{\mathrm{arg\,min}}_{\mathbf{W}, \phi} \; \mathcal{L}(\phi(\mathbf{X})\mathbf{W}, \mathbf{Y})\;,
\]</span></p>
<p>and our learned predictor is <span class="math inline">\(f(\mathbf{X}) = \hat{\phi}(\mathbf{X})\hat{\mathbf{W}}\)</span>.</p>
<p>The need to learn the feature map <span class="math inline">\(\phi\)</span> as well as weights <span class="math inline">\(\mathbf{w}\)</span> makes this problem <em>much</em> harder, both mathematically and computationally. A major enabler of the deep learning revolution has been the development of hardware that is up to the task (especially GPUs), as well as algorithms that make good use of this hardware.</p>
</section>
</section>
<section id="hidden-layers" class="level1 page-columns page-full">
<h1>Hidden Layers</h1>
<p>Ok, so we know that we want to figure out a feature map <span class="math inline">\(\phi:\mathbb{R}^p\rightarrow \mathbb{R}^q\)</span> to make the empirical risk small. There’s no hope of optimizing over all possible feature maps, so instead we need to parameterize our feature map in some way. Here’s what we do:</p>
<div class="page-columns page-full"><p>Let <span class="math inline">\(\alpha_1:\mathbb{R}\rightarrow \mathbb{R}\)</span> be any <em>nonlinear</em> function, and let <span class="math inline">\(\mathbf{U}_1 \in \mathbb{R}^{p} \rightarrow \mathbb{R}^q\)</span>. We’ll define: </p><div class="no-row-height column-margin column-container"><span class=""><span class="math inline">\(\alpha_1\)</span> is often called an <em>activation function.</em></span></div></div>
<p><span class="math display">\[
\phi(\mathbf{X}) = \alpha_1(\mathbf{X}\mathbf{U}_1)\;.
\]</span></p>
<p>Our new prediction rule is</p>
<p><span class="math display">\[
\hat{\mathbf{Y}} = f(\mathbf{X}) = \phi(\mathbf{X})\mathbf{W}= \alpha_1(\mathbf{X}\mathbf{U}_1)\mathbf{W}\;,
\]</span></p>
<p>and our empirical risk minimization problem is</p>
<p><span id="eq-single-layer"><span class="math display">\[
\hat{\mathbf{W}}, \hat{\mathbf{U}}_1 = \mathop{\mathrm{arg\,min}}_{\mathbf{W}\in \mathbb{R}^{q\times \ell}, \mathbf{U}_1 \in \mathbb{R}^{p\times q}} \; \mathcal{L}(\alpha_1(\mathbf{X}\mathbf{U}_1)\mathbf{W}, \mathbf{Y})\;.
\tag{1}\]</span></span></p>
<p><a href="#eq-single-layer" class="quarto-xref">Equation&nbsp;1</a> is an example of a machine learning problem for a neural network with a single <strong><em>hidden layer</em></strong>. The hidden layer refers to the “layer” of computation <span class="math inline">\(\alpha_1(\mathbf{X}\mathbf{U}_1)\)</span>. This layer is “hidden” because it’s not the input of the model (that would be <span class="math inline">\(\mathbf{X}\)</span>) nor is it the output (that would be <span class="math inline">\(\alpha_1(\mathbf{X}\mathbf{U}_1)\mathbf{W}\)</span>). The computational graph is now more complicated: we need to substitute in the explicit computations for <span class="math inline">\(\phi\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  subgraph input
    direction TB  
    x_1 
    x_2
  end

  subgraph hidden_activations
    a_1
    a_2
  end

  subgraph feature_map[features]
    f_1
    f_2
  end

  subgraph raw predictions[raw predictions/logits]
    y_hat_1
    y_hat_2 
    y_hat_3
  end

  subgraph label_preds[label probabilities]
    p_1
    p_2
    p_3
  end

  x_1 --u_1--&gt;a_1
  x_1 --u_2--&gt;a_2
  x_2 --u_1--&gt;a_1
  x_2 --u_2--&gt;a_2

  a_1--alpha_1--&gt;f_1
  a_2--alpha_1--&gt;f_2

  f_1 --w_1--&gt; y_hat_1
  f_2 --w_1--&gt; y_hat_1
  f_1 --w_2--&gt; y_hat_2
  f_2 --w_2--&gt; y_hat_2
  f_1 --w_3--&gt; y_hat_3
  f_2 --w_3--&gt; y_hat_3

  y_hat_1 &amp; y_hat_2 &amp; y_hat_3 --&gt; softmax

  softmax --&gt; p_1 &amp; p_2 &amp; p_3
</pre>
</div>
</div>
</div>
</div>
<p>As you can see, we have to deal with a lot of matrix multiplications and vectorized function operations. We will also, soon, need to think about how to perform the empirical risk minimization problem, which in turn poses questions about things like how to do gradient descent for this kind of problem. While it’s important for us to understand some of the theory, it’s not a great use of anyone’s time to do large amounts of bookkeeping. This is why specialized deep learning libraries exist to help us along.</p>
</section>
<section id="deep-penguin-classification" class="level1">
<h1>Deep Penguin Classification</h1>
<p>Let’s use our friends the penguins as a running example. When working with deep models we often need to construct more complex data structures in order to feed our models to the data. I’m hiding this complexity for today; we’ll discuss it in some more detail in a coming lecture.</p>
<div id="e1830f45" class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>target_ix_dict <span class="op">=</span> {</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Adelie Penguin (Pygoscelis adeliae)"</span> : <span class="dv">0</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Chinstrap penguin (Pygoscelis antarctica)"</span> : <span class="dv">1</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">"Gentoo penguin (Pygoscelis papua)"</span> : <span class="dv">2</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PenguinsDataset(Dataset):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, train <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> train: </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>      url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>      url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv"</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(url)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.drop([<span class="st">"studyName"</span>, <span class="st">"Sample Number"</span>, <span class="st">"Individual ID"</span>, <span class="st">"Date Egg"</span>, <span class="st">"Comments"</span>, <span class="st">"Region"</span>], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df[df[<span class="st">"Sex"</span>] <span class="op">!=</span> <span class="st">"."</span>]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.get_dummies(df, columns <span class="op">=</span> [</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>      <span class="st">"Island"</span>, <span class="st">"Stage"</span>, <span class="st">"Clutch Completion"</span>, <span class="st">"Sex"</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.dropna()</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.df <span class="op">=</span> df</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.transform <span class="op">=</span> <span class="kw">lambda</span> x: torch.tensor(x, dtype <span class="op">=</span> torch.float32)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.target_ix <span class="op">=</span> <span class="kw">lambda</span> x: target_ix_dict[x]</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.target_transform <span class="op">=</span> <span class="kw">lambda</span> x: torch.zeros(</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="dv">3</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>).scatter_(dim<span class="op">=</span><span class="dv">0</span>, index<span class="op">=</span>torch.tensor(x), value<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.df.shape[<span class="dv">0</span>]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> <span class="va">self</span>.df.drop([<span class="st">"Species"</span>], axis <span class="op">=</span> <span class="dv">1</span>).iloc[idx,:]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    label    <span class="op">=</span> <span class="va">self</span>.df.iloc[idx,:][<span class="st">"Species"</span>]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> <span class="va">self</span>.transform(features)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    label    <span class="op">=</span> <span class="va">self</span>.target_ix(label)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    label    <span class="op">=</span> <span class="va">self</span>.target_transform(label)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> features.to(device)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> label.to(device)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features, label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="3dcc69e0" class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  device <span class="op">=</span> <span class="st">"cuda"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  device <span class="op">=</span> <span class="st">"cpu"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Here are our data sets. <code>PenguinsDataset</code> is a custom class that I implemented above, while <code>DataLoader</code> is a utility from <code>torch</code> that automatically handles things like batching and randomization for gradient descent.</p>
<div id="b623d7ff" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> PenguinsDataset()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>val <span class="op">=</span> PenguinsDataset(<span class="va">False</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>val_dataloader <span class="op">=</span> DataLoader(val, batch_size<span class="op">=</span><span class="dv">10</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we’re ready to define our model. To see how we do things in torch, let’s start by implementing logistic regression. To start, we need to create a predictor model that does the prediction step of logistic regression. If you’ll recall, this is nothing more than matrix multiplication. The <code>nn.Linear</code> “layer” supplied by <code>torch</code> implements this:</p>
<div id="c9a804cf" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> penguinLogistic(nn.Module):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.linear <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>      nn.Linear(<span class="dv">14</span>, <span class="dv">3</span>) <span class="co"># (number of features, number of class labels)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.linear(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can optimize this model using the cross-entropy loss and the Adam optimizer. As you’ll recall, logistic regression is nothing more than matrix multiplication plus the cross-entropy loss, so this is a logistic regression model!</p>
<div id="ce484b76" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> penguinLogistic()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we need to actually perform the optimization. The <code>torch</code> package gives us a lot of control over exactly how this happens, and we’ll go over the details in a future lecture.</p>
<div id="ce92a295" class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_loop(model, train_dataloader, val_dataloader, learning_rate, epochs):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  train_size <span class="op">=</span> <span class="bu">len</span>(train_dataloader.dataset)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  val_size <span class="op">=</span> <span class="bu">len</span>(val_dataloader.dataset)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  train_loss_history <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  train_acc_history  <span class="op">=</span> []</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  val_loss_history   <span class="op">=</span> []</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  val_acc_history    <span class="op">=</span> []</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    train_acc  <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    val_loss   <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    val_acc    <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dataloader):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>      optimizer.zero_grad()</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>      pred <span class="op">=</span> model(X)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>      fit <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>      train_loss <span class="op">+=</span> fit.item() <span class="op">/</span> train_size</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>      train_acc <span class="op">+=</span> (pred.argmax(dim <span class="op">=</span> <span class="dv">1</span>) <span class="op">==</span> y.argmax(dim <span class="op">=</span> <span class="dv">1</span>)).<span class="bu">sum</span>() <span class="op">/</span> train_size</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Backpropagation</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>      fit.backward()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>      optimizer.step()</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    train_loss_history <span class="op">+=</span> [train_loss]</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    train_acc_history  <span class="op">+=</span> [train_acc]</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(val_dataloader):</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>      <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        fit <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        val_loss <span class="op">+=</span> fit.item() <span class="op">/</span> val_size</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        val_acc <span class="op">+=</span> (pred.argmax(dim <span class="op">=</span> <span class="dv">1</span>) <span class="op">==</span> y.argmax(dim <span class="op">=</span> <span class="dv">1</span>)).<span class="bu">sum</span>() <span class="op">/</span> val_size</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    val_loss_history <span class="op">+=</span> [val_loss]</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    val_acc_history  <span class="op">+=</span> [val_acc]</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>      <span class="bu">print</span>(<span class="ss">f"epoch </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">: val_loss = </span><span class="sc">{</span>val_loss<span class="sc">}</span><span class="ss">, val_accuracy = </span><span class="sc">{</span>val_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> train_loss_history, train_acc_history, val_loss_history, val_acc_history</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_histories(tlh, tah, vlh, vah):</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>  fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize <span class="op">=</span> (<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">0</span>].plot(tlh, label <span class="op">=</span> <span class="st">"train"</span>)</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">0</span>].plot(vlh, label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">0</span>].legend()</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">0</span>].<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, title <span class="op">=</span> <span class="st">"loss"</span>)</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">0</span>].semilogy()</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">1</span>].plot(tah, label <span class="op">=</span> <span class="st">"train"</span>)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">1</span>].plot(vah, label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">1</span>].legend()</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>  axarr[<span class="dv">1</span>].<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, title <span class="op">=</span> <span class="st">"accuracy"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s go ahead and train!</p>
<div id="50f14370" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>tlh, tah, vlh, vah <span class="op">=</span> training_loop(model, train_dataloader, val_dataloader, <span class="fl">1e-4</span>, <span class="dv">500</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plot_histories(tlh, tah, vlh, vah)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 0: val_loss = 45.5021510404699, val_accuracy = 0.4852941632270813
epoch 50: val_loss = 0.609782618634841, val_accuracy = 0.07352941483259201
epoch 100: val_loss = 0.3989190424189848, val_accuracy = 0.1764705926179886
epoch 150: val_loss = 0.19054848131011515, val_accuracy = 0.44117650389671326
epoch 200: val_loss = 0.06688327999675975, val_accuracy = 0.7352941036224365
epoch 250: val_loss = 0.03516546855954562, val_accuracy = 0.9264706373214722
epoch 300: val_loss = 0.026237546082805183, val_accuracy = 0.9705882668495178
epoch 350: val_loss = 0.02203846328398761, val_accuracy = 0.9852941632270813
epoch 400: val_loss = 0.01984664469080813, val_accuracy = 1.0000001192092896
epoch 450: val_loss = 0.015965097419479313, val_accuracy = 0.9852941632270813</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="intro-deep_files/figure-html/cell-8-output-2.png" class="" width="506" height="302"></p>
</div>
</div>
<p>We can see that our model is doing much better than we would expect from random guessing on this problem, although it may not be competitive with many of the models that you implemented in your analysis of the penguins data set. Further training or tweaks to parameters like batch sizes and learning rates could potentially help improve performance here.</p>
<p>Let’s try adding a hidden layer, as in <a href="#eq-single-layer" class="quarto-xref">Equation&nbsp;1</a>. To do this, we need to add a nonlinearity <span class="math inline">\(\alpha\)</span> and more <code>Linear</code> layers. The important points are:</p>
<ol type="1">
<li>The first dimension of the first linear layer needs to match the number of features of the input.</li>
<li>The final dimension of the last linear layer needs to match the number of possible labels.</li>
<li>The final dimension of each linear layer needs to match the first dimension of the next layer.</li>
</ol>
<p>These rules follow directly from the need to make all the matrix multiplications turn out right.</p>
<p>So, let’s try a model with a single layer of 100 hidden units:</p>
<div id="a2f0523e" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> penguinNN(nn.Module):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>          nn.Linear(<span class="dv">14</span>, <span class="dv">100</span>), <span class="co"># U_1</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>          nn.ReLU(),          <span class="co"># common choice of alpha these days</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>          nn.Linear(<span class="dv">100</span>, <span class="dv">3</span>)   <span class="co"># W</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> penguinNN().to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can optimize it using the same approach as before:</p>
<div id="817ef672" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>tlh, tah, vlh, vah <span class="op">=</span> training_loop(model, train_dataloader, val_dataloader, <span class="fl">1e-4</span>, <span class="dv">500</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plot_histories(tlh, tah, vlh, vah)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 0: val_loss = 10.191597097060258, val_accuracy = 0.3382353186607361
epoch 50: val_loss = 0.06825322088073282, val_accuracy = 0.7647059559822083
epoch 100: val_loss = 0.047590394668719345, val_accuracy = 0.867647111415863
epoch 150: val_loss = 0.053128435331232404, val_accuracy = 0.7058823704719543
epoch 200: val_loss = 0.03455903118147569, val_accuracy = 0.9264706373214722
epoch 250: val_loss = 0.0409484260222491, val_accuracy = 0.779411792755127
epoch 300: val_loss = 0.02161241366582758, val_accuracy = 0.9558823704719543
epoch 350: val_loss = 0.019268721122952068, val_accuracy = 0.9852941632270813
epoch 400: val_loss = 0.057865854571847355, val_accuracy = 0.6764705777168274
epoch 450: val_loss = 0.020513927235322842, val_accuracy = 0.9558823704719543</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="intro-deep_files/figure-html/cell-10-output-2.png" class="" width="506" height="302"></p>
</div>
</div>
<p>This model has also done well on this task, although still not closer to perfect accuracy. It’s also interesting to note that the model’s loss and accuracy both vary quite wildly during the training process. This is not uncommon, but may also point to a possible benefit of using a smaller step-size.</p>
<p>It might appear that we haven’t really gained much from the massive costs of mathematical structure and computational architecture – we could have done better with just tools from <code>sklearn</code>, or even our own hand-built implementations! We’ll soon see the infrastructure working to our advantage when we get to problems involving large data sets with complex structure, like text and images.</p>


</section>

<p><br> <br> <span style="color:grey;">© Phil Chodrow, 2023</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        return container.innerHTML
      } else {
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        console.log("RESIZE");
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>