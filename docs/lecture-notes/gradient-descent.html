<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Phil Chodrow">

<title>Optimization with Gradient Descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/icons/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><p>Optimization with Gradient Descent</p></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../"><b>Machine Learning</b><br>CSCI 0451</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../syllabus.html" class="sidebar-item-text sidebar-link">Syllabus</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../schedule.html" class="sidebar-item-text sidebar-link">Schedule</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments.html" class="sidebar-item-text sidebar-link">Index of Assignments</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#quick-recap" id="toc-quick-recap" class="nav-link active" data-scroll-target="#quick-recap">Quick Recap</a>
  <ul class="collapse">
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients">Gradients</a></li>
  <li><a href="#gradient-descent-for-empirical-risk-minimization" id="toc-gradient-descent-for-empirical-risk-minimization" class="nav-link" data-scroll-target="#gradient-descent-for-empirical-risk-minimization">Gradient Descent For Empirical Risk Minimization</a></li>
  <li><a href="#activity" id="toc-activity" class="nav-link" data-scroll-target="#activity">Activity</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><p>Optimization with Gradient Descent</p></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Phil Chodrow </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<div class="hidden">
$$
<p>$$</p>
</div>
<section id="quick-recap" class="level1 page-columns page-full">
<h1>Quick Recap</h1>
<p>Last time, we considered the problem of <em>empirical risk minimization</em> with a <em>convex</em> loss function. We assumed that we had data, a pair <span class="math inline">\((\mathbf{X}, \mathbf{y})\)</span> where</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\in \mathbb{R}^{n\times p}\)</span> is the <em>feature matrix</em>. There are <span class="math inline">\(n\)</span> distinct observations, encoded as rows. Each of the <span class="math inline">\(p\)</span> columns corresponds to a <em>feature</em>: something about each observation that we can measure or infer. Each observation is written <span class="math inline">\(\mathbf{x}_1, \mathbf{x}_2,\ldots\)</span>. <span class="math display">\[
\mathbf{X}= \left[\begin{matrix}  &amp; - &amp; \mathbf{x}_1 &amp; - &amp;\\
&amp; - &amp; \mathbf{x}_2 &amp; - &amp;\\
&amp; \vdots &amp; \vdots &amp; \vdots &amp;\\
&amp; - &amp; \mathbf{x}_{n} &amp; - &amp;\end{matrix}\right]
\]</span></li>
<li><span class="math inline">\(\mathbf{y}\in \mathbb{R}^{n}\)</span> is the <em>target vector</em>. The target vector gives a label, value, or outcome for each observation.</li>
</ul>
<p>Using this data, we defined the empirical risk minimization problem, which had the general form <span id="eq-empirical-risk-minimization"><span class="math display">\[
\hat{\mathbf{w}} = \mathop{\mathrm{arg\,min}}_{\mathbf{w}} \; L(\mathbf{w})\;,
\tag{1}\]</span></span> where <span class="math display">\[
L(\mathbf{w}) = \frac{1}{n} \sum_{i = 1}^n \ell(f_{\mathbf{w}}(\mathbf{x}_i), y_i)\;.
\]</span></p>
<p>Here, <span class="math inline">\(f_{\mathbf{w}}:\mathbb{R}^p \rightarrow \mathbb{R}\)</span> is our predictor function, which takes in a feature vector <span class="math inline">\(\mathbf{x}_i\)</span> and spits out a prediction <span class="math inline">\(\hat{y}_i\)</span>. We are still assuming that <span class="math inline">\(f_{\mathbf{w}}\)</span> is linear and therefore has the form</p>
<div class="page-columns page-full"><p> <span id="eq-linear-predictor"><span class="math display">\[
f_{\mathbf{w}}(\mathbf{x}) = \langle \mathbf{w}, \mathbf{x} \rangle
\tag{2}\]</span></span></p><div class="no-row-height column-margin column-container"><span class="">Originally we considered classifiers of the form <span class="math inline">\(f_{\mathbf{w}, b}(\mathbf{x}) = \langle \mathbf{w}, \mathbf{x} \rangle - b\)</span>, but we can ignore <span class="math inline">\(b\)</span> for today by using the assumption that the final column of <span class="math inline">\(\mathbf{x}\)</span> is a column of <span class="math inline">\(1\)</span>s, just like we did for the perceptron.</span></div></div>
<p>The function <span class="math inline">\(\ell:\mathbb{R}^2 \rightarrow \mathbb{R}\)</span> was our focus last time: this function takes a prediction <span class="math inline">\(\hat{y}_i\)</span> and a true label <span class="math inline">\(y_i\)</span> and returns a number that tells you <em>how bad</em> that prediction was. When studying the perceptron, we considered the 0-1 loss:</p>
<div class="page-columns page-full"><p> <span class="math display">\[\ell(\hat{y}, y) = 1 - \mathbb{1}[\hat{y}y &gt; 0]\;,\]</span></p><div class="no-row-height column-margin column-container"><span class="">Recall that for this version of the 0-1 loss to make sense we needed to assume that <span class="math inline">\(y\in \{-1, 1\}\)</span>, rather than the more standard assumption <span class="math inline">\(y \in \{0, 1\}\)</span> that we’ll use more frequently in this course.</span></div></div>
<p>which is <span class="math inline">\(0\)</span> if <span class="math inline">\(\hat{y}\)</span> has the same sign as <span class="math inline">\(y\)</span> and 1 if <span class="math inline">\(\hat{y}\)</span> has a different sign from <span class="math inline">\(y\)</span>. At the end of the lecture, we introduced the alternative <em>logistic loss</em> <span class="math display">\[
\ell(\hat{y}, y) = -y \log \sigma(\hat{y}) - (1-y)\log (1-\sigma(\hat{y}))\;,
\]</span></p>
<p>where <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the <em>logistic sigmoid</em> function. Unlike the 0-1 loss, the logistic loss is <em>strictly convex</em> in its first argument. Recall that a strictly convex function <span class="math inline">\(g:\mathbb{R}^n\rightarrow \mathbb{R}\)</span> is a function that satisfies the strict inequality <span class="math display">\[
g(\lambda \mathbf{z}_1 + (1-\lambda)\mathbf{z}_2) &lt; \lambda g( \mathbf{z}_1 ) + (1-\lambda)g(\mathbf{z}_2)\;.
\]</span></p>
<p>for any <span class="math inline">\(\mathbf{z}_1, \mathbf{z}_2 \in \mathbb{R}^n\)</span> and <span class="math inline">\(\lambda \in [0,1]\)</span>. So, when we say that the logistic loss <span class="math inline">\(\ell\)</span> is strictly convex in its first argument, what we mean is that</p>
<p><span class="math display">\[
\ell(\lambda \hat{y}_1 + (1-\lambda)\hat{y}_2, y) &lt; \lambda \ell( \hat{y}_1, y) + (1-\lambda)\ell(\hat{y}_2, y)
\]</span> for any true label <span class="math inline">\(y\)</span>, possible predictions <span class="math inline">\(\hat{y}_1, \hat{y}_2\)</span>, and number <span class="math inline">\(\lambda \in [0,1]\)</span>.</p>
<p>As we discussed, the important thing about empirical risk minimization with linear predictors and convex loss functions is that, if there is an optimal solution, that solution is unique. Furthermore, any local minimizer of the loss is in fact the (unique) global minimizer. So, we can design algorithms that search for <em>local</em> minima. In this lecture we are going to discuss <em>gradient methods</em>, which are the most common class of optimization algorithms in use today.</p>
<section id="gradients" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradients">Gradients</h2>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">We’re not going to talk much about what it means for a function to be multivariate differentiable. You can assume that all the functions we will deal with in this class are unless I highlight otherwise. For a more rigorous definition, you should check out a multivariable calculus class.</span></div></div>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="def-gradient" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 (Gradient of a Multivariate Function) </strong></span>Let <span class="math inline">\(f:\mathbb{R}^p \rightarrow \mathbb{R}\)</span> be a <em>multivariate differentiable</em> function. The <em>gradient</em> of <span class="math inline">\(f\)</span> evaluated at point <span class="math inline">\(\mathbf{z}\in \mathbb{R}^p\)</span> is written <span class="math inline">\(\nabla f(\mathbf{z})\)</span>, and has value</p>
<p><span class="math display">\[
\nabla f(\mathbf{z}) \triangleq
\left(\begin{matrix}
    \frac{\partial f(\mathbf{z})}{\partial z_1} \\
    \frac{\partial f(\mathbf{z})}{\partial z_2} \\
    \cdots \\
    \frac{\partial f(\mathbf{z})}{\partial z_p} \\
\end{matrix}\right) \in \mathbb{R}^p\;.
\]</span></p>
<p>Here, <span class="math inline">\(\frac{\partial f(\mathbf{z})}{\partial z_1}\)</span> is the <em>partial derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(z_1\)</span>, evaluated at <span class="math inline">\(\mathbf{z}\)</span></em>. To compute it:</p>
<blockquote class="blockquote">
<p>Take the derivative of <span class="math inline">\(f\)</span> *with respect to variable <span class="math inline">\(z_1\)</span>, holding all other variables constant, and then evaluate the result at <span class="math inline">\(\mathbf{z}\)</span>.</p>
</blockquote>
</div>
</div>
</div>
</div>
<div class="callout-tip callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(p = 3\)</span>. Let <span class="math inline">\(f(\mathbf{z}) = z_2\sin z_1 + z_1e^{2z_3}\)</span>. The partial derivatives we need are</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial f(\mathbf{z})}{\partial z_1} &amp;= z_2 \cos z_1 + e^{2z_3}\\
\frac{\partial f(\mathbf{z})}{\partial z_2} &amp;= \sin z_1\\
\frac{\partial f(\mathbf{z})}{\partial z_3} &amp;= 2z_1 e^{2z_3}\;.
\end{align}
\]</span></p>
<p>So, the gradient of <span class="math inline">\(f\)</span> evaluated at a point <span class="math inline">\(\mathbf{z}\)</span> is</p>
<p><span class="math display">\[
\nabla f(\mathbf{z}) =
\left(\begin{matrix}
    \frac{\partial f(\mathbf{z})}{\partial z_1} \\
    \frac{\partial f(\mathbf{z})}{\partial z_2} \\
    \frac{\partial f(\mathbf{z})}{\partial z_3} \\
\end{matrix}\right) =
\left(\begin{matrix}
    z_2 \cos z_1 + e^{2z_3}\\
    \sin z_1\\
    2z_1 e^{2z_3}
\end{matrix}\right)
\]</span></p>
</div>
</div>
<p>So, a gradient <span class="math inline">\(\nabla f(\mathbf{z})\)</span> is a vector of the same dimension as <span class="math inline">\(\mathbf{z}\)</span>. What happens if we combine them? This is where we get to the really important aspects of gradients for practical purposes:</p>
<div class="callout-note callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-local-minima" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Local Minima Have <span class="math inline">\(\nabla f(\mathbf{z}_0) = \mathbf{0}\)</span>) </strong></span>Let <span class="math inline">\(f:\mathbb{R}^p \rightarrow \mathbb{R}\)</span> be differentiable. If <span class="math inline">\(\mathbf{z}_0\)</span> is a local minimum of <span class="math inline">\(f\)</span>, then <span class="math inline">\(\nabla f(\mathbf{z}_0) = \mathbf{0}\)</span>.</p>
</div>
<div id="thm-descent-directions" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (<span class="math inline">\(-\nabla f\)</span> Is A Descent Direction) </strong></span>Let <span class="math inline">\(f:\mathbb{R}^p \rightarrow \mathbb{R}\)</span> be differentiable. Then, for any point <span class="math inline">\(\mathbf{z}\in \mathbb{R}^p\)</span>, if <span class="math inline">\(\nabla f(\mathbf{z}) \neq \mathbf{0}\)</span>, then there exists a scalar <span class="math inline">\(\alpha\)</span> such that, if <span class="math inline">\(\mathbf{z}' = \mathbf{z}- \alpha \nabla f(\mathbf{z})\)</span>, then <span class="math inline">\(f(\mathbf{z}') \leq f(\mathbf{z})\)</span>.</p>
</div>
</div>
</div>
</div>
<p><a href="#thm-local-minima">Theorem&nbsp;1</a> and <a href="#thm-descent-directions">Theorem&nbsp;2</a> are important because they give us the mechanics of how to solve the empirical risk minimization problem (<a href="#eq-empirical-risk-minimization">Equation&nbsp;1</a>). Here’s our first version:</p>
<div class="callout-note callout callout-style-simple no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Algorithm: (Batch) Gradient Descent
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Inputs</strong>: Function <span class="math inline">\(f\)</span>, initial starting point <span class="math inline">\(\mathbf{z}^{(0)}\)</span>, <em>learning rate</em> <span class="math inline">\(\alpha\)</span>.</p>
<p>Until convergence, in each iteration <span class="math inline">\(t\)</span>,</p>
<ul>
<li>Compute <span class="math inline">\(\mathbf{z}^{(t+1)} \gets \mathbf{z}^{(t)} - \alpha \nabla f(\mathbf{z}^{(t)})\)</span>.</li>
</ul>
<p>Return the final value of <span class="math inline">\(\mathbf{z}^{(t)}\)</span>.</p>
</div>
</div>
<p>Convergence for gradient descent can be decided in a few different ways. One approach is to declare convergence when <span class="math inline">\(\nabla f(\mathbf{z}^{(t)})\)</span> is close to 0. Another way is to declare convergence when the improvement in the function <span class="math inline">\(f\)</span> is small enough in magnitude.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Sample code:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> np.allclose(grad, np.zeros(<span class="bu">len</span>(grad))):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"converged"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># or</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> f(w_new) <span class="op">-</span> f(w_prev) <span class="op">&lt;</span> <span class="fl">1e-6</span>:</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"converged"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div><p>The following theorem says that <em>gradient descent works</em> if the learning rate is small enough:</p>
<div class="callout-tip callout callout-style-simple no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<div id="thm-gradient-descent-convergence" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 </strong></span>Suppose that <span class="math inline">\(f\)</span> is strictly convex, is differentiable, and has a global minimizer <span class="math inline">\(\mathbf{z}^*\)</span>. Then, there exists some <span class="math inline">\(\alpha &gt; 0\)</span> such that gradient descent applied to <span class="math inline">\(f\)</span> produces a sequence of points <span class="math inline">\(\mathbf{z}^{(0)}, \mathbf{z}^{(1)},\ldots, \mathbf{z}^{(t)}\)</span> that converges to <span class="math inline">\(\mathbf{z}^*\)</span>.</p>
</div>
</div>
</div>
</div>
</section>
<section id="gradient-descent-for-empirical-risk-minimization" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gradient-descent-for-empirical-risk-minimization">Gradient Descent For Empirical Risk Minimization</h2>
<p>Suppose that we have a per-observation loss function <span class="math inline">\(\ell\)</span> that is strictly convex and differentiable. Suppose that we are still dealing with a linear predictor of the form in <a href="#eq-linear-predictor">Equation&nbsp;2</a>. Then, we know that the empirical risk objective function <span class="math inline">\(L\)</span> is also strictly convex and differentiable. It follows from <a href="#thm-gradient-descent-convergence">Theorem&nbsp;3</a> that, if there is a minimizer <span class="math inline">\(\mathbf{w}^*\)</span> for the empirical risk, then we can find it using gradient descent. To do this, we need to be able to calculate the gradient of the loss function <span class="math inline">\(L\)</span>. Here’s how this looks. Keep in mind that we are differentiating with respect to <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
\nabla L(\mathbf{w}) &amp;= \nabla \left(\frac{1}{n} \sum_{i = 1}^n \ell(f_{\mathbf{w}}(\mathbf{x}_i), y_i)\right) \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n \nabla \ell(f_{\mathbf{w}}(\mathbf{x}_i), y_i) \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(\hat{y}_i, y_i)}{d\hat{y}} \nabla f_{\mathbf{w}}(\mathbf{x}_i) \tag{multivariate chain rule} \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(\hat{y}_i, y_i)}{d\hat{y}}  \mathbf{x}_i \tag{gradient of a linear function} \\
              &amp;= \frac{1}{n} \sum_{i = 1}^n  \frac{d\ell(\langle \mathbf{w}, \mathbf{x}_i \rangle, y_i)}{d\hat{y}} \mathbf{x}_i \tag{$\hat{y}_i = \langle \mathbf{w}, \mathbf{x}_i \rangle$} \\
\end{align}
\]</span></p>
<p>The good news here is that for linear models, we don’t actually need to be able to compute more gradients: we just need to be able to compute derivatives of the form <span class="math inline">\(\frac{d\ell(\hat{y}_i, y_i)}{d\hat{y}}\)</span> and then plug in <span class="math inline">\(\hat{y}_i = \langle \mathbf{w}, \mathbf{x}_i \rangle\)</span>.</p>
<p>Let’s do an example with the logistic loss:</p>
<p><span class="math display">\[\ell(\hat{y}, y) = -y \log \sigma(\hat{y}) - (1-y)\log (1-\sigma(\hat{y}))\;.\]</span></p>
<p>A useful fact to know about the logistic sigmoid function <span class="math inline">\(\sigma\)</span> is that <span class="math inline">\(\frac{d\sigma(\hat{y}) }{d\hat{y}} = \sigma(\hat{y}) (1 - \sigma(\hat{y}))\)</span>. So, using that and the chain rule, the derivative we need is</p>
<p><span class="math display">\[
\begin{align}
\frac{d\ell(\hat{y}, y)}{d\hat{y}} &amp;= -y \frac{1}{\sigma(\hat{y})}\frac{d\sigma(\hat{y}) }{d\hat{y}} - (1-y)\frac{1}{1-\sigma(\hat{y})}\left(- \frac{d\sigma(\hat{y}) }{d\hat{y}}\right) \\
&amp;= -y \frac{1}{\sigma(\hat{y})}\sigma(\hat{y}) (1 - \sigma(\hat{y})) - (1-y)\frac{1}{1-\sigma(\hat{y})}\left(- \sigma(\hat{y}) (1 - \sigma(\hat{y}))\right) \\
&amp;= -y (1 - \sigma(\hat{y})) + (1-y)\sigma(\hat{y}) \\
&amp;= \sigma(\hat{y}) - y\;.
\end{align}
\]</span> Finally, we need to plug this back in to our empirical risk, obtaining the gradient of the empirical risk for logistic regression:</p>
<p><span class="math display">\[
\begin{align}
\nabla L(\mathbf{w}) &amp;= \frac{1}{n} \sum_{i = 1}^n (\sigma(\hat{y}_i) - y_i)\mathbf{x}_i \\
              &amp;=\frac{1}{n} \sum_{i = 1}^n (\sigma(\langle \mathbf{w}, \mathbf{x}_i \rangle) - y_i)\mathbf{x}_i\;.
\end{align}
\]</span></p>
<p>So, we can do logistic regression by choosing a learning rate and iterating the update <span class="math inline">\(\mathbf{w}^{(t+1)} \gets \mathbf{w}^{(t)} - \alpha \nabla L(\mathbf{w}^{(t)})\)</span> until convergence.</p>
<p>Let’s see this in action. Here’s a data set:</p>
<div class="cell page-columns page-full" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>p_features <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_blobs(n_samples <span class="op">=</span> <span class="dv">100</span>, n_features <span class="op">=</span> p_features <span class="op">-</span> <span class="dv">1</span>, centers <span class="op">=</span> [(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">1</span>)])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>], c <span class="op">=</span> y)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>xlab <span class="op">=</span> plt.xlabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>ylab <span class="op">=</span> plt.ylabel(<span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="gradient-descent_files/figure-html/cell-2-output-1.png" class="figure-img" width="587" height="429"></p>
<p></p><figcaption class="figure-caption margin-caption">Note that this data is <strong>not</strong> linearly separable. The perceptron algorithm wouldn’t even have converged for this data set, but logistic regression will do great.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The code below is very similar to the code from last time, but I’m going to first transform the feature matrix <span class="math inline">\(\mathbf{X}\)</span> so that it contains a column of constant features. This is going to make our mathematical life a lot easier.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>np.seterr(<span class="bu">all</span><span class="op">=</span><span class="st">'ignore'</span>) </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># add a constant feature to the feature matrix</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>X_ <span class="op">=</span> np.append(X, np.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>)), <span class="dv">1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X, w):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X<span class="op">@</span>w</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic_loss(y_hat, y): </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>y<span class="op">*</span>np.log(sigmoid(y_hat)) <span class="op">-</span> (<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span>np.log(<span class="dv">1</span><span class="op">-</span>sigmoid(y_hat))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> empirical_risk(X, y, loss, w):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> predict(X, w)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss(y_hat, y).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I’ll start by picking a random parameter vector, visualizing the corresponding line, and computing the loss:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># pick a random weight vector and calculate the loss</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="fl">.5</span> <span class="op">-</span> np.random.rand(p_features)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> empirical_risk(X_, y, logistic_loss, w)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>], c <span class="op">=</span> y)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>xlab <span class="op">=</span> plt.xlabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>ylab <span class="op">=</span> plt.ylabel(<span class="st">"Feature 2"</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">101</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> plt.plot(f1, (w[<span class="dv">2</span>] <span class="op">-</span> f1<span class="op">*</span>w[<span class="dv">0</span>])<span class="op">/</span>w[<span class="dv">1</span>], color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> plt.gca().set_title(<span class="ss">f"Loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="gradient-descent_files/figure-html/cell-4-output-1.png" class="" width="587" height="449"></p>
</div>
</div>
<p>It can be hard to put these kinds of numbers in context, but this is a pretty poor value of the loss. You could probably guess that considering how bad the classifier line looks.</p>
<p>Now let’s go ahead and use gradient descent to compute a better value of the parameter vector <span class="math inline">\(\tilde{\mathbf{w}}\)</span>. I’ve shown you the main loop of the algorithm, but not the calculation of the gradient. It’ll be up to you to implement the gradient (as well as variations of gradient descent) in an upcoming blog post.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> hidden.logistic <span class="im">import</span> gradient</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.001</span> <span class="co"># learning rate</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>done <span class="op">=</span> <span class="va">False</span>       <span class="co"># initialize for while loop</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>prev_loss <span class="op">=</span> np.inf <span class="co"># handy way to start off the loss</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> []</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># main loop</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="kw">not</span> done: </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> alpha<span class="op">*</span>gradient(w, X_, y)                      <span class="co"># gradient step</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    new_loss <span class="op">=</span> empirical_risk(X_, y, logistic_loss, w) <span class="co"># compute loss</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    history.append(new_loss)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># check if loss hasn't changed and terminate if so</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.isclose(new_loss, prev_loss):          </span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        done <span class="op">=</span> <span class="va">True</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        prev_loss <span class="op">=</span> new_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can visualize the resulting classifier and check the value of the loss that it achieves.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> empirical_risk(X_, y, logistic_loss, w)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>fig, axarr <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">0</span>].scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>], c <span class="op">=</span> y)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">0</span>].<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>, title <span class="op">=</span> <span class="ss">f"Loss = </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">101</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> axarr[<span class="dv">0</span>].plot(f1, (w[<span class="dv">2</span>] <span class="op">-</span> f1<span class="op">*</span>w[<span class="dv">0</span>])<span class="op">/</span>w[<span class="dv">1</span>], color <span class="op">=</span> <span class="st">"black"</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">1</span>].plot(history)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>axarr[<span class="dv">1</span>].<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Iteration number"</span>, ylabel <span class="op">=</span> <span class="st">"Empirical Risk"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="gradient-descent_files/figure-html/cell-6-output-1.png" class="" width="661" height="468"></p>
</div>
</div>
<p>That looks better! Note that the data is not linearly separable, but our algorithm still converged to a reasonable solution.</p>
</section>
<section id="activity" class="level2">
<h2 class="anchored" data-anchor-id="activity">Activity</h2>
<p>Consider the function <span class="math inline">\(f(w_0, w_1) = \sin(w_0w_1)\)</span>. You can define this function like this:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(w):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sin(w[<span class="dv">0</span>]<span class="op">*</span>w[<span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Mathematically, the gradient of this function is</p>
<p><span class="math display">\[\nabla f(w_0, w_1) = (w_1\cos w_0w_1, w_0 \cos w_0w_1)^T.\]</span></p>
<ol type="1">
<li>Implement a simple loop that uses gradient descent to find a minimum of this function.
<ul>
<li>You’ll have to choose the learning rate <span class="math inline">\(\alpha\)</span>.</li>
<li>The <code>np.cos()</code> function will be useful for programming the gradient.</li>
<li>It’s not the fastest approach, but if you’re not show how to program the gradient you can always first implement it as a list of two floats, and then use <code>np.array(my_list)</code> to convert it into a numpy array.</li>
<li>You’ll also need to pick a random starting guess.</li>
</ul></li>
<li>Find two initial guesses for the parameter vector <span class="math inline">\(\mathbf{w}\)</span> such that you get two <em>different</em> final minimizers (this is possible because <span class="math inline">\(f\)</span> is not convex).</li>
</ol>


</section>
</section>

<p><br> <br> <span style="color:grey;">© Phil Chodrow, 2023</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>