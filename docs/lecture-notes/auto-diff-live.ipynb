{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most important method of this class, which handles backwards navigation along the computational graph, is the `backward` method: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. This case corresponds the top-level node in the computational graph. In our setting this will usually be the value of the loss function. \n",
        "2. This is the step in which we call the node's internal `_backward()` method. This method \"does the calculus\" and depends on how the node was computed. \n",
        "3. We then need to go through every node that is used in the computation of this node and also calculate *their* gradients. \n",
        "\n",
        "The `backward` method handles the recursive logic of automatic differentiation. However, it doesn't do any of the actual *math*. We need to implement this math within each of the arithmetic operations that we are going to implement for the `Value` class. Here's how we implement addition: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. This adds `other` to the computational graph, converting it to a `Value` if needed. \n",
        "2. We need to do the addition itself, log the fact that `self` and `other` were used as inputs into this operation, and return a `Value` that contains both the `data` reflecting the addition and the information about the inputs. \n",
        "3. Define a `_backward` method to update the gradients for `self` and `other`, using upstream gradient information from `out`, and attach it to `out`. This method will then be used when `backward` is called. The exact structure of `_backward` requires that we do some calculus. \n",
        "\n",
        "Let's do another math operation. This operation is very similar, but with a slightly more complex `_backward` method that reflects the chain rule from calculus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def __mul__(self, other):\n",
        "    other = other if isinstance(other, Value) else Value(other)\n",
        "    out = Value(self.data * other.data, (self, other))\n",
        "\n",
        "    def _backward():\n",
        "        self.grad += other.data * out.grad\n",
        "        other.grad += self.data * out.grad\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "Value.__mul__ = __mul__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having defined addition and multiplication, we can also pretty quickly define subtraction: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def __neg__(self): return self * -1\n",
        "def __sub__(self, other):  return self + (-other)\n",
        "\n",
        "Value.__neg__ = __neg__\n",
        "Value.__sub__ = __sub__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see an example of this in action. We can define the function $f(x) = (x + 2)^2 + x^3$. Let's do this and compute $f(3)$: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now here's the thing: because we can represent $f$ in terms of multiplications and additions, we can *also* calculate $f'(3)$, just by running the `backward` method. Since \n",
        "\n",
        "$$\n",
        "f'(x) = 2(x+2) + 3x^2\\;,\n",
        "$$\n",
        "\n",
        "we are expecting that $f'(3) = 2(3+2) + 3\\cdot 3^2 = 37$. Let's check: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks good! \n",
        "\n",
        "## Autodiff Linear Regression\n",
        "\n",
        "We've implemented enough automatic differentiation that we can differentiation any function that can be constructed out of a combination of additions, subtractions, and multiplications. This is enough to do linear regression with stochastic gradient descent! We'll focus on the 1-dimensional version, in which we want to minimize \n",
        "$$\n",
        "\\mathcal{L}(w, b) = \\frac{1}{n}\\sum_{i = 1}^n (wx_i + b - y_i)^2\\;. \n",
        "$$\n",
        "\n",
        "In stochastic gradient descent, we don't actually need to evaluate all of these terms at once: instead, we can just evaluate (and differentiate)\n",
        "$$\n",
        "\\mathcal{L}_i(w, b) =  (wx_i + b - y_i)^2\\;. \n",
        "$$\n",
        "\n",
        "Here's the computational graph describing the loss function: \n",
        "\n",
        "```{mermaid}\n",
        "\n",
        "flowchart LR\n",
        "    subgraph inputs\n",
        "    w\n",
        "    b\n",
        "    end\n",
        "\n",
        "    w & x_i --> *\n",
        "    b & * --> +\n",
        "    + & y_i --> -\n",
        "    - --> m[*]\n",
        "    - --> m[*]\n",
        "```\n",
        "\n",
        "In order to implement this with automatic differentiation, let's first implement the predictor model $f(x) = wx + b$ as a class, using "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Initialize a random slope $w$ and intercept $b$. \n",
        "2. What should happen when the model accepts input. \n",
        "3. This backend to `__call__` isn't necessary; I just implemented it this way because PyTorch wants us to implement a method called `forward` for our models. \n",
        "4. Zero out the gradients (need to do after each round of gradient descent).\n",
        "\n",
        "Now that we've implemented this model, we're already to train it. First let's create some random data: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "w, b = np.random.rand(), np.random.rand()\n",
        "\n",
        "n = 100\n",
        "X = np.random.rand(n)\n",
        "y = w*X + b + 0.05*np.random.randn(n)\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.gca().set(xlabel = \"Predictor\", ylabel = \"Target\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now let's do stochastic gradient descent: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = 0.01\n",
        "epochs = 5\n",
        "\n",
        "model = Linear()\n",
        "\n",
        "order = np.arange(n) # order in which we'll visit the data\n",
        "\n",
        "for t in range(epochs):\n",
        "    np.random.shuffle(order)\n",
        "    # stochastic gradient descent implementation\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Zero out all previous gradient information in the model\n",
        "2. Compute the loss on a single data pair $(x_i, y_i)$ (we're writing $z^2$ as $z \\cdot z$ because we haven't yet implemented powers, only multiplication). \n",
        "3. Compute the gradients of all the model parameters using automatic differentiation. \n",
        "4. Update the model parameters using gradient descent. \n",
        "\n",
        "We're now able to visualize our model parameters: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(X, y, alpha = 0.5, label = \"data\")\n",
        "plt.gca().set(xlabel = \"Predictor\", ylabel = \"Target\", )\n",
        "\n",
        "x_lin = np.linspace(0, 1, 101)\n",
        "y_hat = model.w.data*x_lin + model.b.data\n",
        "\n",
        "plt.plot(x_lin, y_hat, color = \"black\", label = \"fitted model\")\n",
        "l = plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks ok! \n",
        "\n",
        "## From Here to Torch\n",
        "\n",
        "Once we understand the basic idea of automatic differentiation, it's not so hard to see what goes in to making a high-performance framework like Torch: \n",
        "\n",
        "1. Implement lots and lots of math operations like `__add__` and `__mul__`, with their corresponding `_backwards` methods. \n",
        "2. Do these implementations for *arrays* rather than just numbers. \n",
        "3. Do those implementations in speedy, low-level code that can run on specialized hardware. \n",
        "4. Add various utility functions to make it easy to construct more complicated mathematical functions of the inputs. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
