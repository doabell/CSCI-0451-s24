<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-03-08">
<meta name="description" content="In this blog post, you’ll implement kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries.">

<title>Kernel Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/icons/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Kernel Logistic Regression</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../"><b>Machine Learning</b><br>CSCI 0451</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../syllabus.html" class="sidebar-item-text sidebar-link">Syllabus</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../schedule.html" class="sidebar-item-text sidebar-link">Schedule</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../assignments.html" class="sidebar-item-text sidebar-link">Index of Assignments</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a>
  <ul class="collapse">
  <li><a href="#kernel-logistic-regression" id="toc-kernel-logistic-regression" class="nav-link" data-scroll-target="#kernel-logistic-regression"><span class="toc-section-number">1.1</span>  Kernel Logistic Regression</a></li>
  </ul></li>
  <li><a href="#what-you-should-do" id="toc-what-you-should-do" class="nav-link" data-scroll-target="#what-you-should-do"><span class="toc-section-number">2</span>  What You Should Do</a>
  <ul class="collapse">
  <li><a href="#implement-kernel-logistic-regression" id="toc-implement-kernel-logistic-regression" class="nav-link" data-scroll-target="#implement-kernel-logistic-regression"><span class="toc-section-number">2.1</span>  Implement Kernel Logistic Regression</a></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments"><span class="toc-section-number">2.2</span>  Experiments</a>
  <ul class="collapse">
  <li><a href="#basic-check" id="toc-basic-check" class="nav-link" data-scroll-target="#basic-check">Basic Check</a></li>
  <li><a href="#choosing-gamma" id="toc-choosing-gamma" class="nav-link" data-scroll-target="#choosing-gamma">Choosing <code>gamma</code></a></li>
  <li><a href="#vary-the-noise" id="toc-vary-the-noise" class="nav-link" data-scroll-target="#vary-the-noise">Vary the Noise</a></li>
  <li><a href="#try-other-problem-geometries" id="toc-try-other-problem-geometries" class="nav-link" data-scroll-target="#try-other-problem-geometries">Try Other Problem Geometries</a></li>
  </ul></li>
  <li><a href="#blog-post" id="toc-blog-post" class="nav-link" data-scroll-target="#blog-post"><span class="toc-section-number">2.3</span>  Blog Post</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Kernel Logistic Regression</h1>
</div>

<div>
  <div class="description">
    <p>In this blog post, you’ll implement kernel logistic regression, a method for using linear empirical risk minimization to learn nonlinear decision boundaries.</p>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 8, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="hidden">
$$
<p>$$</p>
</div>
<div class="callout-important callout callout-style-simple">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>This is one of two <em>suggested options</em> for a blog post this week. You might want to pick this option if some of the following bullet points describe you:</p>
<ol type="1">
<li>You enjoy working with matrices and vectors in <code>numpy</code>.</li>
<li>You like math and theoretical aspects of machine learning algorithms.</li>
<li>You are willing to read a little extra theory before starting on your blog post.</li>
</ol>
<p><a href="../../assignments/blog-posts/blog-post-penguins.html">The alternative</a> has a more applied flavor.</p>
</div>
</div>
</div>
<section id="introduction" class="level1 page-columns page-full" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>In this blog post you’ll implement and test <em>kernel logistic regression</em> for binary classification. Kernel logistic regression is one of many <em>kernelized linear classifiers.</em></p>
<p>In regular logistic regression, we aim to solve the empirical risk minimization problem</p>
<p><span class="math display">\[
\hat{\mathbf{w}} = \mathop{\mathrm{arg\,min}}_{\mathbf{w}} \; L(\mathbf{w})\;,
\]</span> where <span class="math display">\[
L(\mathbf{w}) = \frac{1}{n} \sum_{i = 1}^n \ell(\langle \mathbf{w}, \mathbf{x}_i \rangle, y_i)\;
\]</span> is the empirical risk and <span class="math display">\[
\ell(\hat{y}, y) = -y \log \sigma(\hat{y}) - (1-y)\log (1-\sigma(\hat{y}))\;,
\]</span> is the logistic loss. Logistic regression is an outstanding algorithm for linear classification, but it can only handle <em>linear decision boundaries</em>. Here’s an example of a data set that has a clear <em>nonlinear</em> pattern that we’d like to learn:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>np.seterr(<span class="bu">all</span><span class="op">=</span><span class="st">"ignore"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>], c <span class="op">=</span> y)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blog-post-kernel-logistic_files/figure-html/cell-2-output-1.png" width="603" height="429"></p>
</div>
</div>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">Follow the instructions <a href="http://rasbt.github.io/mlxtend/installation/">here</a> to install the <code>mlxtend</code> package.</span></div></div>
<p>A linear separator wouldn’t do great on this data set. To see the best we can do, let’s use pre-implemented versions of logistic regression and a visualization tool:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_decision_regions</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression()</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>LR.fit(X, y)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> LR)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>(LR.predict(X) <span class="op">==</span> y)<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blog-post-kernel-logistic_files/figure-html/cell-3-output-1.png" width="600" height="449"></p>
</div>
</div>
<p>Our classifier does better than random chance, but it looks like we could do significantly better if we were able to learn the “curvy shape” of the data. Here’s an example using <em>kernel logistic regression</em>, which you will implement in this assignment.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> solutions.kernel_logistic <span class="im">import</span> KernelLogisticRegression</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="fl">.1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>(KLR.predict(X) <span class="op">==</span> y)<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blog-post-kernel-logistic_files/figure-html/cell-4-output-1.png" width="600" height="449"></p>
</div>
</div>
<section id="kernel-logistic-regression" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="kernel-logistic-regression"><span class="header-section-number">1.1</span> Kernel Logistic Regression</h2>
<p>In the kernel logistic regression problem, we instead solve empirical risk minimization with modified features. The empirical risk is now</p>
<p><span id="eq-empirical-risk-kernelized"><span class="math display">\[
L_k(\mathbf{v}) = \frac{1}{n} \sum_{i = 1}^n \ell(\langle \mathbf{v}, \boldsymbol{\kappa}(\mathbf{x}_i) \rangle, y_i)\;,
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(\mathbf{v}\in \mathbb{R}^n\)</span> (<strong>not </strong> <span class="math inline">\(\mathbb{R}^p\)</span>). The modified feature vector <span class="math inline">\(\boldsymbol{\kappa}(\mathbf{x}_i)\)</span> has entries</p>
<p><span class="math display">\[
\boldsymbol{\kappa}(\mathbf{x}_i) = \left( \begin{matrix}
    k(\mathbf{x}_1, \mathbf{x}_i) \\
    k(\mathbf{x}_2, \mathbf{x}_i) \\
    \vdots \\
    k(\mathbf{x}_n, \mathbf{x}_i)
\end{matrix}\right)\;.
\]</span></p>
<p>Here, <span class="math inline">\(k:\mathbb{R}^2 \rightarrow \mathbb{R}\)</span> is the <em>kernel function</em>. Kernel functions need to satisfy some special mathematical properties. We’re not going to code them up; instead we’re going to use some built-in functions from <code>scikit-learn</code> to handle the kernel functions for us.</p>
<p>Once the model has been trained and an optimal <span class="math inline">\(\hat{\mathbf{v}}\)</span> has been obtained, one can then make a prediction using the formula</p>
<p><span class="math display">\[
\hat{y} = \langle \hat{\mathbf{v}}, \boldsymbol{\kappa}(\mathbf{x}) \rangle\;.
\]</span></p>
<p>If it is desired to return a 0-1 label instead of a real number, one can return <span class="math inline">\(\mathbb{1}[\hat{y} &gt; 0]\)</span>.</p>
</section>
</section>
<section id="what-you-should-do" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> What You Should Do</h1>
<section id="implement-kernel-logistic-regression" class="level2 page-columns page-full" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="implement-kernel-logistic-regression"><span class="header-section-number">2.1</span> Implement Kernel Logistic Regression</h2>
<p>Implement a Python class called <code>KernelLogisticRegression</code>. You’ll be able to use it like the example in the previous section. Your class should implement the following methods:</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">If you’re not sure how to use <code>**kwargs</code> in Python functions and methods, you might want to check <a href="https://realpython.com/python-kwargs-and-args/#using-the-python-kwargs-variable-in-function-definitions">this resource</a>.</span></div></div>
<ul>
<li><code>__init__(self, kernel, **kernel_kwargs)</code> should accept a <code>kernel</code> function and a set of named keyword arguments called <code>kernel_kwargs</code>. All the <code>__init__()</code> method should do is to save these items as instance variables called
<ul>
<li><code>self.kernel</code></li>
<li><code>self.kernel_kwargs</code></li>
</ul></li>
<li><code>fit(self, X, y)</code> will again be the method that learns the optimal parameters <span class="math inline">\(\hat{v}\)</span>. The fit method is going to look a little different this time:
<ul>
<li>First, <code>pad</code> <code>X</code> to make sure that <code>X</code> contains a column of <code>1</code>s. Here’s a function to do this:</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pad(X):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.append(X, np.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>)), <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong><em>Save</em></strong> <code>X</code> as an instance variable called <code>self.X_train</code>.</li>
<li>Compute the <em>kernel matrix</em> of <code>X</code> with itself. If you implemented <code>__init__()</code> correct, this can be done with the call</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>km <span class="op">=</span> <span class="va">self</span>.kernel(X_, X_, <span class="op">**</span><span class="va">self</span>.kernel_kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Minimize the empirical risk <a href="#eq-empirical-risk-kernelized">Equation&nbsp;1</a>. You might find it useful to define a separate function for computing the empirical risk. Note that the predictor is <em>still</em> an inner product, just with a different parameter vector <span class="math inline">\(\mathbf{v}\)</span> and a different matrix column <span class="math inline">\(\boldsymbol{\kappa}(\mathbf{x}_i)\)</span>. This means that, if you’re careful, you can compute the entire empirical risk using just one matrix multiplication!
<ul>
<li>However you find it, save the resulting optimal value of <span class="math inline">\(\mathbf{v}\)</span> as <code>self.v</code>.</li>
<li>You should still use the logistic loss for <span class="math inline">\(\ell\)</span>.</li>
<li>You will probably need to choose a random initial <span class="math inline">\(\mathbf{v}\)</span>. Don’t forget that <span class="math inline">\(\mathbf{v}\)</span> should have <em>length equal to the number of data points</em>, <em>not the number of features</em>.</li>
</ul></li>
<li>If you’ve already implemented gradient descent for logistic regression in <a href="../../assignments/blog-posts/blog-post-optimization.html">this blog post</a>, then it’s not too hard to adapt your method to kernel logistic regression. However, it’s also fine to use the function <code>scipy.optimize.minimize()</code> as demonstrated in <a href="../../lecture-notes/convex-linear-models.html">this lecture</a>.</li>
</ul></li>
<li><code>predict(self, X)</code> should accept a new feature matrix and return binary labels <span class="math inline">\(\{0,1\}\)</span>. For each row of <span class="math inline">\(\mathbf{X}\)</span>, the prediction is obtained using the formula <span class="math inline">\(\mathbb{1}[\langle \hat{\mathbf{v}}, \boldsymbol{\kappa}(\mathbf{x}) \rangle]\)</span>. To do this:
<ul>
<li>Compute the kernel matrix between <code>self.X_train</code> and the new feature input <code>X</code>. Each column of this matrix is <span class="math inline">\(\boldsymbol{\kappa}(\mathbf{x}_j)\)</span> for some <span class="math inline">\(j\)</span>.</li>
<li>Compute inner products of the form <span class="math inline">\(\langle \mathbf{v}, \boldsymbol{\kappa}(\mathbf{x}_j) \rangle\)</span>. If the user supplies a matrix <code>X</code> with multiple columns, you should be able to compute all the predictions at once. This can be done efficiently using matrix multiplication.</li>
<li>Finally, return a binary vector <span class="math inline">\(\hat{\mathbf{y}}\)</span> whose <span class="math inline">\(j\)</span>th entry is <span class="math inline">\(\hat{y}_j = \mathbb{1}[\langle \mathbf{v}, \mathbf{x}_j \rangle &gt; 0]\)</span>.</li>
</ul></li>
<li><code>score(self, X, y)</code> computes the accuracy of the model predictions on the feature matrix <code>X</code> with labels <code>y</code>.</li>
</ul>
<p>You can assume that the user will always only call <code>predict</code> and <code>score</code> after calling <code>fit</code>. If you’d like, you’re welcome to add warnings or handle other cases in which the user may be less cooperative and attempt to call one of those methods first.</p>
<p>My complete implementation of kernel logistic regression was about 50 lines of code, excluding comments.</p>
<p><strong>Docstrings are not expected</strong> for this blog post.</p>
</section>
<section id="experiments" class="level2 page-columns page-full" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="experiments"><span class="header-section-number">2.2</span> Experiments</h2>
<section id="basic-check" class="level3">
<h3 class="anchored" data-anchor-id="basic-check">Basic Check</h3>
<p>Once you’re done, you’ll be able to import and and use your function like this.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> kernel_logistic <span class="im">import</span> KernelLogisticRegression <span class="co"># your source code</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> rbf_kernel</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons, make_circles</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="fl">.1</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here, the <code>rbf_kernel</code> is the kernel function and <code>gamma</code> is a parameter to that kernel function that says how “wiggly” the decision boundary should be. Larger <code>gamma</code> means a more wiggly decision boundary.</p>
<p>Your implementation is likely correct when you can generate new synthetic versions of the data set above (just call <code>make_moons</code> again) and achieve accuracy consistently at or above 90%. To check that, you can just run the code block above a few times.</p>
</section>
<section id="choosing-gamma" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="choosing-gamma">Choosing <code>gamma</code></h3>
<p>When we choose a very large value of <code>gamma</code>, we can achieve a very wiggly decision boundary with very good accuracy on the training data. For example:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>KLR <span class="op">=</span> KernelLogisticRegression(rbf_kernel, gamma <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>KLR.fit(X, y)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(KLR.score(X, y))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog-post-kernel-logistic_files/figure-html/cell-5-output-2.png" width="600" height="449"></p>
</div>
</div>
<p>Here, our classifier draws a little orange blob around each orange data point: points very nearby are classified as orange while other points are classified as blue. This is sufficient to achieve 100% accuracy on the training data. But this doesn’t <em>generalize</em>: generate some new data and we’ll see much worse performance:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># new data with the same rough pattern</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(<span class="dv">200</span>, shuffle <span class="op">=</span> <span class="va">True</span>, noise <span class="op">=</span> <span class="fl">0.2</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plot_decision_regions(X, y, clf <span class="op">=</span> KLR)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> plt.gca().<span class="bu">set</span>(title <span class="op">=</span> <span class="ss">f"Accuracy = </span><span class="sc">{</span>KLR<span class="sc">.</span>score(X, y)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                      xlabel <span class="op">=</span> <span class="st">"Feature 1"</span>, </span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                      ylabel <span class="op">=</span> <span class="st">"Feature 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blog-post-kernel-logistic_files/figure-html/cell-6-output-1.png" width="600" height="449"></p>
</div>
</div>
<p>Whoops! Not so good. We say that the <em>validation</em> or <em>testing</em> accuracy of the classifier is quite low. Cases in which the validation accuracy is low even though the training accuracy is high are classic instances of overfitting.</p>
<div class="page-columns page-full"><p> Design an experiment in which you fit your model for several different values of <code>gamma</code>. Show accuracy on both training data (the data on which the model was <code>fit</code>) and testing data (data generated from the same settings but which the model has never seen before). Please show your findings in the form of an attractive visualization with clear labels and a clear message.</p><div class="no-row-height column-margin column-container"><span class="">My suggestion is to choose <code>gamma in 10**np.arange(-5, 6)</code></span></div></div>
</section>
<section id="vary-the-noise" class="level3">
<h3 class="anchored" data-anchor-id="vary-the-noise">Vary the Noise</h3>
<p>Repeat your experiment with at least two other values of the <code>noise</code> parameter to <code>make_moons</code>. The noise determines how spread out the two crescents of points are. Do your findings suggest that the best value of <code>gamma</code> depends much on the amount of noise?</p>
</section>
<section id="try-other-problem-geometries" class="level3">
<h3 class="anchored" data-anchor-id="try-other-problem-geometries">Try Other Problem Geometries</h3>
<p>Use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles"><code>make_circles</code></a> function to generate some concentric circles instead of crescents. Show a few examples with varying amounts of noise. Can you find some values of <code>gamma</code> that look like they lead to good learning performance for this data set? Here’s an example of a fairly successful classifier: both the points and the accuracy are computed on unseen test data.</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>0.96</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blog-post-kernel-logistic_files/figure-html/cell-7-output-2.png" width="600" height="449"></p>
</div>
</div>
</section>
</section>
<section id="blog-post" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="blog-post"><span class="header-section-number">2.3</span> Blog Post</h2>
<p>Your blog post should describe your approach to your code and written descriptions of your experiments.</p>
<ul>
<li><strong>Please include a walk-through for your user of how you computed the empirical loss.</strong></li>
<li>Please make sure that your figures are appropriately labeled and described.</li>
<li>Please make sure to include a link to the GitHub page containing your source code at the very beginning of the blog post.</li>
</ul>
<p>In case you’re curious, it’s possible to add <a href="https://quarto.org/docs/authoring/figures.html">formal captions</a> to your figures in Quarto. This makes things look a little fancier, but is not required!</p>
<p>Once you’re happy with how things look, render your blog, push it to GitHub, and submit a link to the URL of your blog post on Canvas.</p>


</section>
</section>

<p><br> <br> <span style="color:grey;">© Phil Chodrow, 2023</span></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>